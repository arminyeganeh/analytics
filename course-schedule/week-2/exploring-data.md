---
description: Week 2, Lecture A2.1, 1500 words, 1 hour to complete
---

# Exploring Data



CASE STUDY — Distributions of Body Height and Income Data and describing distributions Let us consider two examples using population data from the USA. For this purpose, we use the height—income—distributions dataset from 2014. Our first example is adult height, which is well approximated by the normal distribution for the vast part of the distribution, but not for extreme values. Average height among adult women in the USA population is around 163 cm (5/4"), with standard deviation 7.1 cm (2.5"). Thus, for example, women shorter than 145 cm (4/8") should be around 0.1 percent in the population. Yet there are more than 0.1 percent American women that short (over 1 percent in our sample). Similarly to height, many real-life variables with some extreme values are still well approximated by the normal for the larger middle part of the distribution, often as much as over 99 percent of the distribution. Whether that makes the normal a good approximation or not for the purpose of data analysis depends on whether we are interested in extreme values or not. Figure 3.10 shows an example: the histogram of the height of American women aged 55—59. We have overlaid the density plot of the bell-shaped theoretical normal distribution that has the same mean and standard deviation as height in the data (l .63, 0.07).

Height (meters) Figure 3.10 Example for approximately normal distribution: women's height Note: Histogram of the height of women. Overlaid with bell curve of the normal distribution. Source: height-income-distributions dataset, Health and Retirement Study 2014, females of age 55—59. N=1991. Second, let us consider a variable that has a few large values: household income. Figure 3.1 1 shows the distribution of household income among households of women age 55 to 59 in the USA. Figure 3.1 la shows the histogram of household income (in 1000 US dollars, without incomes below 1 thousand or above 1 million US dollars). Figure 3.1 1b shows the histogram of the log of household income, overlaid with the density plot of the normal distribution with the same mean and standard deviation. 10.0% 7.5% 5.0% 2.5% 0.0% Household income (thousand USD) In(household income, thousand USD) (a) Distribution of household income (b) Distribution of the natural log of household income Figure 3.11 An example for an approximately lognormal variable: Household income Source: height-income-distributions dataset, Health and Retirement Study 2014, females of age 55—59. N=2012. 3.10 Steps of Exploratory Analysis Steps of Exploratory Data Analysis At the end of this chapter let us review the steps of exploratory data analysis (EDA). With clean and tidy data we can start our analysis. EDA is the first step of analysis: we should describe variables on their own before doing anything else. EDA helps data analysts " know their data, " which is one of our most important pieces of advice. Getting familiar with the details of the data is important. Results of EDA may feed back into data wrangling (cleaning and restructuring data) if they uncover further issues. Moreover, these results should influence the next steps of analysis and may lead to transforming or changing the data. They are also essential for putting results in context. Finally, the results of EDA may lead to asking new questions that we previously haven't thought of. Exploratory data analysis and data wrangling form an iterative process. EDA may uncover issues that call for additional data cleaning, stepping back again. Then, further steps of data analysis may raise additional questions or issues that call for additional EDA and, possibly, additional data wrangling. For example, when exploring employee churn at our company (whether and why some employees quit within a short time), we may uncover a 100 % churn rate in one department (everybody seems to have quit). That may be true churn that we would explore in detail. Or, it may be something else — e.g., an error in the data or closing of a department. Those cases would call for fixing the error or modifying the data for the analysis by excluding employees of that department from the analysis. Exploratory data analysis should start by focusing on the variables that are the most important for the analysis. If other variables turn out to be important, we can always come back to explore those variables later. For each variable, it is good practice to start by describing its entire distribution. For qualitative variables with a few values, that is best done by listing the frequencies and producing simple bar charts. For quantitative variables we usually produce histograms to visualize the distributions. An important first question is whether the distribution has extreme values. If yes, we need to decide what to do with them: if they are obvious errors, we replace them with missing values (or, less frequently, correct them). Similarly, if extreme values are not relevant for the question of the analysis, it makes sense to drop those observations. If they are not obvious errors and may be relevant for the question of the analysis, it is good practice to keep them. We may also decide to transform variables with extreme values. Examining the distribution of a variable can answer other important questions such as whether the distribution is symmetric or skewed, or how many modes it has. We can also go further and see if it is well approximated by one of the theoretical distributions we considered. The next step is looking at descriptive statistics. These summarize some important features of distributions in more precise ways. Having numeric values is necessary to appreciate the magnitude of subsequent results, understand the amount of variation in each variable, see if some values are very rare, and so on. Summary statistics may call for data transformations. Examples include changing units of measurement (e.g., expressing values in thousand dollars) and creating relative measures (e.g., GDP per capita). It usually makes sense to look at minimum, maximum, mean, and median values, standard deviation, and the number of valid observations. Other measures of spread may also be informative, and we may compute the mean-median measure of skewness, standardized by the standard deviation or the mean. Exploratory data analysis may go substantially further, comparing variables or comparing distributions across groups of observations. As in all cases, our advice should be considered as a starting point. Data analysts should make conscious decisions at many points of the analysis, including the focus and details of exploratory data analysis. Start all data analysis with exploratory data analysis (EDA). • Results from EDA will be important for data cleaning, reality checks, understanding context, or asking further questions. • Explore all aspects of the distribution of your most important variables, including potential extreme values. • Produce visualization and tables guided by their usage.

```
PRACTICE QUESTIONS
```

1. The distribution of quantitative variables may be visualized by a histogram or a density plot (kernel density estimate). What's the difference between the two and which one would you use? List at least one advantage for each. How about qualitative variables with a few values?
2. The mean, median, and mode are statistics of central tendency. Explain what they are precisely.
3. The standard deviation, variance, and inter-quartile range are statistics of spread. Explain what they are and give the formula for each.
4. What are percentiles, quartiles, and quintiles? Is the median equal to a percentile?
5. Why do we take the sum of squared deviations from the mean as a measure of spread, not the sum of the deviations themselves?
6. A distribution with a mean higher than the median is skewed. In what direction? Why? Give an intuitive explanation. 7, Extreme values are a challenge to data analysis if they are relevant for the question of the analysis. List two reasons why.
7. What kind of real-life variables are likely to be well approximated by the normal distribution?

What are well approximated by the lognormal distribution? Give an example for each. Exercises

1. What is a box plot, what is a violin plot, and what are they used for? 1(). Based on what you have learnt about measurement scales and descriptive statistics, decide if it is possible to calculate the mean, mode, and median of the following variables that tell us information about the employees at a company: (a) number of years spent in higher education (b) the level of education (high school, undergraduate, graduate, doctoral school) (c) field of education (e.g., IT, engineering, business administration) (d) binary variable that shows whether someone has a university degree.
2. Take Figure 3.9 in Case Study 3.C1. Describe its usage, its main geometric object and how it encodes information, and scaffolding. Would you want to add annotation to it? What and why?
3. Take Table 3.6 in Case Study 3.B1, Comparing hotel prices in Europe: Vienna vs. London Describe its usage, encoding, and scaffolding. Would you do some things differently? What and why?
4. What kind of real-life variables are likely to be well approximated by the Bernoulli distribution? Give two examples.
5. What kind of real-life variables are likely to be well approximated by the binomial distribution? Give two examples.
6. What kind of real-life variables are likely to be well approximated by the power-law distribution? Give two examples. DATA EXERCISES Easier and/or shorter exercises are denoted by \[ \*l; harder and/or longer exercises are denoted by \[ \*\*l.
7. Pick another city beyond Vienna from the hotels-vienna dataset, and create a data table comparable to the one used in our case study. Visualize the distribution of distance and the distribution of price and compute their summary statistics. Are there extreme values? What would you do with them? Describe the two distributions in a few sentences.
8. Use the data on used cars collected from a classified ads site (according to the Chapter 1 data exercise). Visualize the distribution of price and the distribution of age, and compute their summary statistics. Are there extreme values? What would you do with them? Describe the two distributions in a few sentences. \[\*]
9. Pick another season from the football dataset and examine the extent of home team advantage in ways similar to our case study. Compare the results and discuss what you find. \[ \*l
10. Choose the same 2016/7 season from the football dataset as in our data exercise and produce a different table with possibly different statistics to show the extent of home team advantage. Compare the results and discuss what you find. \[\*]
11. Choose a large country (e.g., China, Japan, the United Kingdom) and find data on the population of its largest cities. Plot the histogram of the distribution and create a table with the most important summary statistics. Plot the histogram of log population as well. Finally, create a log rank-log population plot. Is the normal, the lognormal, or the power-law distribution a good approximation of the distribution? Why? \[

REFERENCES AND FURTHER READING There are quite a few papers on the phenomenon of home advantage (Pollard, 2006), and two excellent books on many aspects of soccer and data — Sally & Anderson (2013) and Kuper & Szymanski (2012). On the idea of extreme values and their potentially large role, an interesting book is Nassim Nicholas Taleb: The Black Swan (Taleb, 2007). A great book on the emergence of statistics is (Salsburg, 2001). It offers a non-technically demanding yet precise discussion of how key concepts such as distribution, random sampling, or correlation emerged. An early apostle of good graphs is Ronald A. Fisher. In his 1925 book (Fisher, 1925) the first chapter after the introduction is called Diagrams. Data visualization has now a robust literature as well as a large variety of online resources. In particular, our section on graphs has been shaped by the approach of Alberto Cairo, see for instance The Functional Art: An Introduction to Information Graphics and Visualization (Cairo, 201 2) and How Charts Lie (Cairo, 2019). There are many great books on data visualization. Edward Tufte, Visual Explanations: Images and Quantities, Evidence and Narrative (Tufte, 1997) or any other Tufte book are classics. Two recent wonderful books, both with R code, are Kieran Healy, Data Visualization —A Practical Introduction (Healy, 2019) and Claus O. Wilke, Fundamentals of Data Visualization (Wilke, 2019). Another great resource is Chapter 03 in the book R for Data Science by Garrett Grolemund and Hadley Wickham (Grolemund & Wickham, 2017).

UNDER THE HOOD: MORE ON THEORETICAL DISTRIBUTIONS Let's introduce a few more useful concepts that theoretical statisticians use to describe theoretical distributions. We will rarely use these concepts in this textbook. However, they are frequently used in more traditional and more advanced statistical textbooks as well as in statistically more sophisticated analyses. The first concept is the probability distribution function, or pdf In essence, the pdf is the theoretical generalization of the histogram and its smoother cousin, the density plot. The pdf of qualitative (also called discrete) variables, including binary variables, shows the probability of each value in the distribution. The pdf of quantitative (continuous) variables, such as normal or lognormal variables, shows the probability of each value and the values in its close neighborhood in the distribution. The pdf is expressed in a formula as a function of the parameters of the distribution, and it is often represented graphically as a bar chart like the histogram (for qualitative variables) or as a continuous curve like a density plot (for continuous variables). A variation on the pdf is the cdf, the cumulative distribution function. For each value in the distribution, the cdf shows the probability that the variable is equal to that value or a lower value. Thus, at each value of the distribution, the cdf equals the pdf plus the sum of all pdf below that value.

Hence the name "cumulative": the cdf accumulates the pdf up to that point. Similarly to the pdf, the cdf is expressed as a function of the parameters of the theoretical distribution and is often represented graphically. The third advanced concept that is good to know about is the moments of distributions. Moments are a more general name for statistics like the mean and the variance for theoretical distributions. These are the expected value of the variable or the expected value of the square, cube, of the variable, or, like the variance, the expected value of the square, cube, of the variable minus its mean. Moments are numbered according to the power they take, so that the first moment is E\[x], the second moment is E\[x2], and so on. The variance is a second moment of the variable minus its mean so it is also called the second centered moment: E\[(x— Theoretical distributions are fully captured by a few parameters: these are statistics that determine the distributions. For each distribution we introduce their parameters, establish the range of possible values, show the shape of the histogram, and describe how the mean and standard deviation are related to the parameters of the distribution. Bernoulli distribution The distribution of a zero—one binary variable is called Bernoulli. The name comes from Jacob Bernoulli, the mathematician from the 1600s who first examined it. The Bernoulli distribution is one of those rare theoretical distributions that we observe over and over: all zero—one variables are distributed Bernoulli. (Note the use of words: if the distribution of a variable is Bernoulli, we say "it is distributed Bernoulli"; we'll use this expression for other theoretical distributions, too.) Examples include whether a customer makes a purchase, whether the CEO of a firm is young, whether a portfolio produces a large negative loss, or whether the online price of a product is lower than its offline price. The Bernoulli distribution has one parameter: p, the probability of observing value one (instead of value zero). With only two possible values, zero and one, the range of the Bernoulli distribution is zero to one, and its histogram consists of two bars: the frequency of observations with value zero, and the frequency of observations with value one. If, instead of frequency, the histogram shows the proportion of each value, the height of the bar at value one is equal to p, and the height of the bar at zero equals 1 — p. The mean of a Bernoulli variable is simply p, the proportion of ones. (To verify this try p 0 or 1 or p = 0.5.) Its variance is p(l — p) so its standard deviation is p(l — p). Binomial distribution The binomial distribution is based on the Bernoulli distribution. A variable has a binomial distribution if it is the sum of independent Bernoulli variables with the same p parameten Some actual variables that may have a binomial distribution include the number of car accidents, the number of times our portfolio experiences a large loss, or the number of times an expert correctly predicts if a new movie will be profitable. Binomial variables have two parameters: p, the probability of one for each Bernoulli variable and n, the number of Bernoulli variables that are added up. The possible values of a binomial variable are zero, one, and all other integer numbers up to n. Its range is therefore 0 through n. The histogram of a binomial variable has n+ 1 bars (zero, one, through n) if not grouped in bins. The binomial distribution has one mode in the middle, and it is symmetric so its median, mean, and mode are the same. With large n the histogram of a binomial variable is bell shaped. The mean of a binomial variable is np, and its variance is np(l — p), so its standard deviation is np(l — p). The other distributions we cover in this section may approximate quantitative variables (as defined in Chapter 2, Section 2.1). These theoretical distributions are for continuous variables that include fractions as well as irrational numbers such as Tt or the square root of two. In real data few variables can take on such values. Even variables that may in principle be continuous such as distance or time are almost always recorded with countable values such as integers or fractions rounded to a few decimal places. The continuous distributions are best seen as potential approximations of the distribution of real-life quantitative variables. Uniform distribution The uniform distribution characterizes continuous variables with values that are equally likely to occur within a range spanned by a minimum value and a maximum value. Examples of real-life variables that may be approximately uniformly distributed are rare; the day of birth of people is an example. The uniform distribution is more often used as a benchmark to which other distributions may be compared. The uniform distribution has two parameters, the minimum value a and the maximum value b. The histogram of the uniform distribution is completely flat between a and b with zero frequency below a and above b. It is therefore symmetric. It has no mode: any value is just as frequent as any other value. The mean of a uniformly distributed variable is 2±-é the variance is ( b- 12a)2 the standard deviation is 12 Power-law distribution While the lognormal distribution may well approximate the distribution of variables with skewness and some extreme values, it is usually a bad approximation when those extreme values are very extreme. Distributions with very large extreme values may be better approximated by the power-law distribution. The power-law distribution is also known as the scale-free distribution or the Pareto distribution, and it is closely related to Zipf's law. The power-law distribution is a very specific distribution with large extreme values. Its specificity is perhaps best captured by its scale invariance property (hence the name scale-free distribution). Let's take two values in the distribution and their ratio (say, 2:1). Let's compute the number of observations with one value (or within its neighborhood) relative to the number of observations with the other value (its neighborhood). For example, there may be 0.6 times as many cities with a population around 200 thousand than around 100 thousand in a country If the variable has a power-law distribution, this proportion is the same for all value-pairs with the same ratio through the entire distribution. In the city population example, this means that there should be 0.6 times as many cities with 600 thousand inhabitants than 300 thousand, 2 million than 1 million, and so on. This is the scale invariance property. A related, though less intuitive, property, of the power-law distribution is that a scatterplot of the log of the rank of each observation against the log of the value of the variable yields a straight line. The log of the rank of the observation with the largest value is In(l) 0, the log of the rank of the second largest observation is /n(2), and so on. Figure 3.12 shows an example using the population of the 159 largest Japanese cities in 2015 (those larger than 150 000 inhabitants). You will be asked to produce similar plots using other data as data exercises.

5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 In(population, in thousands) Figure 3.12 Log rank—log value plot. Size of Japanese cities Note: The natural log of the rank in the distribution (1 st largest, 2nd largest, ) and the natural log of population. Source: city-size-japan dataset. N=1 59. This latter property of the power-law distribution is the consequence of the fact that the probability of values in the close neighborhood of any value x is proportional to x-a : the value to a negative power, therefore the name power-law. Larger values are less likely than smaller values, and how much less likely depends only on the parameter a. The larger the a, the smaller the likelihood of large values. This a is the only parameter of the power-law distribution. The range of the power-law distribution is zero to infinity (neither included). The power-law distribution is similar to the lognormal in that it is very asymmetric with a long right tail and thus allows for very extreme values. It is different from the lognormal by having a substantially larger likelihood of very extreme large values and thus a substantially fatter right tail. The left part of the power-law distribution is also very different from the lognormal: its histogram continuously decreases in contrast with the up-then-down histogram of the lognormal. There are many variables in real life that are well approximated by a power-law distribution. To be more precise, this is usually true for the upper part of the distribution of such real-life variables. In other words, they are well approximated by power-law above certain values but not below. Examples include the population of cities, the size of firms, individuals' wealth, the magnitude of earthquakes, or the frequency of words in texts (ranking word frequency "the" would be most frequent, followed by "be," and so on). Indeed, the idea that the frequency of a word is inversely proportional to its rank in the frequency table is often referred to as the original version of Zipf's law. These variables have extremely large values with low proportion but still a much higher proportion than what a lognormal distribution would imply. An informative statistic of power-law distributions is the share of the values that is attributed to the top x percent of the distribution. One example is that in the USA, the richest 1 % own 40% of the total wealth. Another example is the so-called 80-20 u rule" that posits that 80% of total sales for a product will be concentrated among 20% of customers. Of course, 80-20 is not so much a rule as an empirical observation that holds in some cases but not necessarily in others. The point is that the fraction of wealth (or population in cities, workers in firms, energy released by earthquakes in the top 20%, top 5%, or top 1% characterizes the power-law distribution.  
