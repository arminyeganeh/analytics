---
description: Week 2, Lecture A2.1, 1500 words, 1 hour to complete
---

# Exploring Data



&#x20;A histogram of a quantitative variable can inform us about the shape of the distribution, whether it has extreme values, or where its center is approximately. But visual displays don't produce the numbers that are often important to answer our questions. How far are hotels from the city center in general? What's the spread of prices? How skewed is the distribution of customer ratings? To answer such questions we need numerical summaries of variables. They are called statistics, and this section covers the most important ones. A statistic of a variable is a meaningful number that we can compute from the data. Examples include mean income or the range of prices. Basic summary statistics are numbers that describe the most important features of the distribution of a variable. Summary statistics can answer questions about variables in our data, and they often lead to further questions to examine. Most readers are probably well acquainted with many summary statistics, including the mean, the median (the middle value), various quantiles (terciles, quartiles, percentiles), and the mode (the value with the highest frequency in the data). The mean, median, and mode are also called measures of central tendency, because they give an answer to where the center of the distribution is. Those answers may be the same (the mean, median, and mode may be equal, or very close to each other), or they may be different. Importantly, we use the terms mean, average, and expected value as synonyms, and we use the notation E\[x] as well ask. 3.6 Summary Statistics for Quantitative Variables

The mean of a quantitative variable is the value that we can expect for a randomly chosen observation. The mean of a 0/1 binary variable is the proportion of observations with value 1. No observation would have the expected value as it is between 0 and 1 since the value of the binary variable can be only 0 or 1. Similarly, most readers know the most important measures of spread, such as the range (the difference between the largest and smallest value), inter-quantile ranges (e.g., the 90—10 percentile range, or the inter-quartile range), the standard deviation, and the variance. The standard deviation captures the typical difference between a randomly chosen observation and the mean. The variance is the square of the standard deviation. The variance is a less intuitive measure, but it is easier to work with because it is a mean value itself. The formulae are: Var\[x] (3.1) n Std\[x] (3.2) Note that alternative formulae for the variance and the standard deviation divide by n — 1 not n. Most data are large enough that this makes no practical difference. It turns out that dividing by n — 1 is the correct formula if we use the statistic in the data to infer the standard deviation in the population that our data represents (see more details in Chapter 5, Section 5.12). Since it makes little difference in practice, and dividing by n is easier to remember, we will continue to divide by n in this textbook. The standard deviation is often used to re-calculate differences between values of a quantitative variable, in order to express those values relative to what a typical difference would be. In a formula, this amounts to dividing the difference by the standard deviation. Such measures are called standardized differences. A widely used standardized difference is from the mean value; it is called the standardized value of a variable or the z-score of the variable. Xstandardized — (3.3) Std\[x] While measures of central value (such as mean, median) and spread (such as range, standard deviation) are usually well known, summary statistics that measure skewness are less frequently used. At the same time skewness can be an important feature of a distribution, showing whether a few observations are responsible for much of the spread. Moreover, there is a very intuitive measure for skewness (which exists in a few variants). Recall that a distribution is skewed if it isn't symmetric. A distribution may be skewed in two ways, having a long left tail or having a long right tail. A long left tail means having a few observations with small values with most observations having larger values. A long right tail means having a few observations with large values with most observations having smaller values. Earlier we showed that the hotel price distributions has a long right tail — such as in Figure 3.4. That is quite typical: skewness with a long right tail is frequent among variables in business, economics, and policy, such as with prices, incomes, and population. The statistic of skewness compares the mean and the median and is called the mean—median measure of skewness. When the distribution is symmetric, its mean and median are the same. When it is skewed with a long right tail, the mean is larger than the median: the few very large values in the right tail tilt the mean further to the right. Conversely, when a distribution is skewed with a long left tail, the mean is smaller than the median: the few very small values in the left tail tilt the mean further to

the left. The mean-median measure of skewness captures this intuition. In order to make this measure comparable across various distributions, we use a standardized measure, dividing the difference by the standard deviation. (Sometimes this measure is multiplied by 3, and then it's called Pearson's second measure of skewness. Yet other times the difference is divided by the mean, median, or some other statistic.) Skewness = c— median\[x] (3.4) Std\[x] Table 3.5 summarizes the most important descriptive statistics we discussed.

```
Type of statistic Name of statistic	Formula	Intuitive content
```

The value we expect chosen observation Median The value of the observation in the middle Mode The value (bin) with the highest frequency Spread Range max\[x] — min\[x] Width of the interval of possible values Inter-quantile range qupper \[X] — mower X Distance between the upper quantile and the lower quantile Variance Var\[x] Standard deviation Std\[x]Typical distance between observations and the mean Skewness Mean—median E— Skewness = median (x) Std\[xl The extent to which values in the tail skewness pull the mean

CASE STUDY — Comparing Hotel Prices in Europe: Vienna vs. London Comparing distributions over two groups We are interested in comparing the hotel markets over Europe, and would like to learn about characteristics of hotel prices. To do that, let us focus on comparing the distribution of prices in Vienna to another city, London. The data we use is an extended version of the dataset we used so far. The hotels-europe dataset includes the same information we saw, for 46 cities and 10 different dates. We will explore it more in Chapter 9. For this case study, we consider the same date as earlier (weekday in November 2017) for Vienna and London.

3.B1 Case Study

We focus on hotels with 3 to 4 stars that are in the actual city of Vienna or London. We have no extreme value of price in London, so we need to drop the single, above 1000 dollars priced hotel in Vienna. In our sample, there are N 435 hotels in the London dataset compared to the N = 207 hotels in Vienna. Figure 3.6 shows two histograms side by side. To make them comparable, they have the same bin size (20 dollars), the same range of axes, and each histogram shows relative frequencies. The same range of axes means that the x axis goes up to 500 dollars both Vienna and London because the maximum price is close to 500 dollars in London. The histograms reveal many important features. Here is a selected set of observations we can make: The range is around 50 dollars in both cities but it ends below 400 dollars in Vienna while it goes close to 500 dollars in London. The London distribution of prices covers more of the higher values, and it is more spread out (i.e. has a larger difference between the minimum and maximum price). Hotel prices tend to be higher in London. Both distributions have a single mode, but their location differs. The bin with the highest frequency (the mode) in Vienna is the 80-100-dollar bin, and it is the 120—140-dollar bin in London. 300 Price (US dollars) Price (US dollars) (a) Vienna (b) London Figure 3.6 The distribution of hotel price in Vienna and London Source: hotels-europe dataset. Vienna and London, 3-4 stars hotels only, for a November 2017 weekday. Vienna: N=207, London: N=435. The same price distributions can be visualized with the help of density plots. Figure 3.7 shows the Vienna and London distributions laid on top of each other The density plots do not convey more information than the histograms. In fact, they are less specific in showing the exact range or the prevalence of extreme values. But comparing density plots on a single graph is just easier -- we can see immediately where the mass of hotels are in Vienna and in London.

Price (US dollars) Figure 3.7 Density plots of hotel prices: Vienna and London Note: Kernel density estimates with Epanechnikov smoothing method. Source: hotels-europe dataset, Vienna and London, 3—4 stars hotels only, for a November 2017 weekday. Vienna: N=207, London: N=435. We can quantify some aspects of the distributions, too. Table 3.6 contains some important summary statistics in the two datasets. Table 3.6 Descriptive statistics for hotel prices in two cities City N Mean Median Min Max Std Skew

```
London 435 202.36	186	49	491	88.13	0.186
Vienna	207 109.98	100	50	383	42.22	0.236
```

Source: hotels-europe dataset. Vienna and London, November 2017, weekday. Average price is 1 10 dollars in Vienna and 202 dollars in London. The difference is 92 dollars, which is almost an extra 90 % relative to the Vienna average. The mean is higher than the median in both cities, indicating a somewhat skewed distribution with a long right tail. Indeed, we can calculate the standardized mean—median measures of skewness, and it is more positive in Vienna ((1 10 100)/42 0.236) than in London ((202 - 1 86) / 88 O. 186). The range of prices is substantially wider in London (491 — 49 442) than in Vienna (383 — 50 — 343), and the standard deviation shows a substantially larger spread in London (88 versus 42). The first column shows that the London dataset has about twice as many observations (435 versus 207). These summary statistics are in line with the conclusions we drew by inspecting the visualized distributions. Hotel prices in London tend to be substantially higher on average. They are also more spread, with a minimum close to the Vienna minimum, but many hotels above 200 dollars. These together imply that there are many hotels in London with a price comparable to hotel prices in Vienna, but there are also many hotels with substantially higher prices. 3.7 Visualizing Summary Statistics Visualizing Summary Statistics The summary statistics we discussed are often presented in table format. But there are creative ways to combine some of them in graphs. We consider two such graphs. The more traditional visualization is the box plot, also called the box and whiskers plot shown in Figure 3.8a. The box plot is really a one-dimensional vertical graph, only it is shown with some width so it looks better. The center of a box plot is a horizontal line at the median value of the variable, placed within a box. The upper side of the box is the third quartile (the 75th percentile) and the lower side is the first quartile (the 25th percentile). Vertical line segments on both the upper and lower side of the box capture most of the rest of the distribution. The ends of these line segments are usually drawn at 1.5 times the inter-quartile range added to the third quartile and subtracted from the first quartile. These endpoints are called adjacent values in the box plot. The lines between the lower (upper) adjacent value and the 25th (75th) percentile range are called whiskers. Observations with values not contained within those values are usually added to the box plot as dots with their respective values. The box plot conveys many important features of distributions such as their skewness and shows some of the quantiles in an explicit way. A smarter-looking alternative is the violin plot shown in Figure 3.8b. In essence, the violin plot adds a twist to the box plot by overlaying a density plot on it. Violin plots show the density plot on both sides of the vertical line, but there is no difference between the two sides. In a sense, as with a box plot, we have two sides here purely to achieve a better look. Compared to the traditional box Adjacent line 4— Upper adjacent value Confidence interval Whiskers75th percentile Inter-quartile range MedianMedian Median Whiskers 4-— 25th percentile Adjacent line 4-— Lower adjacent values Confidence interval O outside values (a) Box plot (b) Violin plot Figure 3.8 The structure of the box plot and the violin plot 78 plot, the basic violin plot shows fewer statistics and does not show the extreme values. In exchange, it gives a better feel for the shape of the distribution. As there are many complementing features of box plots and violin plots, we advise you to consider both. Review Box 3.5 Summary statistics and their visualization • Measures of central value: Mean (average), median, other quantiles (percentiles), mode. • Measures of spread: Range, inter-quantile range, variance, standard deviation. • Measure of skewness: The mean-median difference. • The box plot is a visual representation of many quantiles and extreme values. • The violin plot mixes elements of a box plot and a density plot. in Football Distribution and summary statistics\
The idea of home team advantage is that teams that play on their home turf are more likely to play better and win compared to the same game played at the other team's stadium (the other team is also called the "away" team). In particular, this case study asks whether professional football (soccer) teams playing in their home stadium have an advantage and what is the extent of that advantage. These questions are interesting in order for fans to know what to expect from a game, but also for professional managers of football teams who want to maximize the number of wins and the support of fans. If home team advantage is important, managers need to know its magnitude to

benchmark the performance of their teams. Here we use data from the English Premier League, with the same data we started with in Chapter 2. Here we focus on games in the 2016/7 season. In each season, each pair of teams plays twice, once in the home stadium of one team, and once in the home stadium of the other team. This gives 380 games total (20 x 19: each of the 20 teams plays once at home against each of the other 19 teams). The observations in the data table we use here are the games; there are N = 380 observations. The most important variables are the names of the home team and the away team and the goals scored by each team. From these two quantitative variables we created a goal difference variable: home team goals — away team goals. Examining the distribution of this goal difference can directly answer our question of whether there is a home team advantage and how large it is. Let's look at the distribution of the home team — away team goal difference first. Figure 3.9 shows the histogram. While the goal difference is a quantitative variable, it doesn't have too many values so we show a histogram that shows the percentage of each value instead of bins. The mode is zero: 22.1 % of the games end with a zero goal difference (a draw). All other goal differences are of smaller percentage - the larger the difference, the smaller their frequency. This gives an approximately bell-shaped curve, except it is skewed with more observations (taller bars)

3.C1 Case Study 79

to the right of zero. That suggests home team advantage already. But let's look more closely into this histogram. The most striking feature of the histogram is that for each absolute goal difference, the positive value is more frequent than the negative value. The home team — away team goal difference is +1 in 20.8% of the games while It IS —1 in 13.2% of the games; it's +2 in 17.1% of the games and —2 in only 8.2% of the games, and so on. This clearly shows that home teams score more goals so there is a home team advantage. It also seems pretty large. But how large is it? To answer that we need to provide a number and put it into context.

Figure 3.9 The distribution of home team away team goal difference Source: football dataset. English Premier League, season 2016—2017, all games. N=380. To that end, we calculated the mean and the standard deviation of the goal difference as shown in Table 3.7. Moreover, we calculated the relative frequency of games in which the home team wins (positive goal difference), games in which the away team wins (negative goal difference), and games that end with a draw (zero goal difference: we know their proportion is 22%). Table 3.7 shows the results. Table 3.7 Statistic Value Mean 0.4 Standard deviation 1.9 Percent positive 49 380
