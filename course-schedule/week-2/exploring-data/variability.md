# Variability

"The mean of a quantitative variable is the value that we can expect for a randomly chosen observation. The mean of a 0/1 binary variable is the proportion of observations with a value of 1. Similarly, most readers know the most important **measures of spread**, such as the range (the difference between the largest and smallest value), inter-quantile ranges (e.g., the 90—10 percentile range, or the inter-quartile range), the standard deviation, and the variance. The standard deviation captures the typical difference between a randomly chosen observation and the mean. The variance is the square of the standard deviation. The variance is a less intuitive measure, but it is easier to work with because it is a mean value itself.&#x20;

Note that alternative formulae for the variance and the standard deviation divide by (n — 1) not (n). Most data are large enough that this makes no practical difference. It turns out that dividing by n — 1 is the correct formula if we use the statistic in the data to infer the standard deviation in the population that our data represents.&#x20;

The standard deviation is often used to re-calculate differences between values of a quantitative variable, in order to express those values relative to what a typical difference would be. In a formula, this amounts to dividing the difference by the standard deviation. Such measures are called standardized differences. A widely used standardized difference is from the mean value; it is called the standardized value of a variable or the z-score of the variable.&#x20;

While measures of central value (such as mean, median) and spread (such as range, standard deviation) are usually well known, summary statistics that measure skewness are less frequently used. At the same time skewness can be an important feature of a distribution, showing whether a few observations are responsible for much of the spread. Moreover, there is a very intuitive measure for skewness (which exists in a few variants). Recall that a distribution is skewed if it isn't symmetric. A distribution may be skewed in two ways, having a long left tail or having a long right tail. A long left tail means having a few observations with small values with most observations having larger values. A long right tail means having a few observations with large values with most observations having smaller values. Earlier we showed that the hotel price distributions has a long right tail — such as in Figure 3.4. That is quite typical: skewness with a long right tail is frequent among variables in business, economics, and policy, such as with prices, incomes, and population. The statistic of skewness compares the mean and the median and is called the mean—median measure of skewness. When the distribution is symmetric, its mean and median are the same. When it is skewed with a long right tail, the mean is larger than the median: the few very large values in the right tail tilt the mean further to the right. Conversely, when a distribution is skewed with a long left tail, the mean is smaller than the median: the few very small values in the left tail tilt the mean further to

the left. The mean-median measure of skewness captures this intuition. In order to make this measure comparable across various distributions, we use a standardized measure, dividing the difference by the standard deviation. (Sometimes this measure is multiplied by 3, and then it's called Pearson's second measure of skewness. Yet other times the difference is divided by the mean, median, or some other statistic.)"
