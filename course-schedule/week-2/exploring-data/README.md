---
description: Week 2, Lecture A2.1, 1500 words, 1 hour to complete
---

# Exploring Data



Frequencies and Probabilities Let's start with the most basic property of variables: what values they can take and how often they take each of those values. We first introduce the concept of frequency that makes sense from the viewpoint of the data we have. Then we generalize frequencies to the concept of probability that makes sense in more abstract settings. The absolute frequency, or count, of a value of a variable is simply the number of observations with that particular value in the data. The relative frequency is the frequency expressed in relative, or percentage, terms: the proportion of observations with that particular value among all observations in the data. If a variable has missing values, this proportion can be relative to all observations including the missing values or only to observations with non-missing values. Most often we express proportions excluding observations with missing values. When our goal is to check the data cleaning process, absolute frequencies including missing values is the better option. Probability is a general concept that is closely related to relative frequency. Probability is a measure of the likelihood of an event. An event is something that may or may not happen. In the context of data, an event is the occurrence of a particular value of a variable. For example, an event in a data table is that the manager of the firm is female. This event may occur various times in the data. The probability of the event is its relative frequency: the proportion of firms with a female manager among all firms in the data. Considering a more abstract example, whether there is pasta for lunch at the canteen today is an event, and the probability of pasta for lunch today is a measure of its likelihood. Probabilities are always between zero and one. We denote the probability of an event as P(event), so that 0 P(event) 1. Sometimes they are expressed in percentage terms so they are between 0% and 100%: 0% P(event) 100%. Considering a single event, it either happens or does not. These two are mutually exclusive: the probability of an event happening and it also not happening is zero. We denote an event not happening as Nevent so æevent) 0. Probabilities are more general than relative frequencies as they can describe events without data. However, with some creative thinking, we can often think of potential data that contains the event so that its probability is a proportion. Data on what's for lunch at our canteen for many days is an example. If we had such data, the frequency of the event (pasta served for lunch) would give its probability. But not always. Sometimes probabilities may be defined for events for which no data can be imagined. These include subjective probabilities that describe the degree of uncertainty an individual feels about events that may happen in the future but have no precedents. Examples include the probability that you, the reader, will like this textbook enough to keep it for future reference, or the probability that rising sea levels will flood the underground system of Singapore within 20 years. Abstract probabilities are interesting and important from various points of view. But data analysts work with actual data. For most of them, probabilities and frequencies are closely related. Thus, most data analysts tend to think of probabilities as relative frequencies in actual data or data they can imagine. Frequencies are summarized by distributions. The distribution of a variable gives the frequency of each value of the variable in the data, either in terms of absolute frequency (number of observations), or relative frequency (proportion or percent). The distribution of a variable completely describes the variable as it occurs in the data. It does so focusing on the variable itself, without considering the values of other variables in the data. It is good practice to examine distributions of all important variables as the first step of exploratory data analysis. The simplest and most popular way to visualize a distribution is the histogram. The histogram is a bar chart that shows the frequency (absolute or relative) of each value. The bars can be presented horizontally; that's more common in business presentations and when there are few bars. The more traditional presentation is vertical bars. For binary variables, the distribution is the frequency of the two possible values and thus the histogram consists of two bars. For variables that take on a few values, the histogram shows as many bars as the number of possible values. For variables with many potential values, showing bars for each value is usually uninformative. Instead, it's better to group the many values in fewer groupings or bins. The histogram with binned variables shows bars with the number or percentage of observations within each bin. It is good practice to create bins of equal width so each bin covers the same range of values. As the case study will illustrate, the size of bins can have important consequences for how histograms look and how much they reveal about the properties of distributions. For any histogram, we need to decide on the bin size, either by setting the number of bins or the bin width. (Letting our statistical software set the bin size is also one such decision.) Very wide bins may lump together multiple modes. But very narrow bins may show a lot of ups and downs

for random reasons and thus can blur important properties of a distribution. It is good practice to experiment with a few alternative bin sizes to make sure that important features of the distribution don't remain hidden. Visual inspection of a histogram reveals many important properties of a distribution. It can inform us about the number and location of modes: these are the peaks in the distribution that stand out from their immediate neighborhood. Most distributions with many values have a center and tails, and the histogram shows the approximate regions for the center and the tails. Some distributions are more symmetric than others. Asymmetric distributions, also called skewed distributions, have a long left tail or a long right tail; histograms visualize those. Histograms also show if there are extreme values in a distribution: values that are very different from the rest. We'll discuss extreme values in detail in Section 3.5. Density plots — also called kernel density estimates — are an alternative to histograms for variables with many values. Instead of bars, density plots show continuous curves. We may think of them as curves that wrap around the corresponding histograms. Similarly to histograms, there are details to set for density plots, and those details may make them look very different. The most important detail to specify is the bandwidth, the closest thing to bin size for histograms. Density plots are less precise than histograms, and hence we advise you to rely on histograms when possible.

Review Box 3.3 Distributions and their visualization

A distribution of a variable shows the frequency, absolute or relative, of each potential value of the variable in the data. 3.A1 The histogram is a bar graph showing the frequency of each value of a variable if the variable has few potential values. Density plots (also known as kernel density estimates) are continuous curves; they can be viewed as wrapped around corresponding histograms. CASE STUDY — Finding a Good Deal among Hotels: Data Exploration Describing distributions The broader question on hotels we will be discussing in chapters to come is how to find a good deal among hotels. Here, we explore the most important variables we'll use in that analysis, with a focus on location — asking where hotels are located and what is the share of hotels close to the city center. Describing the distribution of hotels in quality or distance can help business decisions on hotel development. This case study uses the hotels-vienna dataset. We introduced this data in Chapter 1, Section 1 .AI . We will keep focusing on the data from Vienna for a particular night. The data we work with contains proper hotels only, without apartment houses, and so on. We have N = 264 hotels. Let us start with stars, a measure of hotel quality. Stars are determined by the services hotels offer, according to detailed criteria. Hotel stars can take on a few distinct values; thus a histogram becomes a set of bars — one for each value. Case Study

Figures 3. la and 3.1b show the histogram of stars in our data. The horizontal axis shows the potential values of this variable such as 3 stars, 4 stars, and also 3.5 stars (3 stars plus). In Figure 3.1b, the vertical axis shows absolute frequencies: the number of observations corresponding to each value. In Figure 3.1 a, we show relative frequency (percent). According to the histogram, there are 88 hotels with 3 stars, 14 with 3.5 stars, and 116 hotels with 4 stars. In relative terms this means 33% (88/264 = 0.33) with 3 stars, 5% with 3.5 stars, and 44% with 4 stars. Indeed, most properties are either 3 or 4 star hotels.

*   1.0 1.5 - 2.0 - 2.5 - 3.0 3.5 4.0 4.5 5.0 Star rating (N. stars) Star rating (N. stars) (a) Relative frequency (percent) (b) Absolute frequency (count) Figure 3.1 Histogram of hotel stars Note: Histogram for a qualitative variable. Source: hotels-vienna dataset. Vienna, hotels only, for a November 2017 weekday. N=264. From now on we will focus on proper hotels and the mid-quality segment: our data will consist of hotels in Vienna with three to four stars. This is a subset of the hotels-vienna dataset described in Chapter 1, excluding non-hotel accommodation, hotels with fewer than three stars or more than four stars. We have N = 218 such hotels in Vienna. The next variable we explore is hotel room price. The distribution of prices may be visualized in multiple ways. In this data, prices range from 50 dollars to 383 dollars plus a single hotel offering a room for 1012 dollars. Let us disregard that extreme value for the rest of the section; we'll come back to that decision soon. We will work with N 217 observations in this section. A histogram with a bar for each value is not a particularly informative visualization, but we show it for educational purposes in Figure 3.2a. Most values occur once, with some values (like 1 10 dollars) occurring several times. This histogram shows more bars, thus more hotels, with room price below 200 dollars, with most values between 80 and 150 dollars. Figure 3.2b shows a histogram with 10-dollar bins. This graph is far more informative. Most of the distribution is between 60 and 180 dollars, with few observations above 180 dollars. It suggests a distribution with a mode at 80 dollars. The distribution is skewed, with a long right tail. A long right tail means that there are several values on the right end of the histogram that are a lot further away from its center than the values on the left end of the histogram. There is one extreme value at 380 dollars. Price (US dollars) Price (US dollars) (a) Each value separately (b) 10-dollar wide bins Figure 3.2 Histogram of hotel price Note: Panel (a): bars at each value; panel (b): bars for 1 0-dollar bins; excluding extreme value. Source: hotels-vienna dataset. Vienna, 3-4 stars hotels only, for a November 2017 weekday. N=217. We can increase bin width further, lumping together more values in a bin and thus reducing the number of bars. Figure 3.3 shows histograms of the same price distribution with wider bins, first of 40 dollars, second of 80 dollars. The wider the bin, the fewer details of the distribution are revealed by the histogram. All show the same long right tail, but the 80-dollar bin puts the mode in the leftmost bin, whereas it was in the second bin earlier. The histograms with the wider bins suggest a more gradual decline in frequency.

    (a) 40-dollar bins (b) 80-dollar bins Figure 3.3 The distribution of hotel price: Histograms with wide bins Note: Panel (a): 40-dollar bins; panel (b): 80-dollar bins; excluding extreme value. Source: hotels-vienna dataset. Vienna, 3-4 stars hotels only, for a November 2017 weekday. N=217.

Extreme Values

These graphs (Figures 3.2a, 3.2b, 3.3a, and 3.3b) taken together suggest a trade-off. Narrower bins give you a more detailed view but make it harder to focus on the really important properties of the distribution. Finding a good compromise may depend on the purpose. To design further analytic work, a 20-dollar bin is very useful. For presenting to a more generalist audience, a 40-dollar bin may be better. Designing bin size is a necessary task, and requires practice. Some quantitative variables have extreme values: substantially larger or smaller values for one or a handful of observations than the rest. Sometimes extreme values don't belong in the distribution because they are errors. Most frequently, such errors are due to mistakes in digits or units of measurement, such as company revenues in dollars instead of millions of dollars or number of visits to a supermarket per year instead of per week. In addition, extreme values may not belong in the distribution because they represent patterns that we aren't interested in. But other times extreme values are an integral part of the distribution. Indeed, they may be the most important observations. For example, in investment decisions extreme future returns and their frequencies are among the most important things to know as they may lead to huge losses or huge gains. The overall gain or loss of such an investment may be determined by a few extreme events as opposed to the cumulative result of many small gains or losses. Similarly, when the question is about what damages to expect from floods or earthquakes, it's the largest of damages that we should care about. Some data analysts call extreme values outliers. We prefer not using that term as it implies that they do not belong in the distribution, which may or may not be the case. The most worrying problem with extreme values is that they may not show up in a dataset even if they are an inherent characteristic of a variable. Extreme values, by nature, are indeed rare. For example, data spanning a few years may not have instances of the most extreme losses on an asset that may come in the future. When extreme values are included in our data, we need to identify them. Visualizing the distribution via a histogram is a good way to catch extreme values. What to do, if anything, with extreme values is a more difficult question. It depends both on why extreme values occur and what the ultimate question of the analysis is. Extreme values that occur due to error should be replaced by a missing value marker or, in the rare case when one can infer the correct value, by that correct value. Error-caused extreme values are rare but are often straightforward to catch. For example, earnings a thousand times higher than the average in low-skilled occupations are surely an error and so are more than 168 working hours a week. More often, extreme values are an inherent part of the distribution. The size of the largest countries or firms, the salaries of chief executives in a firm, the best-paid actors among all actors, or a large drop in asset prices on the day of a stock-market crash are not errors in the data. What we do in such cases depends on the question of our analysis. If the question is more relevant for the rest of the observations, we may discard the observations with extreme values and restrict the analysis to the rest. When we look into how salaries and other incentives may keep employees at a firm, it makes sense not to focus on chief executives but restrict the analysis to the other employees. When we want to know how distance to main highways affects the price of commercial real estate, we may discard the largest cities and focus on smaller towns. It is good practice to be explicit about such decisions when presenting the results of the analysis, saying that the analysis is relevant for some kinds of observations but not for others. In fact, what, if anything, we should do with observations with extreme values of a variable depends also on the role of the variable in our analysis. Starting with Chapter 4, Section 3.U1 , we will distinguish a y variable and one or more x variables in the analysis. Typically, our analysis will aim to uncover patterns in how values of y tend to differ for observations that have different values of x. hotel price differences by how far they are from the city center, differences in the quality of management by firm size, and so on. Data analysts tend to be conservative when discarding observations with extreme y values: they usually keep them unless they are errors. However, data analysts tend to be less conservative when discarding observations with extreme x values: they often discard them even if they are parts of the distribution. The reason is in the different roles y and x play in the analysis. Discarding observations with extreme x values narrows the scope of the comparisons. That's a transparent decision, because it defines what kinds of comparisons we want to make. In contrast, discarding observations with extreme y values changes the result of the comparisons. The consequences of this decision are not straightforward, and it's often safer to avoid those consequences. We'll discuss this issue in more detail in Chapter 8, Section 7.U1.

Review Box 3.4 Extreme values • Some variables have extreme values: substantially larger or smaller values for a few observations than the values for the rest of the observations. • Extreme values may be genuine or they may be an error. • When errors, extreme values should be corrected or treated as missing values. • When genuine, extreme values should be kept, unless we have a substantive reason to restrict the analysis to the subset of observations without extreme values. CASE STUDY - Finding a Good Deal among Hotels: Data Exploration Identifying and managing extreme values Both price and distance to the city center have extreme values in the hotels-vienna dataset. In subsequent chapters, we will use this data to find hotels that are underpriced for their location and quality. Hotel price will be our yvariable, and so we drop extreme values only if they are errors. In contrast, distance will be an x variable, so we can drop extreme values even if they are parts of the distribution if we want to narrow our analysis. We first look at distance. The data here includes all hotels in Vienna with 3 to 4 stars, regardless of their distance to the city center. Figure 3.4 shows the histogram of distance.

3.A2 Case Study

Distance to city center (miles) Figure 3.4 Histogram of distance to the city center. Source: hotels-vienna dataset. Vienna, all hotels with 3 to 4 stars. N=217. For this histogram we used 0.5-mile-wide bins. This way we can see the extreme values in more detail even though the rest of the histogram looks a bit less nice (too uneven). The y axis shows the frequency. The histogram shows three hotels above 8 miles: two at around 1 1 miles and one at close to 1 3 miles. We see another group of hotels between 6 and 8 miles that are a little separated from the rest of the distribution (no value between 8 and 1 1 miles). We decided to drop the three hotels that are more than 8 miles away from the city center and keep the ones at 6 to 8 miles. The extreme values we dropped are not errors. But they are values that would not be relevant for the main question of the analysis: finding hotels that are underpriced relative to their distance to the city center (and their quality). Eleven and 13 miles are far enough from the city center that we think we wouldn't choose these hotels if our aim was to stay in Vienna to explore the city. At the same time we didn't discard the hotels at 6 to 8 miles, thinking that maybe some of them are so inexpensive that they could be good deals even factoring in their distance. Note that this decision is arbitrary, but one such decision is necessary. In a thorough analysis, we would see if including the 8+ miles hotels, or excluding the 6—8 miles hotels, changes the answer to our question. To better understand the features of hotels far from the center, we investigated our "city\_actual" variable. It turns out that even within the 8-mile radius, a few hotels are in villages (such as Voesendorf) that are related to Vienna but are not Vienna proper. Hence, we decided to drop these hotels, too. The result is a sample of 208 hotels. Next we looked at prices, using the data of the 208 hotels that are within 8 miles from the city center and are in Vienna Earlier, we pointed out a single observation with a price of 1012 dollars. This is an extreme value indeed. We dropped it because we decided that it is almost surely an error. It's a room in a 3 star hotel and is unlikely to cost over a thousand dollars. Moreover, in the hotels-europe dataset that contains prices for several dates for these same hotels, and many more, the price of this hotel is around 100 dollars on all other dates not 1000 dollars. We have identified, in Figure 3.2b, an additional observation with an extreme value, close to 400 dollars. We decided to keep this observation. This is a high price for this kind of a hotel (3 stars). At the same time, inspecting the hotels-europe dataset reveals another date with a similarly high

price, and the prices on the rest of the dates, while considerably lower, are not lower by an order of magnitude that would indicate a digit error. Thus, we can't conclude that this extreme value is an error. To summarize, our exploratory analysis led us to focus our sample. Our key steps were:

1. We started with full data, N = 428.
2. We inspected the histograms of the qualitative variables. • Accommodation type — could be apartment, house and so on; kept hotels only, N = 264. • Stars — kept only: 3, 3.5, 4 stars, N = 218.
3. We looked at quantitative variables, focusing on extreme values. • Price: the extreme value of 1012 dollars is a likely error, dropped it; kept all others, N= 217. • Distance: some hotels are far away; defined cutoff; dropped beyond 8 miles, 214. • Distance, one more step: looked at variable city\_actual, realized that some hotels are not in Vienna proper; dropped them, N = 207.
4. The final sample is hotels with 3 to 4 stars, below 400 dollars, less than 8 miles from center, in Vienna proper, N = 207. Good Graphs: Guidelines for Data Visualization Now that we have introduced visualization of distributions, let's pause and spend some time on how to produce good graphs in general. These thoughts are meant to guide all decisions that go into producing graphs. They correspond to the practice of data visualization professionals; see some important references at the end of the chapter. Before we begin, let us point out that our principles and suggestions are aimed at data analysts not visual designers. Typically, data analysts want to spend less time designing graphs than visual designers. As a result, they are more likely to use ready-made graph types and templates, and they benefit more from following a few simple rules instead of engaging in a creative process each and every time. Just like with any of our advice, this is not a must do list. Instead it shows how to think about decisions data analysts must take. You may take other decisions, of course. But those decisions should be conscious instead of letting default settings determine the look of your graphs. The starting principle is that all of our decisions should be guided by the usage of the graph. The usage of a graph is a summary concept to capture what we want to show and to whom. Its main elements are purpose, focus, and audience. Table 3.1 explains these concepts and gives some examples. Note that some of the examples use graphs that we haven't introduced yet; this is because we want the advice to serve as reference later on. Once usage is clear, the first set of decisions to make are about how we convey information: how to show what we want to show. For those decisions it is helpful to understand the entire graph as the overlay of three graphical objects: 1 . Geometric object: the geometric visualization of the information we want to convey, such as a set of bars, a set of points, a line; multiple geometric objects may be combined.
5. Scaffolding: elements that support understanding the geometric object, such as axes, labels, and legends.
6. Annotation: adding anything else to emphasize specific values or explain more detail. 3.5 Good Graphs: Guidelines for Data Visualization Table 3.1 Usage of a graph Concept Explanation Typical cases Examples Purpose Focus Audience The message that the graph should convey One graph, one message To whom the graph wants to convey its message Main conclusion of the analysis An important feature of the data Documenting many features of a variable Multiple related graphs for multiple messages Wide audience Non-data-analysts with domain knowledge Analysts y and x are positively associated There are extreme values of y at the right tail of the distribution All potentially important properties of the distribution of y A histogram of ythat identifies extreme values, plus a box plot of y that summarizes many other features of its distribution Journalists Decision makers Fellow data analysts, or our future selves when we want to reproduce the analysis When we design a graph, there are many decisions to make. In particular, we need to decide how the information is conveyed: we need to choose a geometric object, which is the main object of our graph that visualizes the information we want to show. The geometric object is often abbreviated as a geom. The same information may be conveyed with the help of different geometric objects, such as bar charts for a histogram or a curved line for a density plot. In practice, a graph may contain more than one geometric object, such as a set of points together with a line, or multiple lines. Choosing the details of the geometric object, or objects, is called encoding. Encoding is about how we convey the information we want using the data we have, and it means choosing elements such as height, position, color shade. For a graph with a set of bars as the geometric object, the information may be encoded in the height of these bars. But we need to make additional choices, too. These include general ones such as color and shade as well as choices specific to the geometric object, such as width of the bars or lines, or size of the dots. In principle, a graph can be built up freely from all kinds of graphical objects. Data visualization experts and designers tend to follow this bottom-up approach. In contrast, most data analysts start with choosing a predefined type of graph: a single geometric object or a set of some geometric objects and a scaffolding, possibly some annotation. One graph type is the histogram that we introduced earlier in this chapter. Histograms are made of a set of bars as a geometric object, with the information in the data (frequency) encoded in the height of the ban The scaffolding includes an x axis with information on the bins, and a y axis denoting either the absolute (count) or relative frequency (percent). We'll introduce many more graph types in this chapter and subsequent chapters of the textbook. Table 3.2 offers some details and advice. The next step is scaffolding: deciding on the supporting features of the graph such as axes, labels, and titles. This decision includes content as well as format, such as font type and size. Table 3.3 summarizes and explains the most important elements of scaffolding. Table 3.2 The geometric object, its encoding, and graph types Concept General advice Examples Geometric object One or more geoms Encoding Graph type Pick an object suitable for the information to be conveyed May combine more than one geom to support message or add context Pick one encoding only Can pick a standard object to convey information Set of bars comparing quantity A line showing value over time Dots for the values of a time series variable over time, together with a trend line Histogram: height of bars encodes information (frequency) Don't apply different colors or shades Histogram: bars to show frequency Scatterplot: values of two variables shown as a set of dots Table 3.3 Scaffolding Element General advice Examples Graph title Title should be part of the text; it Swimming pool ticket sales and temperature should be short emphasizing main message Swimming pool sales fluctuate with weather Axis title Each axis should have a title, with the Distance to city center (miles) name of the actual variable and unit of measurement Household income (thousand US dollars) Axis labels Value numbers on each axis, next to 0, 2, 4, 6, 8, 10, 12 14 miles for distance to city tics, should be easy to read center Gridlines Add horizontal and, if applicable, Vertical gridlines for histogram vertical gridlines to help reading off Both horizontal and vertical gridlines for numbers scatterplots Legends Add legend to explain different Two groups, such as "male" and "female" elements of the geometric object Time series graphs for two variables; it's best to Legends are best if next to the element they explain put legends next to each line Fonts Large enough size so the audience can read them Font size "10"

Lastly, we may add annotation — if there is something else we want to add or emphasize. Such additional information can help put the graph into context, emphasize some part of it. The two main elements of annotation are notes and visual emphasis, see Table 3.4 for some advice.

3.A3 Case Study Table 3.4 Annotation Concept General advice Examples Graph notes Added emphasis Add notes to describe all important details about how the graph was produced and using what data May add extra annotation to graph to emphasize main message Lowess non-parametric regression with scatterplot Hotels-vienna dataset. Vienna, all hotels with 3 to 4 stars. N=217 A vertical line or circle showing extreme values on a histogram An arrow pointing to a specific observation on a scatterplot CASE STUDY — Finding a Good Deal among Hotels: Data Exploration The anatomy of a graph Let us use a previous graph here, to illustrate the most important parts of a good graph. Recall that we should should keep in mind usage, encoding, scaffolding, annotation. We use a previous graph with the histogram of hotel distance to the city center (Figure 3.4), but here we added some annotation (Figure 3.5). Usage. We use this graph to search for extreme values and document them. The main message is that the three hotels that are located more than 8 miles away from the city center are separate from the rest. The target audience is a specialized one: fellow data analysts. The figure may serve to document the reasons of our decisions, again a special usage. Encoding. The graph shows the distribution in enough detail to spot extreme values, but it also shows the main part of the distribution to put those extreme values in context. Our encoding choice was a histogram with a 0.5-mile bin. Alternative choices would have been a density plot or wider or narrower bins for the histogram. Choosing the histogram with a 0.5-mile bin led to a balance for the usage of the graph: showing the main part distribution and the extreme values. One message, one encoding: we use a single color as bar height is enough to help compare through distance bins. Scaffolding. The x axis denotes distance to city center Although bins are every 0.5 mile, the labels are at 2-mile intervals to help readability. The y axis denotes the number of hotels per bin. It is absolute frequency here not percentage, because our focus is on counting observations with extreme values. Notice the labels on the yaxis: they are in increments of 10. The scaffolding includes horizontal and vertical gridlines to help reading off numbers. Annotation. We point out the main message of the graph: the three hotels beyond 8 miles from the center appear to form their own cluster. We used a colored rectangle, but we could have circled them, or had an arrow pointed at them. For a scientific audience we could have skipped that annotation because that audience would understand the issue anyway and may appreciate a clean histogram.

Figure 3.5 Histogram of distance to the city center Source: hotels-vienna dataset. Vienna, all hotels with 3 to 4 stars. N=217. A histogram of a quantitative variable can inform us about the shape of the distribution, whether it has extreme values, or where its center is approximately. But visual displays don't produce the numbers that are often important to answer our questions. How far are hotels from the city center in general? What's the spread of prices? How skewed is the distribution of customer ratings? To answer such questions we need numerical summaries of variables. They are called statistics, and this section covers the most important ones. A statistic of a variable is a meaningful number that we can compute from the data. Examples include mean income or the range of prices. Basic summary statistics are numbers that describe the most important features of the distribution of a variable. Summary statistics can answer questions about variables in our data, and they often lead to further questions to examine. Most readers are probably well acquainted with many summary statistics, including the mean, the median (the middle value), various quantiles (terciles, quartiles, percentiles), and the mode (the value with the highest frequency in the data). The mean, median, and mode are also called measures of central tendency, because they give an answer to where the center of the distribution is. Those answers may be the same (the mean, median, and mode may be equal, or very close to each other), or they may be different. Importantly, we use the terms mean, average, and expected value as synonyms, and we use the notation E\[x] as well ask. 3.6 Summary Statistics for Quantitative Variables

The mean of a quantitative variable is the value that we can expect for a randomly chosen observation. The mean of a 0/1 binary variable is the proportion of observations with value 1. No observation would have the expected value as it is between 0 and 1 since the value of the binary variable can be only 0 or 1. Similarly, most readers know the most important measures of spread, such as the range (the difference between the largest and smallest value), inter-quantile ranges (e.g., the 90—10 percentile range, or the inter-quartile range), the standard deviation, and the variance. The standard deviation captures the typical difference between a randomly chosen observation and the mean. The variance is the square of the standard deviation. The variance is a less intuitive measure, but it is easier to work with because it is a mean value itself. The formulae are: Var\[x] (3.1) n Std\[x] (3.2) Note that alternative formulae for the variance and the standard deviation divide by n — 1 not n. Most data are large enough that this makes no practical difference. It turns out that dividing by n — 1 is the correct formula if we use the statistic in the data to infer the standard deviation in the population that our data represents (see more details in Chapter 5, Section 5.12). Since it makes little difference in practice, and dividing by n is easier to remember, we will continue to divide by n in this textbook. The standard deviation is often used to re-calculate differences between values of a quantitative variable, in order to express those values relative to what a typical difference would be. In a formula, this amounts to dividing the difference by the standard deviation. Such measures are called standardized differences. A widely used standardized difference is from the mean value; it is called the standardized value of a variable or the z-score of the variable. Xstandardized — (3.3) Std\[x] While measures of central value (such as mean, median) and spread (such as range, standard deviation) are usually well known, summary statistics that measure skewness are less frequently used. At the same time skewness can be an important feature of a distribution, showing whether a few observations are responsible for much of the spread. Moreover, there is a very intuitive measure for skewness (which exists in a few variants). Recall that a distribution is skewed if it isn't symmetric. A distribution may be skewed in two ways, having a long left tail or having a long right tail. A long left tail means having a few observations with small values with most observations having larger values. A long right tail means having a few observations with large values with most observations having smaller values. Earlier we showed that the hotel price distributions has a long right tail — such as in Figure 3.4. That is quite typical: skewness with a long right tail is frequent among variables in business, economics, and policy, such as with prices, incomes, and population. The statistic of skewness compares the mean and the median and is called the mean—median measure of skewness. When the distribution is symmetric, its mean and median are the same. When it is skewed with a long right tail, the mean is larger than the median: the few very large values in the right tail tilt the mean further to the right. Conversely, when a distribution is skewed with a long left tail, the mean is smaller than the median: the few very small values in the left tail tilt the mean further to

the left. The mean-median measure of skewness captures this intuition. In order to make this measure comparable across various distributions, we use a standardized measure, dividing the difference by the standard deviation. (Sometimes this measure is multiplied by 3, and then it's called Pearson's second measure of skewness. Yet other times the difference is divided by the mean, median, or some other statistic.) Skewness = c— median\[x] (3.4) Std\[x] Table 3.5 summarizes the most important descriptive statistics we discussed.

```
Type of statistic Name of statistic	Formula	Intuitive content
```

The value we expect chosen observation Median The value of the observation in the middle Mode The value (bin) with the highest frequency Spread Range max\[x] — min\[x] Width of the interval of possible values Inter-quantile range qupper \[X] — mower X Distance between the upper quantile and the lower quantile Variance Var\[x] Standard deviation Std\[x]Typical distance between observations and the mean Skewness Mean—median E— Skewness = median (x) Std\[xl The extent to which values in the tail skewness pull the mean

CASE STUDY — Comparing Hotel Prices in Europe: Vienna vs. London Comparing distributions over two groups We are interested in comparing the hotel markets over Europe, and would like to learn about characteristics of hotel prices. To do that, let us focus on comparing the distribution of prices in Vienna to another city, London. The data we use is an extended version of the dataset we used so far. The hotels-europe dataset includes the same information we saw, for 46 cities and 10 different dates. We will explore it more in Chapter 9. For this case study, we consider the same date as earlier (weekday in November 2017) for Vienna and London.

3.B1 Case Study

We focus on hotels with 3 to 4 stars that are in the actual city of Vienna or London. We have no extreme value of price in London, so we need to drop the single, above 1000 dollars priced hotel in Vienna. In our sample, there are N 435 hotels in the London dataset compared to the N = 207 hotels in Vienna. Figure 3.6 shows two histograms side by side. To make them comparable, they have the same bin size (20 dollars), the same range of axes, and each histogram shows relative frequencies. The same range of axes means that the x axis goes up to 500 dollars both Vienna and London because the maximum price is close to 500 dollars in London. The histograms reveal many important features. Here is a selected set of observations we can make: The range is around 50 dollars in both cities but it ends below 400 dollars in Vienna while it goes close to 500 dollars in London. The London distribution of prices covers more of the higher values, and it is more spread out (i.e. has a larger difference between the minimum and maximum price). Hotel prices tend to be higher in London. Both distributions have a single mode, but their location differs. The bin with the highest frequency (the mode) in Vienna is the 80-100-dollar bin, and it is the 120—140-dollar bin in London. 300 Price (US dollars) Price (US dollars) (a) Vienna (b) London Figure 3.6 The distribution of hotel price in Vienna and London Source: hotels-europe dataset. Vienna and London, 3-4 stars hotels only, for a November 2017 weekday. Vienna: N=207, London: N=435. The same price distributions can be visualized with the help of density plots. Figure 3.7 shows the Vienna and London distributions laid on top of each other The density plots do not convey more information than the histograms. In fact, they are less specific in showing the exact range or the prevalence of extreme values. But comparing density plots on a single graph is just easier -- we can see immediately where the mass of hotels are in Vienna and in London.

Price (US dollars) Figure 3.7 Density plots of hotel prices: Vienna and London Note: Kernel density estimates with Epanechnikov smoothing method. Source: hotels-europe dataset, Vienna and London, 3—4 stars hotels only, for a November 2017 weekday. Vienna: N=207, London: N=435. We can quantify some aspects of the distributions, too. Table 3.6 contains some important summary statistics in the two datasets. Table 3.6 Descriptive statistics for hotel prices in two cities City N Mean Median Min Max Std Skew

```
London 435 202.36	186	49	491	88.13	0.186
Vienna	207 109.98	100	50	383	42.22	0.236
```

Source: hotels-europe dataset. Vienna and London, November 2017, weekday. Average price is 1 10 dollars in Vienna and 202 dollars in London. The difference is 92 dollars, which is almost an extra 90 % relative to the Vienna average. The mean is higher than the median in both cities, indicating a somewhat skewed distribution with a long right tail. Indeed, we can calculate the standardized mean—median measures of skewness, and it is more positive in Vienna ((1 10 100)/42 0.236) than in London ((202 - 1 86) / 88 O. 186). The range of prices is substantially wider in London (491 — 49 442) than in Vienna (383 — 50 — 343), and the standard deviation shows a substantially larger spread in London (88 versus 42). The first column shows that the London dataset has about twice as many observations (435 versus 207). These summary statistics are in line with the conclusions we drew by inspecting the visualized distributions. Hotel prices in London tend to be substantially higher on average. They are also more spread, with a minimum close to the Vienna minimum, but many hotels above 200 dollars. These together imply that there are many hotels in London with a price comparable to hotel prices in Vienna, but there are also many hotels with substantially higher prices. 3.7 Visualizing Summary Statistics Visualizing Summary Statistics The summary statistics we discussed are often presented in table format. But there are creative ways to combine some of them in graphs. We consider two such graphs. The more traditional visualization is the box plot, also called the box and whiskers plot shown in Figure 3.8a. The box plot is really a one-dimensional vertical graph, only it is shown with some width so it looks better. The center of a box plot is a horizontal line at the median value of the variable, placed within a box. The upper side of the box is the third quartile (the 75th percentile) and the lower side is the first quartile (the 25th percentile). Vertical line segments on both the upper and lower side of the box capture most of the rest of the distribution. The ends of these line segments are usually drawn at 1.5 times the inter-quartile range added to the third quartile and subtracted from the first quartile. These endpoints are called adjacent values in the box plot. The lines between the lower (upper) adjacent value and the 25th (75th) percentile range are called whiskers. Observations with values not contained within those values are usually added to the box plot as dots with their respective values. The box plot conveys many important features of distributions such as their skewness and shows some of the quantiles in an explicit way. A smarter-looking alternative is the violin plot shown in Figure 3.8b. In essence, the violin plot adds a twist to the box plot by overlaying a density plot on it. Violin plots show the density plot on both sides of the vertical line, but there is no difference between the two sides. In a sense, as with a box plot, we have two sides here purely to achieve a better look. Compared to the traditional box Adjacent line 4— Upper adjacent value Confidence interval Whiskers75th percentile Inter-quartile range MedianMedian Median Whiskers 4-— 25th percentile Adjacent line 4-— Lower adjacent values Confidence interval O outside values (a) Box plot (b) Violin plot Figure 3.8 The structure of the box plot and the violin plot 78 plot, the basic violin plot shows fewer statistics and does not show the extreme values. In exchange, it gives a better feel for the shape of the distribution. As there are many complementing features of box plots and violin plots, we advise you to consider both. Review Box 3.5 Summary statistics and their visualization • Measures of central value: Mean (average), median, other quantiles (percentiles), mode. • Measures of spread: Range, inter-quantile range, variance, standard deviation. • Measure of skewness: The mean-median difference. • The box plot is a visual representation of many quantiles and extreme values. • The violin plot mixes elements of a box plot and a density plot. in Football Distribution and summary statistics\
The idea of home team advantage is that teams that play on their home turf are more likely to play better and win compared to the same game played at the other team's stadium (the other team is also called the "away" team). In particular, this case study asks whether professional football (soccer) teams playing in their home stadium have an advantage and what is the extent of that advantage. These questions are interesting in order for fans to know what to expect from a game, but also for professional managers of football teams who want to maximize the number of wins and the support of fans. If home team advantage is important, managers need to know its magnitude to

benchmark the performance of their teams. Here we use data from the English Premier League, with the same data we started with in Chapter 2. Here we focus on games in the 2016/7 season. In each season, each pair of teams plays twice, once in the home stadium of one team, and once in the home stadium of the other team. This gives 380 games total (20 x 19: each of the 20 teams plays once at home against each of the other 19 teams). The observations in the data table we use here are the games; there are N = 380 observations. The most important variables are the names of the home team and the away team and the goals scored by each team. From these two quantitative variables we created a goal difference variable: home team goals — away team goals. Examining the distribution of this goal difference can directly answer our question of whether there is a home team advantage and how large it is. Let's look at the distribution of the home team — away team goal difference first. Figure 3.9 shows the histogram. While the goal difference is a quantitative variable, it doesn't have too many values so we show a histogram that shows the percentage of each value instead of bins. The mode is zero: 22.1 % of the games end with a zero goal difference (a draw). All other goal differences are of smaller percentage - the larger the difference, the smaller their frequency. This gives an approximately bell-shaped curve, except it is skewed with more observations (taller bars)

3.C1 Case Study 79

to the right of zero. That suggests home team advantage already. But let's look more closely into this histogram. The most striking feature of the histogram is that for each absolute goal difference, the positive value is more frequent than the negative value. The home team — away team goal difference is +1 in 20.8% of the games while It IS —1 in 13.2% of the games; it's +2 in 17.1% of the games and —2 in only 8.2% of the games, and so on. This clearly shows that home teams score more goals so there is a home team advantage. It also seems pretty large. But how large is it? To answer that we need to provide a number and put it into context.

Figure 3.9 The distribution of home team away team goal difference Source: football dataset. English Premier League, season 2016—2017, all games. N=380. To that end, we calculated the mean and the standard deviation of the goal difference as shown in Table 3.7. Moreover, we calculated the relative frequency of games in which the home team wins (positive goal difference), games in which the away team wins (negative goal difference), and games that end with a draw (zero goal difference: we know their proportion is 22%). Table 3.7 shows the results. Table 3.7 Statistic Value Mean 0.4 Standard deviation 1.9 Percent positive 49 380
