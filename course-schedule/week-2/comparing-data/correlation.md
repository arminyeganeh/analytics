# Correlation

The formula for the covariance between two variables x and y in a dataset with n observations is:&#x20;

The correlation coefficient divides this by the product of the two standard deviations:&#x20;

The covariance may be any positive or negative number, while the correlation coefficient is bound to be between negative one and positive one. But their sign is always the same. With positive correlation, y and x are positively correlated. With negative covariance and correlation we say that y and x are negatively correlated. The magnitude of the correlation coefficient shows the strength of the association: the larger the magnitude, the stronger the mean-dependence between the two variables. Data analysts tend to consider a correlation of 0.2 or less (in absolute value) weak, and a correlation above 0.7 in absolute value is usually considered strong. If two variables are independent, they are also mean-independent and thus the conditional expectations are all the same: E\[ylx]=E\[y] for any value of x. The covariance and the correlation coefficient are both zero in this case. In short, we say that independence implies mean independence and zero correlation. But the reverse is not true. We can have zero correlation but mean-dependence (e.g., a symmetrical U-shaped conditional expectation has an average of zero), and we can have zero correlation and zero mean-dependence without complete independence (e.g., the spread of y may be different for different values of x).&#x20;

Spending a little time with its formula can help understand how the covariance is an average measure of mean dependence. The product within the sum in the numerator multiplies the deviation of x from its mean (Xi — R) with the deviation of y from its mean (Yi — y), for each observation i. The entire formula is the average of these products across all observations. If a positive deviation of x from its mean goes along with a positive deviation of y from its mean, the product is positive. Thus, the average of this product across all observations is positive. The more often a positive — \* goes together with a positive — y, the more positive is the covariance. Or, the larger are the positive deviations that go together the larger the covariance. If, on the other hand, a positive — \* goes along with a negative — y, the product tends to be negative. Thus the average of this product is negative. The more often a positive — goes together with a negative — y, the more negative the covariance. Finally, if a positive — \* goes along with a positive — y some of the time and a negative — y at other times, and these two balance each other out, the positive values of the product and the negative values of the product cancel out. In this case the average is zero. Exact zero covariance rarely happens in real data because that would require an exact balancing out. The more balanced the positive deviation in and positive deviation in instances are with the positive deviation in x, and negative deviation in instances, the closer the covariance is to zero. Also note that the formulae of the covariance or the correlation coefficient allow for all kinds of variables, including binary variables and ordered qualitative variables as well as quantitative variables. The covariance and the correlation coefficient will always be zero if the two variables are meanindependent, positive if positively mean-dependent, and negative if negatively mean-dependent. Thus, they give a quick and not completely meaningless picture about mean-dependence among binary and ordered qualitative variables. However, they are more appropriate measures for qualitative variables. That's because the differences Y — y and & — make less sense when y and x are qualitative variables. For that reason, data analysts use other correlation-like measures for qualitative variables, but those measures are beyond the scope of our textbook.
