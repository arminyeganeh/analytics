# Variation Sources

Sources of Variation in x Our final section in this chapter is a note on variation in x, the variable (or variables) we condition on to make comparisons in y. The first thing to note is that we need variation in x, and the more variation we have the better in general. In data with no variation in x, all observations have the same values and it's impossible to make comparisons. This may sound trivial, but it's essential to keep in mind. Similarly, the more variation in x, the better the chances for comparison. For example, when data analysts want to uncover the effect of price changes on sales, they need many observations with different price values. If prices don't change at all, there is no way to learn how they may affect sales. If prices change very rarely or the changes are negligible in magnitude, there isn't much room for comparison and thus there isn't much to learn. The second question is where that variation in x comes from. As we shall see in subsequent chapters (e.g., Chapters 19 through 24), data analysts need to understand the sources of variation in x. This is a somewhat fancy way of saying that data analysts should have a good understanding of why values of x may differ across observations. From this perspective, there are two main types of data: experimental data and observational data. In experimental data, the value of x differs across observations because somebody made them different. In a medical experiment assessing the effects of a drug, some patients receive the drug while others receive a placebo, and who receives what is determined by a rule designed by the experimenter, such as a coin flip or a computer generated sequence of numbers. Here x is the binary variable indicating whether the person received the drug, instead of the placebo. Such variation is called controlled variation. Uncovering the effect of an experiment amounts to comparing y (such as whether a subject recovers from the illness or not) across the various values of x (whether a subject received the drug or not). In contrast, in observational data, no variable is fully controlled by an experimenter or any other person. Most data used in business, economics, and policy analysis are observational. Typical variables in such data are the results of the decisions of many people with diverging goals and circumstances, such as customers, managers of firms, administrators in a government, or members of the board of the monetary authority. Thus, typically, variation in these variables has multiple sources. Whether the variation in conditioning variables is controlled (experimental) or not (observational) is extremely important for causal analysis. Learning the effects of a variable x is a lot easier when we have data from an experiment, in which variation in x is controlled. With observational variation, of the many other things that affect an intervention variable, some may affect the outcome variable in a direct way, too. Disentangling those effects requires data analysts to further condition on many variables at the same time, using methods that we'll cover later in this textbook. Even with the best methods, conditioning on variables is possible only if those variables are measured in the data, which typically is not the case. We'll return to these questions in Chapter 10 and, in more detail, in Chapters 19 through 24. For example, the price of a product (x) sold by a retailer may vary across time. In a sense that variation has one source, the decisions of people in charge at the retail company. But that decision, in turn, is likely affected by many things, including costs, predicted customer demand, and the pricing decisions of competitors, all of which may change through time and thus lead to variation in prices. Here a data analyst may want to uncover what would happen if the retailer increased its price (x) on sales (y), using observational data. That requires conditioning on price: looking at differences in sales across observations with different prices. But the results of this comparison won't tell us what would happen if the retailer increased the price. That's because the question is about changing x as an autonomous decision, whereas, in the data, x tends to change together with some other things. The data analyst then may go on to try to further condition on those other things that are sources of variation, such as the price charged by the competitors, or seasonal variation in demand. If lucky, the data analyst may be able to measure all those variables. But that's a tall order in most cases. The power of experimental data is that there is no need to measure anything else. The other frequent goal of data analysis, making predictions about y, poses somewhat different requirements for variation in x. Understanding the sources of variation in x or, more realistically, the many variables, is still useful, although not in the way it is in causal analysis. Here the main question is stability: whether the patterns of association between y and all those x variables are the same in our data as in the situation for which we make the prediction. Controlled variation in x helps only in the rare case when x would also be controlled in the situation we care about. But uncovering cause and effect relationships can be helpful in prediction in general. We shall discuss these issues in more detail in Part Ill, from Chapter 13 through Chapter 18. This chapter introduced some fundamental concepts and methods of conditioning y on x, the statistical concept of comparing values of y by values of x (or more x variables). We'll return to conditioning yon x in Chapter 7 where we introduce regression analysis. Before doing so, we discuss some general principles and methods in the next chapter that help draw conclusions from our data about the situation we are really interested in. Main Takeaways Data analysis answers most questions by comparing values of y by values of x. Be explicit about what y and x are in your data and how they are related to the question of your analysis. E\[ylx] is mean y conditional on x. Data Exercises 115 Often many x variables are used for prediction and we may further condition on many other variables for causal analysis.

PRACTICE QUESTIONS l. Give an example with two independent events. Can independent events happen at the same time? 2. Give an example of two mutually exclusive events. Can mutually exclusive events happen at the same time? 3. What's the conditional probability of an event? Give an example. 4. What's the conditional mean of a variable? Give an example. 5. How is the correlation coefficient related to the covariance? What is the sign of each when two variables are negatively associated, positively associated, or independent? 6. Describe in words what it means that hotel prices and distance to the city center are negatively correlated. 7. When we want to compare the mean of one variable for values of another variable, we need variation in the conditioning variable. Explain this. 8. What's the difference between the sources of variation in x in experimental data and observational data? 9. What's the joint distribution of two variables, and how can we visualize it? 10. What's a scatterplot? What does it look like for two quantitative variables, each of which can be positive only, if the two variables are positively correlated? 11. What's a bin scatter, and what is it used for? 12. What's a latent variable, and how can we use latent variables in data analysis? Give an example. 13. List two ways to combine multiple measures of the same latent variable in your data for further analysis, and list an advantage and a disadvantage of each way. 14. You want to know if working on case studies in groups or working on them independently is a better way to learn coding in R. What would be your y and x variables here and how would you measure them? 15. Can you tell from the shape of a bin scatter if y and x are positively correlated? Can you tell from it how strong their correlation is? DATA EXERCISES Easier and/or shorter exercises are denoted by \[ _l; harder and/or longer exercises are denoted by \[_

1. Are central hotels better? To answer this, using the hotels-vienna dataset (as discussed in Chapter 3, Section 3.A1), create two categories by the distance from center: close and far (by picking a cutoff of your choice). Show summary statistics, compare stars and ratings and prices for close and far hotels. Create stacked bar charts, box plots, and violin plots. Summarize your findings.
2. Using the wms-management-survey dataset, pick a country different from Mexico, reproduce all figures and tables of our case study, and compare your results to what we found for Mexico.
3. Use the wms-management-survey dataset from a country of your choice, and create a management score from a meaningful subset of the 18 items (e.g. managing talent). Carry out

an analysis to uncover the patterns of association with employment. Summarize what you find, and comment on which visualization you find the most useful. \[ \* ] 4. Use the wms-management-survey dataset from a country of your choice, and produce a principal component using all 18 items to form an alternative management score variable. Use this principal component and the simple average management score to produce bin scatters, scatterplots, and calculate conditional statistics to uncover the patterns of their association with employment. Compare your results and comment on which y measure you would use in presenting them. \[ \* ] 5. Use the football dataset and pick a season. Create three groups of teams, based on their performance in the previous season (new teams come from the lower division, and you may put them in the lowest bin). Examine the extent of home team advantage (as in Chapter 3, Section 3.C 1) by comparing it across these three groups of teams. Produce bin scatters and scatterplots, and calculate conditional statistics. Discuss what you find, and comment on which visualization you find the most useful. \[ REFERENCES AND FURTHER READING Regarding the World Management Survey, you will find plenty of reading at the survey website at https://worldmanagementsurvey.org/academic—research/manufacturing—2/ — with links to papers. For a business perspective, you could have a look at the Harvard Business Review article by Bloom et al. (2017). For a more detailed review of the project, consider reading Bloom et al. (2014).

UNDER THE HOOD: INVERSE CONDITIONAL PROBABILITIES, BAYES' RULE As we introduced in Section 4.3, inverse conditional probabilities are two conditional probabilities, in which the role of the conditioning event and the conditional event are switched: P(eventl I event2) and P(event2 1 eventl). In this section we discuss their relationship to each other. Suppose that we want to know if an athlete used an illegal substance (doping). For this case we collect lab tests. Does the positive result of the test indicate that there is illegal substance in the body of the athlete? In other words, we are interested in whether the athlete has doped given the positive test result. But tests are imperfect so a test result will not reveal doping for sure. Instead, what we may hope for is a probability: the likelihood that someone doped, or not doped, given the result of the test. These are conditional probabilities: P(dopedl positive) and P(not doped I positive). (Knowing one of these two gives the other one as the two sum up to one.) Imperfection of tests mean that they may give positive results even if athletes don't dope: P(positive I not doped) > 0. Tests that are used in real life are usually validated so the level of their imperfection is known. Thus we typically know P(positivel not doped). What we are interested is the inverse probability: P(notdopedl positive). The relation of inverse conditional probabilities tells us how the imperfect nature of a doping test determines how confident we can be concluding that an athlete doped if the result of the test is positive. The two inverse conditional probabilities are related although their relation might seem complicated. We can derive one from the other using the formula that links conditional probabilities and
