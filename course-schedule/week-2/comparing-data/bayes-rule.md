# Bayes' Rule

UNDER THE HOOD: INVERSE CONDITIONAL PROBABILITIES, BAYES' RULE As we introduced in Section 4.3, inverse conditional probabilities are two conditional probabilities, in which the role of the conditioning event and the conditional event are switched: P(eventl I event2) and P(event2 1 eventl). In this section we discuss their relationship to each other. Suppose that we want to know if an athlete used an illegal substance (doping). For this case we collect lab tests. Does the positive result of the test indicate that there is illegal substance in the body of the athlete? In other words, we are interested in whether the athlete has doped given the positive test result. But tests are imperfect so a test result will not reveal doping for sure. Instead, what we may hope for is a probability: the likelihood that someone doped, or not doped, given the result of the test. These are conditional probabilities: P(dopedl positive) and P(not doped I positive). (Knowing one of these two gives the other one as the two sum up to one.) Imperfection of tests mean that they may give positive results even if athletes don't dope: P(positive I not doped) > 0. Tests that are used in real life are usually validated so the level of their imperfection is known. Thus we typically know P(positivel not doped). What we are interested is the inverse probability: P(notdopedl positive). The relation of inverse conditional probabilities tells us how the imperfect nature of a doping test determines how confident we can be concluding that an athlete doped if the result of the test is positive. The two inverse conditional probabilities are related although their relation might seem complicated. We can derive one from the other using the formula that links conditional probabilities and
