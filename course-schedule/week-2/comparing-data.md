---
description: Week 2, Lecture A2.2, 1500 words, 1 hour to complete
---

# Comparing Data

Comparison and Correlation Simple tools to uncover patterns of association by comparing values of y by values of x Motivation Are larger companies better managed? To answer this question, you downloaded data from the World Management Survey. How should you describe the relationship between firm size and the quality of management? In particular, can you describe that with the help of a single number, or an informative graph? To answer the previous question, you have to use the variables in the data to measure the quality of management. In particular, you can use the quality of the many kinds of management practices separately, or you can use a summary measure. What are the advantages and disadvantages of each approach? If you want to use a summary measure, what's the best way to create it, and how should you interpret its magnitude?\
Many questions that data analysis can answer are based on comparing values of one variable, y, against values of another variable, x, and often other variables. Such comparisons are the basis of uncovering the effect of x. if x affects y, the value of y should be different if we were to change the value of x. Uncovering differences in y for different values of one or more x variables is also essential for prediction: to arrive at a good guess of the value of ywhen we don't know it but we know the value of x. We start by emphasizing that we need to measure both y and x well for meaningful comparisons. We introduce conditioning, which is the statistical term for uncovering information related to one variable for different values of another variable. We discuss conditional comparisons, or further conditioning, which takes values of other variables into account as well. We discuss conditional probabilities, conditional distributions, and conditional means. We introduce the related concepts of dependence, mean-dependence, and correlation. Throughout the chapter, we discuss informative visualizations of the various kinds of comparisons. In this chapter, we use the Management quality and firm size: describing patterns of association case study, which uses the vms-management-survey dataset. The question is to what extent the quality of management tends to be different when comparing larger firms to smaller firms. We illustrate conditional probabilities, conditional means, and various aspects of conditional distributions with this case study. Much of data analysis is built on comparing values of a y variable by values of an x variable, or more x variables. Such a comparison can uncover the patterns of association between the two variables: whether and how observations with particular values of one variable (x) tend to have particular values of the other variable (y). The y and x notation for these variables is as common in data analysis as it is for the axes of the Cartesian coordinate system in calculus. The role of y is different from the role of x. It's the values of y we are interested in, and we compare observations that are different in their x values. This asymmetry comes from the goal of our analysis. One goal of data analysis is predicting the value of a yvariable with the help of other variables. The prediction itself takes place when we know the values of those other variables but not the y variable. To predict y based on the other variables we need a rule that tells us what the predicted y value is as a function of the values of the other variables. Such a rule can be devised by analyzing data where we know the y values, too. Those other variables are best thought of as many x variables, such as ... We use the same letter, x, for these variables and distinguish them by subscripts only because their role in the prediction is similar. For instance, to predict the price of Airbnb rentals, we need x variables that matter for that price, such as number of rooms and beds, and location (we will investigate this in Chapters 14 and 16). To predict whether applicants for unsecured loans will repay their loans, we need variables that matter for that repayment probability, such as applicants' income, occupation, age, and family status. The other most frequent goal of data analysis is to learn about the effect of a causal variable x on an outcome variable y. Here, we typically are interested in what the value of y would be if we could change x. how sales would change if we raised prices; whether the proportion of people getting sick in a group would decrease if they received vaccination (we will investigate this in Chapter 23); how the employment chances of unemployed people would increase if they participated in a training program. Data analysis can help uncover such effects by examining data with both y and x and comparing values of y between observations with different values of x. Examples of observations with different x values indicate weeks with different prices, groups with different vaccination rates, or people who participated in a training program versus people who didn't. Often, when trying to uncover the effect of x on y, data analysts consider other variables, too. For example, they want to compare weeks in which the price of the product is different but the price of competing products is the same; groups with different vaccination rates but of the same size and living conditions; people who participate and don't participate in the training program but have the same level of skills and motivation. We often denote these other variables by another letter, z (or Zl,Z2, .. .) , to emphasize that their role is different from the role of the x variable. In sum, deciding on what's y and what's x in the data is the first step before doing any meaningful analysis. In this chapter we discuss some general concepts of comparisons and introduce some simple and intuitive methods. Review Box 4.1 y and x in data analysis Most of data analysis is based on comparing values of a y variable for different values of an x variable (or more variables). • For prediction we want to know what value of y to expect for different values of various x variables, such as . • For causal analysis we want to know what value of y to expect if we changed the value of x, often comparing observations that are similar in other variables (z, z, ...). 4.A1 CASE STUDY — Management Quality and Firm Size: Describing Patterns of Association Question and data In this case study we explore whether, and to what extent, larger firms are better managed. Answering this question can help benchmarking management practices in a specific company. It can also help understand why some firms are better managed than others. Size may be a cause in itself as achieving better management may require fixed costs that are independent of firm size with benefits that are larger for larger firms. Whether firm size is an important determinant of better management can help in answering questions such as the potential benefits of company mergers (merged companies are larger than their component companies), or what kinds of firms are more likely to implement changes that require good management. We use data from the World Management Survey to investigate our question. We introduced the survey in Chapter 1. In this case study we analyze a cross-section of Mexican firms from the 2013 wave of the survey. Mexico is a medium-income and medium-sized open economy with a substantial heterogeneity of its firms along many dimensions. Thus, Mexican firms provide a good example to study the quality of management. We excluded 33 firms with fewer than 100 employees and 2 firms with more than 5000 employees. There are 300 firms in the data, all surveyed in 2013. (Among the Data Exercises, you'll be invited to carry out an analogous analysis of firms from a different country.) The y variable in this case study should be a measure of the quality of management. The x

variable should be a measure of firm size.

```
4.A1 Case Study	99
```

Recall that the main purpose of this data collection was to measure the quality of management in each firm. The survey included 18 "score" variables. Each score is an assessment by the survey interviewers of management practices in a particular domain (tracking and reviewing performance or time horizon and of targets) measured on a scale of 1 (worst practice) to 5 (best practice). Our measure of the quality of management is the simple average of these 18 scores. We — following the researchers who collected the data call it "the " management score. By construction, the range of the management score is between 1 and 5 because that's the range of all 18 items within the average. The mean is 2.9, the median is also 2.9, and the standard deviation is 0.6. The histogram (Figure 4.1) shows a more-or-less symmetric distribution with the vast majority of the firms having an average score between 2 and 4. 25% 20% 15% 10% 5% 0% Management score Figure 4.1 Distribution of the management score variable Note: Histogram, bin width set to 0.25. Source: wms-management-survey dataset. Mexican firms with 100—5000 employees. N=300. We measure firm size by employment: the number of workers employed by the firm. The range of employment we kept in the Mexican data is 100 to 5000. The mean is 760 and the median is 350, signaling substantial skewness with a long right tail. The histogram (Figure 4.2a) shows this skewness. It also shows two extreme values: the largest four firms have 5000 employees, followed by two firms with about 4500 employees and three with 4000 employees. Recall from Chapter 3, Section 3.9 that distributions with long right tails may be well approximated by the lognormal distribution. To check this, we also show the histogram of the natural log of employment (Figure 4.2b). That histogram is still skewed with a longer right tail, but it is substantially more symmetric. Also note that the extreme values are not so extreme anymore with log employment. Thus, we can conclude that the distribution of employment in this data is skewed, it is closer to lognormal than normal, but even the lognormal is not the best approximation.

Firm size (employment) (a) Number of employees Figure 4.2 The distribution of employment Firm size (In(employment)) (b) Natural log of number of employees

Note: Histograms. Source: ms-management-survey dataset. Mexican firms with 100—5000 employees. N=300. Conditioning The word statisticians use for comparison is conditioning. When we compare the values of y by the values of x, we condition y on x. y is also called the outcome variable; x is also called the conditioning variable. Most of the time, we will simply call them y and x. When data analysts want to uncover values of y for observations that are different in x but similar in z, they do one more step of conditioning: they compare y by x conditional on z. That is called further conditioning or a conditional comparison. Thus the word conditioning can be confusing if used without more context. It may mean a simple comparison — uncovering values of yfor different values of x— or it may mean a conditional comparison — uncovering values of y for observations that are different in x but the same in z. Therefore we will try to be more specific and always add the necessary context. Conditioning is an abstract concept. In practice we explore conditional probabilities, conditional means, and conditional distributions. The next sections discuss these in detail.

Review Box 4.2 Conditioning

• Conditioning is the statistical term used for comparing values, or statistics, of y by values of x. • Going one more step, we can further condition on z. compare y by values of x among observations with similar value for z. 4.3 Conditional Probabilities 101 Conditional Probabilities In Section 3.2 of the previous chapter, we introduced probability as the generalization of relative frequency. The probability of a value of a variable in a dataset is its relative frequency (percentage). In more abstract settings, the probability of an event is the likelihood that it occurs. In this section we discuss comparing the probability of events, or the probability of values of variables, by another event, or by values of another variable. Conditional probability is the probability of one event if another event happens. The event the conditional probability is about is called the conditional event; the other event is called the conditioning event. Conditional probabilities are denoted as P(eventl I event2): the probability of eventl conditional on event2. Note that the conditional probability is not symmetrical: P(eventl I event2) # P(event2 1 eventl) in general. Pairs of probabilities of this sort are called inverse conditional probabilities. Thus, inverse conditional probabilities are not equal in general. In fact, they are related to each other in a somewhat complicated way. Understanding their relation can be important to understand a lot of real-life problems, such as understanding the probability of having a condition after receiving a test result. It is also useful to understand the logic of generalizing results from the data that we'll discuss in Chapter 5. We discuss inverse conditional probabilities and their relationship in more detail in Under the Hood section 4.U1. Joint probabilities are related to conditional probabilities. The joint probability of two events is the probability that both occur: P(eventl & event2). When two events are mutually exclusive, their joint probability is zero (the two never happen together). Another probability related to two events denotes the likelihood that one event or the other happens. This is the sum of the two probabilities minus their joint probability: P(eventl OR event2) P(eventl) + P(event2) — P(eventl & event2). If the two events are mutually exclusive, we subtract zero from the sum of the two probabilities. The conditional probability can be expressed as the corresponding joint probability divided by the probability of the conditioning event: P(eventl I event2) — P(eventl & event2) (4.1) P(event2) Two events are independent if the probability of one of the events is the same regardless of whether or not the other event occurs. In the language of conditional probabilities this means that the conditional probabilities are the same as the unconditional probabilities: P(eventl I event2) P(eventl) and P(event2 1 eventl) P(event2). Less intuitive, but also true is that the joint probability of independent events equals the product of their individual probabilities: P(eventl & event2) = P(eventl) x P(event2). (You can see this after plugging it into the formula that relates conditional and joint probabilities.) In data, the events refer to values of variables. Often, the conditional variable, y, is a binary variable: y = 0 or y 1. Then the conditional probability is the probability that y 1 if x has some value: = llx= value). Since y is binary, we know the probability of y = 0 if we know the probability of y 1, be it a conditional or unconditional probability; e.g., P(y Olx = value) 1 - llx

value). When x is binary too, there are two conditional probabilities: llx= 1)

(4.2) (4.3) With more values for either of the two variables (y, x), we have more numbers to compare: P(y valuelx value). With relatively few values, visualization often helps. There are many options for using standard graphs as well as creating individualized graphs. A good solution is the stacked bar chart . It presents the relative frequencies within bars that are on top of each other and thus always add up to a height of 100 %. To visualize conditional probabilities if both y and x have few values, a good visualization is to show stacked bar charts of y for the values of x. Review Box 4.3 Conditional probability • Conditional probability of an event: the probability of an event if another event (the conditioning event) happens: P(eventl I event2) • Two events are independent if the probability of one of the events is the same regardless of whether the other event occurs or not: P(eventl I event2) P(eventl) and P(event2 1 eventl) P(event2). The joint probability of two events is the probability that both happen at the same time: P(eventl & event2) CASE STUDY — Management Quality and Firm Size: Describing Patterns of Association Conditional probabilities Both the management score and employment are quantitative variables with many values. They do not lend themselves to investigating conditional probabilities, at least not without transforming them. Thus, to illustrate conditional probabilities, we consider the individual score variables as y - recall that there are 18 of them, each with five potential values, 1 to 5. For x, we created a qualitative variable by creating three bins of employment: small, medium, large. These three bins are obviously arbitrary. We have chosen them to be bounded by round numbers: 100—199, 200— 999, and 1000+ (with 72, 1 56, and 72 firms, respectively). Thus, for each score variable we have 1 5 conditional probabilities: the probability of each of the 5 values of y by each of the three values Of x— e.g., P(y= llx small). Listing 1 5 conditional probabilities in a table is not a very good way to present them. But stacked bar charts are a great way to visualize them. Figure 4.3 shows two examples, one for lean management and one for performance tracking (each with values 1,2,..,5), each separately by small, medium, and large as firm size bins. For both lean management and performance tracking, the figures show the same pattern of association between the quality of management and firm size. Small firms are more likely to have low scores and less likely to have high scores than medium-sized firms, and, in turn, medium-sized firms are more likely to have low scores and less likely to have high scores than large firms. For lean management, scores 4 and 5 together take up 1 1% of small firms (only score 4, actually), 27% of medium-sized ones, and 36% of large ones. For performance tracking, the corresponding percentages are 40%, 57%, and 68%. These results suggest that larger firms are more likely to 4.4 Conditional Distribution, Conditional Expectation 103 be better managed. As a data exercise, you will be invited to produce similar stacked bar charts to confirm that the patterns for other management variables are the same.

```
Small m Medium 	Large	Small 	Medium 	Large
Firm size (employment), 3 bins	Firm size (employment), 3 bins
(a) Lean management	(b) Performance tracking
```

Figure 4.3 Quality of specific management practices by three bins of firm size: conditional probabilities Note: Firm size as defined by number of employees. Stacked bar charts by firm size bins (bins are 100—199, 200—999, 1 000+, with 72, 1 56, and 72 firms, respectively). Source: wms-management-survey dataset. Mexican firms with 100—5000 employees., N=300.

Conditional Distribution, Conditional Expectation Just as all variables have a distribution (Chapter 3, Section 3.2), all y variables have a conditional distribution if conditioned on an x variable. This is a straightforward concept if the x variable has few values. The simplest case is a binary x when the conditional distribution of y is two distributions, one for each of the two x values. Conditional distributions with few x values are best presented by visualization: for each x value we show stacked bar charts if y has few values or histograms if y is a quantitative variable with many values. The previous section covered conditional stacked bar charts. Here we discuss conditional histograms. Whatever we said about histograms in Chapter 3, Section 3.3 applies here: the look of the histograms are affected by our choice of bin size; density plots may be an alternative but are more sensitive to parameter settings. Our additional advice here is to make sure that the histograms are fully comparable across the x values: most importantly, they have the same bins of y, and they have the same scale on the vertical axis. Comparing histograms can reveal qualitative differences in the distributions. We can tell which distribution tends to be to the left or right of the other, how their modes compare, which one looks more spread out, which one looks more skewed. To make quantitative comparisons, however, we need to compare statistics that summarize the important features of distributions. The most important conditional statistic is the conditional mean, also known as the conditional expectation, which shows the mean (average, expected value) of y for each value of x. The abstract formula for the conditional mean is (4.4) From a mathematical point of view this is a function: if we feed in a number value for variable x, the conditional expectation gives us the number that is the mean of y for observations that have that particular x value. Note that while the overall mean of y, E\[y], is a single number in a data table, the conditional mean Elylx] varies in the data, because it can be different for different values of x. Analogously, we can look at conditional medians, other conditional quantiles, conditional standard deviations, and so on. Comparing box plots and violin plots is a great way to visualize conditional statistics of y when x has few values. Review Box 4.4 Conditional distribution and conditional mean — quantitative y, few values of x • Conditional distribution of yby x is the distribution of yamong observations with specific values of x. Etylx] denotes the conditional mean (conditional expectation) of y for various values of x. • If x has few values, it is straightforward to visualize conditional distributions and calculate conditional means and other conditional statistics. Conditional Distribution, Conditional Expectation with Quantitative x When the x is quantitative with many values, things are more complicated. It is usually impossible or impractical to plot histograms or compute the conditional mean for each and every value of x. First, there are too many values of x and, typically, too few observations for each value. Second, even if we had enough observations, the resulting statistics or pictures would typically be too complex to make sense of. We have two approaches to deal with these problems. The first approach circumvents the problem of too many x values by reducing them through creating bins. With few bins and thus many observations within each bin, we can calculate conditional means, plot histograms, box plots, and so on, in ways that are easy to interpret. Creating bins from x is not only the simplest approach, but it often produces powerful results. Usually with just three bins of x— small, medium, large — we can capture the most important patterns of association between a quantitative y and a quantitative x. Visualization of conditional means of y for bins of x is called a bin scatter. A bin scatter is a figure with the values of the binned x on the horizontal axis and the corresponding conditional mean values of y on the vertical axis. It is good practice to visualize bin scatters with meaningful x values for the x bins, such as their midpoint or the median value of observations within the bin. The second approach keeps x as it is. The most widely used tool is the scatterplot, which is a visualization of the joint distribution of two variables. The joint distribution of two variables is the 4.A3 Case Study 105 frequency of each value combination of the two variables. A scatterplot is a two-dimensional graph with the x and y values on its two axes, and dots or other markers entered for each observation in the data with the corresponding x and y values. With small or moderately large datasets, scatterplots can reveal interesting things. With a very large number of observations, scatterplots can look like a big cloud and allow us to infer less information. Starting with Chapter 7, we'll consider the most widely used method to uncover E\[ylx]: regression analysis. This is a natural continuation of the methods considered in this chapter. But we don't discuss regression here as it requires more time and space. And, before moving on to regression analysis, we consider a few more topics within this chapter and the next two chapters. Review Box 4.5 Joint and conditional distributions of two quantitative variables • The joint distribution of two variables shows the frequency of each value combination of the two variables. • The scatterplot is a good way to visualize joint distributions. • Conditional expectations may be computed and visualized in bins created from the conditioning variable. CASE STUDY — Management Quality and Firm Size: Describing Patterns of Association Conditional mean and joint distribution Let's return to our case study on management quality and firm size, using our data of Mexican

firms. y is the management score, and x is employment. Recall that this data contains firms with 100 to 5000 employees, and the distribution of employment is skewed with a long right tail (Figure 4.2a). The next two charts (Figures 4.4a and 4.4b) show two bin scatters with mean y conditional on three x bins and ten x bins. We created the three bins in the same way as earlier, 100-199, 200— 999, and 1000+ employees, with 72, 156, and 72 firms in the bins, respectively. Our approach to create the ten bins was different: instead of looking for round numbers to separate them, we simply split the data into ten equal-sized subsamples by the number of employees. Most statistical software can create such bins in a straightforward way. On both bin scatter graphs, we show the average management score as a point corresponding to the midpoint in the employment bin (e.g., 150 for the 100—199 bin). The bin scatter with three bins implies a clear positive association: larger firms are better managed, on average. In particular, the mean management score is 2.68 for small firms, 2.94 for medium-sized ones, and it is 3.19 for large firms. The bin scatter with ten bins shows a less straight linear relationship, with a very similar mean in bins 4 through 8. But, overall, that picture too shows higher means in bins of larger firm size. The magnitude of the difference in mean management quality between large and small firms is moderate (about 0.5 for the 1-5 scale).

Finally, note that the bin scatters reflect the very skewed distribution of employment by having larger distances between bin midpoints at larger sizes. We could have presented the bin scatters with the same bins but showing log employment on the x axis instead of employment; that would have shown a more even distribution of the bins and a more linear pattern. Such figures would show the exact same association: management tends to be of higher quality for larger firms, and that difference is smaller, in terms of absolute employment, at higher levels of employment. You'll be invited to do this as a data exercise. 1000 1500 2000 2500 3000 500 1000 1500 2000 2500 3000 3500 Firm size (employment), 3 bins Firm size (employment), 10 bins (a) Three bins of employment (b) Ten bins of employment Figure 4.4 Mean management quality score and firm size Note: Bin scatters. Source: wms-management-survey dataset. Mexican firms with 100—5000 employees. N=300. The bin scatters show a positive pattern of association on average. But does that mean that all larger firms are better managed in this data? Not at all. To appreciate the distribution of the management score around its conditional mean values, we look at the scatterplot. On the left panel (Figure 4.5a) we see the consequences of the very skewed employment distribution: most observations are clustered in the leftmost part of the figure. We can see a positive slope on this graph among larger firms, but it's hard to see any pattern among smaller firms. To make the patterns more visible, the right panel (Figure 4.5b) shows the same scatterplot with the natural log of employment on the x axis instead of employment itself. This amounts to stretching the employment differences between firms at lower levels of employment and compressing those differences at higher levels. (We'll spend a lot more time on what such a transformation does to variables and comparisons of variables later, in Chapter 8.) This scatterplot leads to a more spread out picture, reflecting the more symmetric distribution of the x variable. Here the positive association between mean management score and (log) employment is more visible. In any case, we also see a lot of variation of the management score at every level of employment. Thus, there is a lot of spread of the management score among firms with the same size. As for other features of the distribution, the scatterplot doesn't show a clear pattern between employment (or log employment) and either spread or skewness of the distributions. But that may be because such features are not always easy to read off a scatterplot. 4.A3 Case Study 107

```
Firm size (employment)	Firm size (In(employment))
(a) By employment	(b) By log employment
```

Figure 4.5 The joint distribution of the management quality score and firm size\
Note: Scatterplots.\
Source: ums-management-survey dataset. Mexican firms with 100—5000 employees. N=300. To gain yet more insight into whether, and to what extent, the spread or skewness of the management score distribution differ at different levels of employment, we produced box plots and violin plots of the management score for three employment bins (Figure 4.6).

```
Firm size (employment), 3 bins	Firm size (employment), 3 bins
(a) Box plots	(b) Violin plots
```

Figure 4.6 Conditional summary statistics of the management score by bins of firm size Note: Visuals of the conditional summary statistics: box plot and violin plot. Source: wms-management-survey dataset. Mexican firms with 1 00—5000 employees. N=300.

Both the box plots and the violin plots reveal that the median management score is higher in larger firms, reflecting the same positive association as the bin scatters and the scatterplot. That positive pattern is true when we compare almost any statistic of the management score: median, upper and lower quartiles, minimum and maximum. These figures also show that the spread of management score is somewhat smaller in smaller firms. That means that small firms are more similar to each other in their management scores, besides having lower scores on average. In contrast, larger firms differ more from each other in terms of their management score.

Dependence, Covariance, Correlation After discussing the conditional mean and the conditional distribution and their visualizations, let's introduce a few related concepts that are often used in data analysis. Dependence of two variables, also called statistical dependence means that the conditional distributions of one variable (y) are not the same when conditional on different values of the other variable (x). In contrast, independence of variables means that the distribution of one conditional on the other is the same, regardless of the value of the conditioning variable. These concepts may be viewed as generalizations of independent events (see Section 4.3). Dependence of y and x may take many forms. For example y may be more spread out or more skewed for some x values. But the most important form of dependence is mean-dependence: the mean of y is different when the value of x is different. In other words, the conditional expectation E\[ylx] is not always the same but varies with the value of x. The covariance and the correlation coefficient are measures of this mean-dependence. To be a bit more precise, they measure mean dependence in an average sense. E\[ylx] may have ups and downs by the value of x, and the covariance and correlation coefficient are average measures of those ups and downs. When y and x are positively correlated, E\[ylx] tends to be higher when the value of x is higher. When y and x are negatively correlated, E\[ylx] tends to be lower when the value of x is higher. The two measures are very closely related: the correlation coefficient is the standardized version of the covariance. The formula for the covariance between two variables x and y in a dataset with n observations is Cov\[x, y] (4.5) n The correlation coefficient divides this by the product of the two standard deviations: Cov\[x, y] Corr\[x, y] (4.6) Std\[x] Std\[y] —1 Corr\[x,y] 1 (4.7) The covariance may be any positive or negative number, while the correlation coefficient is bound to be between negative one and positive one. But their sign is always the same: the covariance is zero when the correlation coefficient is zero; the covariance is positive when the correlation coefficient is positive; the covariance is negative when the correlation coefficient is negative. When the correlation coefficient is zero we say that y and x are uncorrelated. With positive correlation, y and x are positively correlated. With negative covariance and correlation we say that y and x are negatively correlated. The magnitude of the correlation coefficient shows the strength 4.6 Dependence, Covariance, Correlation 109 of the association: the larger the magnitude, the stronger the mean-dependence between the two variables. Data analysts tend to consider a correlation of 0.2 or less (in absolute value) weak, and a correlation above 0.7 in absolute value is usually considered strong. If two variables are independent, they are also mean-independent and thus the conditional expectations are all the same: E\[ylxl Ely] for any value of x. The covariance and the correlation coefficient are both zero in this case. In short, we say that independence implies mean independence and zero correlation. But the reverse is not true. We can have zero correlation but mean-dependence (e.g., a symmetrical U-shaped conditional expectation has an average of zero), and we can have zero correlation and zero mean-dependence without complete independence (e.g., the spread of y may be different for different values of x). Spending a little time with its formula can help understand how the covariance is an average measure of mean-dependence. The product within the sum in the numerator multiplies the deviation of x from its mean (Xi — R) with the deviation of y from its mean (Yi — y), for each observation i. The entire formula is the average of these products across all observations. If a positive deviation of x from its mean goes along with a positive deviation of y from its mean, the product is positive. Thus, the average of this product across all observations is positive. The more often a positive — \* goes together with a positive — y, the more positive is the covariance. Or, the larger are the positive deviations that go together the larger the covariance. If, on the other hand, a positive — \* goes along with a negative — y, the product tends to be negative. Thus the average of this product is negative. The more often a positive — goes together with a negative — y, the more negative the covariance. Finally, if a positive — \* goes along with a positive — y some of the time and a negative — y at other times, and these two balance each other out, the positive values of the product and the negative values of the product cancel out. In this case the average is zero. Exact zero covariance rarely happens in real data because that would require an exact balancing out. The more balanced the positive deviation in and positive deviation in instances are with the positive deviation in x, and negative deviation in instances, the closer the covariance is to zero. Also note that the formulae of the covariance or the correlation coefficient allow for all kinds of variables, including binary variables and ordered qualitative variables as well as quantitative variables. The covariance and the correlation coefficient will always be zero if the two variables are meanindependent, positive if positively mean-dependent, and negative if negatively mean-dependent. Thus, they give a quick and not completely meaningless picture about mean-dependence among binary and ordered qualitative variables. However, they are more appropriate measures for qualitative variables. That's because the differences Y — y and & — make less sense when y and x are qualitative variables. For that reason, data analysts use other correlation-like measures for qualitative variables, but those measures are beyond the scope of our textbook.

Review Box 4.6 Covariance and correlation The covariance measures the mean-dependence of two variables: Cov\[x, y] • The correlation coefficient is a standardized version of the covariance and ranges between —1 and 1: Corr\[x,y] • The covariance and the correlation are both zero if the variables are independent, positive if the variables are positively associated, and negative if they are negatively associated. From Latent Variables to Observed Variables Before closing the chapter, let's discuss two more topics briefly The first one is the concept of latent variables. Often, the y or x variables we have in question are abstract concepts: the quality of management of a firm, skills of an employee, risk tolerance of an investor, health of a person, wealth of a country Typically, such variables are not parts of an actual dataset, and they can't be because they are too abstract. Such variables are called latent variables. Data analysis can help answer questions involving latent variables only by substituting observed variables for them. Those observed variables are called proxy variables, where "proxy" means substitute (the word proxy is used as noun, adjective, and verb). The quality of management may be proxied by answers to survey questions on management practices, as in our case study; employee skills may be proxied by qualifications or measures of past performance; and so on. The most important thing to keep in mind here is that data analysis compares values of measured variables. Even if those variables are supposed to measure abstract concepts, it's never the abstract concepts themselves that we have in our data. Thus we can never examine things such as skills or attitudes or health; instead, we examine proxies such as measures of performance, answers to survey questions, or results of doctors' diagnoses. This is simply re-iterating the point we made earlier in Chapter 1, Section 1.3: the content of a variable is determined by how it is measured — not by what name somebody attached to it. A specific issue arises when our data contains not one but more variables that could serve as proxies to the latent variable we want to examine. The question here is how to combine multiple observed variables. Data analysts use one of three main approaches: Use one of the observed variables Take the average (or sum) of the observed variables Use principal component analysis (PCA) to combine the observed variables. Using one measured variable and excluding the rest has the advantage of easy interpretation. It has the disadvantage of discarding potentially useful information contained in the other measured variables. Taking the average of all measured variables makes use of all information in a simple way. When all of those variables are measured using the same scale, this approach yields a combined measure with a natural interpretation. When the variables are measured at different scales, we need to bring the observed variables to the same scale. Usually, we do that by standardizing them: subtracting the mean and dividing by the standard deviation (see Chapter 3, Section 3.6). This standardized measure is also called a z-score. By taking a simple average of all these variables we give them equal weight. This may be a disadvantage if some of the variables are better measures of the latent variable than others. The third approach remedies that problem. Principal component analysis (PCA) is a method to give higher weights to the observable variables that are better measures. PCA finds those weights by examining how strongly they would be related with the weighted average. The logic is an iterative process: create an average, examine how each variable is related to it by looking at their correlations, give 4.A4 Case Study 111 higher weights to those with stronger correlation, start over. The actual technique takes an ingenious approach to do the whole thing in one step. Of the three approaches, we recommend the second one: taking a simple average of the observed variables after making sure that they are measured at the same scale. This is the simplest way to combine all variables in a meaningful way. In principle, PCA produces a better combined measure, but it is more complicated to produce and harder to present to non-expert audiences. Moreover, it often gives similar results to a simple average. Thus we recommend that PCA is used as a robustness check, if at all. If the results of our analysis are very different with a simple average and with a PCA measure, some of our observed variables are very differently related to the average measure than others. It is good practice then to go back and understand the content of those variables and, perhaps, discard some of them from the analysis. Review Box 4.7 Latent and proxy variables • Latent variables are abstract concepts that are not actual variables in the data. • Proxy variables (proxies) are variables in the data that measure latent variables. • When more proxy variables are available for a single latent variable it is good practice to take their average, after making sure that they are measured on the same scale (for example, by standardizing them). CASE STUDY — Management Quality and Firm Size: Describing Patterns of Association Correlation and latent variable The covariance between firm size and the management score in the Mexican sample we use is 177. The standard deviation of firm size is 977; the standard deviation of management score is 0.6. The correlation coefficient is 0.30 (177/(977 \* 0.6) =0.30). This result shows a positive association: firms with more employees tend to have a higher management score. The magnitude of the correlation is moderate, presumably because many other things matter for the quality of management besides the size of a firm. Table 4.1 shows the correlation coefficient in seven broad categories of industrial classification (plus one "other" category with the industries with very few firms, combined). Table 4.1 Management score and employment: correlation and average management score by industry Industry Management—employment correlation Observations Auto Chemicals Electronics, equipment, machinery 0.50 0.05 0.28 26 69 36

Management—employment Industry correlation Observations

Food, drinks, tobacco 0.05 34 Materials, metals 0.32 50 Textile, apparel, leather 0.36 38 Wood, furniture, paper 0.28 37 Other 0.63 10 All 0.30 300

Source: wms-management-survey dataset. Mexican firms with 100—5000 employees. N=300. The table reveals that the management quality-firm size correlation varies considerably across industries. The correlation is strongest in the auto and "other" industries. At the same time, we see hardly any correlation among firms in the chemicals and food industries. Before concluding our case study, note that it illustrates the measurement issues related to latent variables, too. From a conceptual point of view, the y variable in our case study is management quality, a latent variable. We have 18 measures for this latent variable in the data; those are the 1 8 score variables on the quality of various aspects of management. Each of these 18 variables is measured by the survey (as we discussed in Chapter 1, Section 1 .C 1), and each is measured on the same I-to-5 scale. For the measure of the overall quality of management, we used two of the three strategies we recommended in Section 4.7. To illustrate conditional probabilities, visualized by the stacked bar charts in Figures 4.3a and 4.3b, we used 2 of the 18 score variables. Each one is an imperfect measure of the overall quality of management, but each has a clear interpretation: the rating of the particular aspect of management quality by the interviewer For most of the case study, we used the average score: the simple average of the 18 scores. We could use this simple average because each of the 18 variables aimed to measure an aspect of the same thing, management quality, and each was measured on the same scale (1 to 5). As a data exercise you are invited to try the third option we recommended in Section 4.7, and create a principal component from the 18 scores instead of their simple average. When analyzing the relationship between firm size and management quality, using this principal component measure turns out to give very similar results to what we have uncovered using the average score measure. This concludes our case study. What did we learn from it about the association between firm size and management quality? We found that, among Mexican manufacturing firms, larger firms tend to be better managed. Large firms (with 1000-5000 employees) have an average score of 3.19, compared to 2.94 for medium-sized firms (with 200—999 employees), and 2.68 for small ones (with 100-199 employees). We also found that the correlation, while positive, is not very strong, perhaps because other things matter for the quality of management besides firm size. When disaggregating the results into smaller industry groups, we found that the strength of the management—size correlation differs in some industries from the rest, but we haven't seen any clear pattern that would tell us why. Finally,

```
4.8 Sources of Variation in x	113
```

we have seen that management quality is not only better, on average, among larger firms, but it is also somewhat more spread among larger firms. These results inform the business or policy questions we may be interested in. When considering the management practices of a specific firm, we should have firms of similar size as a benchmark. And, better management of a larger firm may be a potential benefit of increased firm size — e.g., through a merger between companies. As for the methods discussed in this chapter, this case study illustrated what we can do to uncover patterns of associations and conditional distributions when both y and x are quantitative variables. We have seen that creating bins from x can lead to informative visualizations, such as a bin scatter or box plots of y by bins of x. Three bins (small, medium, large) appeared a good choice in our case. For example, the bin scatter with ten bins did not give much more information than the bin scatter with three bins. We have also seen that the correlation coefficient is a useful statistic to summarize mean-dependence between y and x, and it allows us to dig a little deeper by showing whether and how the correlation differs across groups by a third variable (here industry). Finally, we have seen that, with rich enough data, we can use an average score variable calculated from many (here 18) variables to measure a latent variable, management quality in our case study. Sources of Variation in x Our final section in this chapter is a note on variation in x, the variable (or variables) we condition on to make comparisons in y. The first thing to note is that we need variation in x, and the more variation we have the better in general. In data with no variation in x, all observations have the same values and it's impossible to make comparisons. This may sound trivial, but it's essential to keep in mind. Similarly, the more variation in x, the better the chances for comparison. For example, when data analysts want to uncover the effect of price changes on sales, they need many observations with different price values. If prices don't change at all, there is no way to learn how they may affect sales. If prices change very rarely or the changes are negligible in magnitude, there isn't much room for comparison and thus there isn't much to learn. The second question is where that variation in x comes from. As we shall see in subsequent chapters (e.g., Chapters 19 through 24), data analysts need to understand the sources of variation in x. This is a somewhat fancy way of saying that data analysts should have a good understanding of why values of x may differ across observations. From this perspective, there are two main types of data: experimental data and observational data. In experimental data, the value of x differs across observations because somebody made them different. In a medical experiment assessing the effects of a drug, some patients receive the drug while others receive a placebo, and who receives what is determined by a rule designed by the experimenter, such as a coin flip or a computer generated sequence of numbers. Here x is the binary variable indicating whether the person received the drug, instead of the placebo. Such variation is called controlled variation. Uncovering the effect of an experiment amounts to comparing y (such as whether a subject recovers from the illness or not) across the various values of x (whether a subject received the drug or not). In contrast, in observational data, no variable is fully controlled by an experimenter or any other person. Most data used in business, economics, and policy analysis are observational. Typical variables in such data are the results of the decisions of many people with diverging goals and circumstances, such as customers, managers of firms, administrators in a government, or members of the board of the monetary authority. Thus, typically, variation in these variables has multiple sources. Whether the variation in conditioning variables is controlled (experimental) or not (observational) is extremely important for causal analysis. Learning the effects of a variable x is a lot easier when we have data from an experiment, in which variation in x is controlled. With observational variation, of the many other things that affect an intervention variable, some may affect the outcome variable in a direct way, too. Disentangling those effects requires data analysts to further condition on many variables at the same time, using methods that we'll cover later in this textbook. Even with the best methods, conditioning on variables is possible only if those variables are measured in the data, which typically is not the case. We'll return to these questions in Chapter 10 and, in more detail, in Chapters 19 through 24. For example, the price of a product (x) sold by a retailer may vary across time. In a sense that variation has one source, the decisions of people in charge at the retail company. But that decision, in turn, is likely affected by many things, including costs, predicted customer demand, and the pricing decisions of competitors, all of which may change through time and thus lead to variation in prices. Here a data analyst may want to uncover what would happen if the retailer increased its price (x) on sales (y), using observational data. That requires conditioning on price: looking at differences in sales across observations with different prices. But the results of this comparison won't tell us what would happen if the retailer increased the price. That's because the question is about changing x as an autonomous decision, whereas, in the data, x tends to change together with some other things. The data analyst then may go on to try to further condition on those other things that are sources of variation, such as the price charged by the competitors, or seasonal variation in demand. If lucky, the data analyst may be able to measure all those variables. But that's a tall order in most cases. The power of experimental data is that there is no need to measure anything else. The other frequent goal of data analysis, making predictions about y, poses somewhat different requirements for variation in x. Understanding the sources of variation in x or, more realistically, the many variables, is still useful, although not in the way it is in causal analysis. Here the main question is stability: whether the patterns of association between y and all those x variables are the same in our data as in the situation for which we make the prediction. Controlled variation in x helps only in the rare case when x would also be controlled in the situation we care about. But uncovering cause and effect relationships can be helpful in prediction in general. We shall discuss these issues in more detail in Part Ill, from Chapter 13 through Chapter 18. This chapter introduced some fundamental concepts and methods of conditioning y on x, the statistical concept of comparing values of y by values of x (or more x variables). We'll return to conditioning yon x in Chapter 7 where we introduce regression analysis. Before doing so, we discuss some general principles and methods in the next chapter that help draw conclusions from our data about the situation we are really interested in. Main Takeaways Data analysis answers most questions by comparing values of y by values of x. Be explicit about what y and x are in your data and how they are related to the question of your analysis. E\[ylx] is mean y conditional on x. Data Exercises 115 Often many x variables are used for prediction and we may further condition on many other variables for causal analysis.

PRACTICE QUESTIONS l. Give an example with two independent events. Can independent events happen at the same time? 2. Give an example of two mutually exclusive events. Can mutually exclusive events happen at the same time? 3. What's the conditional probability of an event? Give an example. 4. What's the conditional mean of a variable? Give an example. 5. How is the correlation coefficient related to the covariance? What is the sign of each when two variables are negatively associated, positively associated, or independent? 6. Describe in words what it means that hotel prices and distance to the city center are negatively correlated. 7. When we want to compare the mean of one variable for values of another variable, we need variation in the conditioning variable. Explain this. 8. What's the difference between the sources of variation in x in experimental data and observational data? 9. What's the joint distribution of two variables, and how can we visualize it? 10. What's a scatterplot? What does it look like for two quantitative variables, each of which can be positive only, if the two variables are positively correlated? 11. What's a bin scatter, and what is it used for? 12. What's a latent variable, and how can we use latent variables in data analysis? Give an example. 13. List two ways to combine multiple measures of the same latent variable in your data for further analysis, and list an advantage and a disadvantage of each way. 14. You want to know if working on case studies in groups or working on them independently is a better way to learn coding in R. What would be your y and x variables here and how would you measure them? 15. Can you tell from the shape of a bin scatter if y and x are positively correlated? Can you tell from it how strong their correlation is? DATA EXERCISES Easier and/or shorter exercises are denoted by \[ _l; harder and/or longer exercises are denoted by \[_

1. Are central hotels better? To answer this, using the hotels-vienna dataset (as discussed in Chapter 3, Section 3.A1), create two categories by the distance from center: close and far (by picking a cutoff of your choice). Show summary statistics, compare stars and ratings and prices for close and far hotels. Create stacked bar charts, box plots, and violin plots. Summarize your findings.
2. Using the wms-management-survey dataset, pick a country different from Mexico, reproduce all figures and tables of our case study, and compare your results to what we found for Mexico.
3. Use the wms-management-survey dataset from a country of your choice, and create a management score from a meaningful subset of the 18 items (e.g. managing talent). Carry out

an analysis to uncover the patterns of association with employment. Summarize what you find, and comment on which visualization you find the most useful. \[ \* ] 4. Use the wms-management-survey dataset from a country of your choice, and produce a principal component using all 18 items to form an alternative management score variable. Use this principal component and the simple average management score to produce bin scatters, scatterplots, and calculate conditional statistics to uncover the patterns of their association with employment. Compare your results and comment on which y measure you would use in presenting them. \[ \* ] 5. Use the football dataset and pick a season. Create three groups of teams, based on their performance in the previous season (new teams come from the lower division, and you may put them in the lowest bin). Examine the extent of home team advantage (as in Chapter 3, Section 3.C 1) by comparing it across these three groups of teams. Produce bin scatters and scatterplots, and calculate conditional statistics. Discuss what you find, and comment on which visualization you find the most useful. \[ REFERENCES AND FURTHER READING Regarding the World Management Survey, you will find plenty of reading at the survey website at https://worldmanagementsurvey.org/academic—research/manufacturing—2/ — with links to papers. For a business perspective, you could have a look at the Harvard Business Review article by Bloom et al. (2017). For a more detailed review of the project, consider reading Bloom et al. (2014).

UNDER THE HOOD: INVERSE CONDITIONAL PROBABILITIES, BAYES' RULE As we introduced in Section 4.3, inverse conditional probabilities are two conditional probabilities, in which the role of the conditioning event and the conditional event are switched: P(eventl I event2) and P(event2 1 eventl). In this section we discuss their relationship to each other. Suppose that we want to know if an athlete used an illegal substance (doping). For this case we collect lab tests. Does the positive result of the test indicate that there is illegal substance in the body of the athlete? In other words, we are interested in whether the athlete has doped given the positive test result. But tests are imperfect so a test result will not reveal doping for sure. Instead, what we may hope for is a probability: the likelihood that someone doped, or not doped, given the result of the test. These are conditional probabilities: P(dopedl positive) and P(not doped I positive). (Knowing one of these two gives the other one as the two sum up to one.) Imperfection of tests mean that they may give positive results even if athletes don't dope: P(positive I not doped) > 0. Tests that are used in real life are usually validated so the level of their imperfection is known. Thus we typically know P(positivel not doped). What we are interested is the inverse probability: P(notdopedl positive). The relation of inverse conditional probabilities tells us how the imperfect nature of a doping test determines how confident we can be concluding that an athlete doped if the result of the test is positive. The two inverse conditional probabilities are related although their relation might seem complicated. We can derive one from the other using the formula that links conditional probabilities and
