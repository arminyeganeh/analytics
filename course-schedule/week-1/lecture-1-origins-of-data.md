---
description: Week 1, Lecture A1.1, 1500 words, 1 hour to complete
---

# Origins of Data

### Data

Data is "factual information, such as measurements or statistics, used as a basis for reasoning, discussion, or calculation." (Merriam-Webster) Data rarely comes in a form that can directly answer our questions.&#x20;

### Data analysis

Data analysis is a process. It starts with formulating a question and collecting appropriate data or assessing whether the available data can help answer the question. Then comes cleaning and organizing the data, tedious but essential tasks that affect analysis results as much as any other step in the process. Exploratory data analysis gives context to results and helps us decide the details of the analytical method to be applied. The primary analysis consists of choosing and implementing the method to answer the question, with potential robustness checks. Along the way, correct interpretation and effective presentation of the results are crucial. Carefully crafted data visualization helps us summarize our findings and convey key messages. The final step is to answer the original question, with potential qualifications and directions for future inquiries.

### Data table

Data is most straightforward when it comes in the format of a data table. A data table consists of observations (cases) and variables (features). In a data table, each row is a unique observation, and whatever is in a row is information about that specific observation. Columns are variables: column number one is variable number one, column number two is variable number two, etc. A dataset is a broader concept that includes, potentially, multiple data tables with different kinds of information.&#x20;

### Data format

A common file format for data tables is a comma-separated-values file (CSV). CSV files are text files of a data table, in which columns are separated by a character called a delimiter (often a comma or a semicolon).

Variables are identified by names. The data table may have variable names already, and analysts are free to use those names or rename the variables. Some analysts prefer short names that are easier to work with in code; others prefer long names that are more informative.&#x20;

Observations are identified by single or multiple identifier (ID) variables that uniquely identify each observation. ID variables may be numeric or text containing letters or other characters. ID variables typically come in the first column of data tables. We use the notation&#x20;

$$
X_i
$$

to refer to the value of variable x for the i-th observation. i typically refers to the position of the observation in the dataset (row number).&#x20;

### Data Structures

Observation structures can be **cross-sectional**, **time series**, or **multi-dimensional**. Cross-sectional data belong to the same time and refer to different units, such as different individuals, families, houses, firms, and countries. Ideally, all observations in a cross-sectional dataset are observed at the exact same time. In practice, this often means there have been some time intervals among observations. When that interval is narrow, data analysts treat it as if it were a single point in time.

In most cross-sectional data, the order of observations in the dataset does not matter: the first data row may be switched with the second data row, and the information content of the data would be the same. Cross-sectional data has the simplest structure.

Observations in time series data refer to a single unit observed multiple times, e.g., a shop's monthly sales values. In time series data, there is a natural ordering of the observations, which is typically important for the analysis.&#x20;

Multi-dimensional (panel) data, as its name suggests, has more than one dimension. A common type of panel data has many units, each observed multiple times. Such data is called longitudinal data, or cross-section time series data. Examples include countries observed repeatedly for several years, data on employees of a firm on a monthly basis, or prices of several company stocks observed on many days.

Multi-dimensional datasets can be represented in table formats in various ways. For Xt data, the most convenient format has one observation representing one unit observed at one time (country-year observations, person-month observations, company-day observations) so that one unit (country, employee, company) is represented by multiple observations. In Xt data tables, observations are identified by two ID variables: one for the cross-sectional units and one for time. Xt data is called balanced if all cross-sectional units have observations for the very same time periods. It is called unbalanced if some cross-sectional units are observed more times than others. We shall discuss other specific features of multi-dimensional data in Chapter 23 where we discuss the analysis of panel data in detail.

Another important feature of data is the level of aggregation of observations. Data with information on people may have observations at different levels: age is at the individual level, home location is at the family level, and real estate prices may be available as averages for zip code areas. Data with information on manufacturing firms may have observations at the level of plants, firms as legal entities (possibly with multiple plants), industries with multiple firms, and so on. Time series data on transactions may have observations for each transaction or for transactions aggregated over some time period.

Sometimes data is available at a level of aggregation that is different from the ideal level. If data is too disaggregated (i.e., by establishments within firms when decisions are made at the firm level), we may want to aggregate all variables to the preferred level. If, however, the data is too aggregated (i.e., industry-level data when we want firm-level data), there isn't much that can be done. Such data miss potentially important information.

### Data Quality

Data analysts should know their data. They should know how the data was born, with all details of measurement that may be relevant for their analysis. They should know their data better than their audience. Few things have more devastating consequences for a data analyst's reputation than someone in the audience pointing out serious measurement issues the analyst didn't consider.

Garbage in - garbage out. This summarizes the prime importance of data quality. The results of an analysis cannot be better than the data it uses. If our data is useless to answer our question, the results of our analysis are bound to be useless, no matter how fancy a method we apply to it. Conversely, with excellent data even the simplest methods may deliver very useful results. Sophisticated data analysis may uncover patterns from complicated and messy data but only if the information is there.

| Aspect             | Description                                                                                                                                                                                                      |
| ------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Content            | The content of a variable is determined by how it was measured, not by what it was meant to measure. As a consequence, just because a variable is given a particular name, it does not necessarily measure that. |
| Validity           | The content of a variable (actual content) should be as close as possible to what it is meant to measure (intended content).                                                                                     |
| Reliability        | Measurement of a variable should be stable, leading to the same value if measured the same way again.                                                                                                            |
| Comparability      | A variable should be measured the same way for all observations.                                                                                                                                                 |
| Coverage           | Ideally, observations in the collected dataset should include all of those that were intended to be covered (complete coverage). In practice, they may not (incomplete coverage).                                |
| Unbiased selection | If coverage is incomplete, the observations that are included should be similar to all observations that were intended to be covered (and, thus, to those that are left uncovered).                              |

### How data is born

Data can be collected for the purpose of the analysis, or it can be derived from information collected for other purposes. The structure and content of data purposely collected for the analysis are usually better suited to analysis. Such data is more likely to include variables that are the focus of the analysis, measured in a way that best suits the analysis, and structured in a way that is convenient for the analysis. Frequent methods to collect data include scraping the Web for information (web scraping) or conducting a survey. Data collected for other purposes (secondary data) can be also very useful to answer our inquiries. Data collected for the purpose of administering, monitoring, or controlling processes in business, public administration, or other environments are called administrative (admin) data. Admin data usually cover a complete population: all employees in a firm, all customers of a bank, or all tax filers in a country.

Often, data collected for other purposes is available at a low cost for many observations. At the same time, the structure and content of such data are usually further away from the needs of the analysis compared to purposely collected data. This trade-off has consequences that vary across data, methods, and questions to be answered.

Data quality is determined by how the data was born, and data collection affects various aspects of data quality in different ways. For example, the validity of the most important variables tends to be higher in primary sources of data, while coverage tends to be more complete in admin data. However, that's not always the case, and even when it is, we shouldn't think in terms of extremes. Instead, it is best to think of these issues as part of a continuum.&#x20;

### Linking data

Linking data from different sources can result in valuable datasets. The purpose of linking data is to leverage the advantages of multiple sources while compensating for their disadvantages. Data may be linked individually or in aggregation, e.g., industry-level information linked to firms, zip-code-level information linked to individuals, and so on. In the end, linkages are rarely perfect: there are usually observations that cannot be linked. Therefore, data analysts should assess coverage and selection bias. The variables in the large but inexpensive data may allow uncovering some important patterns, but they may not be enough to gain a deeper understanding of those patterns. Collecting additional data for a subset of the observations may provide valuable insights at extra cost, but keeping this additional data collection small can keep those costs contained.

### Sources of data

Some international organizations, governments, central banks, and some other organizations collect and store data to be used for analysis. Often, such data is available free of charge. For example, the World Bank collects many time series of government finances, business activity, health, and many others, for all countries. We shall use some of that data in our case studies. Another example is FRED, collected and stored by the US Federal Reserve system, which includes economic time series data on the USA and some other countries. One way to gather information from such providers is to visit their website and download a data table. Instead of clicking and downloading, data analysts may use an **Application Programming Interface**, or API, to directly load data into a statistical software package. An API is a software intermediary, or an interface, that allows programs, or scripts, to talk to each other. Using an API, data analysts may load these datasets into their statistical software as part of the code they write for that software. Besides data collected and provided for the purposes of analysis, there is a lot of information out there that can be turned into useful data even though it is not collected for the purpose of analysis.&#x20;

Collecting data from the web using code is called **web scraping**. Well-written web scraping code can load and extract data from multiple web pages. Some websites are easier to scrape than others, depending on how they structure and present information. There are many web scraping software solutions available, and there is a lot of help available online. In the end, scraping is writing code so it requires both general experience in coding and learning the specifics of scraping.&#x20;

**Surveys** collect data by asking people questions and recording their answers. Typically, the answers are short and easily transformed into variables, either qualitative or quantitative. People answering questions are called respondents. The set of questions presented to respondents is called a questionnaire. There are two major kinds of surveys: **self-administered surveys** and **interviews**.&#x20;

### Surveys

In self-administered surveys, respondents are left on their own to answer questions. Typically, the questions are presented to them in writing. Web surveys are the most important example. Respondents see the questions on the screen of a web-connected device, and they tap, click, or enter text to answer the questions. The answers are immediately recorded and transformed into an appropriate data table. This feature is an important advantage of web surveys: there is no need for anyone else to enter the data or put it into an appropriate structure. That means lower costs and less room for error. Respondents need to be recruited to participate in web surveys just like in any other survey. Apart from that, however, web surveys have low marginal costs: once the web questionnaire is up and running, having more respondents answer them incurs practically no extra cost. Before the web, self-administered surveys were done on paper, and respondents were most often required to mail the filled-out questionnaires. That method entailed higher costs and, arguably, more burden on respondents. Besides low costs and quick access to data, web surveys have other advantages. They can present questions with visual aids and interactive tools, and they can embed videos and sound. They may be programmed to adapt the list and content of questions based on what respondents entered previously. Web surveys can include checks to ensure cleaner data. For example, they can give alerts to respondents if their answers do not seem to make sense or are inconsistent with the information given earlier A disadvantage of self-administered questionnaires is that they leave little room for clarifying what the questions are after This may affect the validity of measurement: respondents may answer questions thinking that some questions are about something different from what was intended. Web surveys have an advantage over traditional paper-based surveys in that they can accommodate explanations or help. However, it is up to the respondents to invoke such help, and there is little room to follow up if the help received is not enough. Another disadvantage of some self-administered surveys is a high risk of incomplete and biased coverage. Potential respondents are left on their own to decide whether to participate, and those that can and choose to participate may be different from everyone else. People without access to the Internet can't answer web surveys. People who can't read well can't answer self-administered surveys that are presented in writing. The coverage issue is more severe in some cases (e.g., surveys of children, of the elderly, or in developing countries) than in others (e.g., surveys of university students). Moreover, when respondents are left on their own, they may be less likely to participate in surveys than when someone is there to talk them into it. With ingenuity and investment these issues may be mitigated (offering web connection, presenting questions in voice, offering compensation). Nevertheless, incomplete and biased coverage need special attention in the case of self-administered surveys.&#x20;

### Interviews

**Interviews** are the other main way to conduct surveys besides self-administration. They create a survey situation with two participants: an interviewer and a respondent. During survey interviews, interviewers ask the questions of the survey and nothing else. In a broader sense, interviews may include freer conversations, but here we focus on surveys. Interviews may be conducted in-person or over the telephone. Modern interviews are often done with the help of a laptop, tablet, or other device. Such computer-aided interviews share some advantages with web surveys. They allow for checks on admissible answers and consistency across answers, and, sometimes, the use of visualization, videos, and voice. With answers entered into such devices, they can then produce data tables that are ready to use. An advantage of interviewers is the potential for high validity. Interviewers can, and are often instructed to, help respondents understand questions as they are intended to be understood. Interviewers may also help convince respondents to participate in surveys thus leading to better coverage. At the same time, comparability of answers may be an issue with interviews. Different interviewers may ask the same survey question in different ways, add different details, may help in different ways, or record answers differently. All this may result in interviewer effects: systematic differences between answers recorded by different interviewers even if the underlying variables have no such differences. It is good practice to mitigate these interviewer effects during data collection by precise instructions to interviewers and thorough training. The main disadvantage of interviews is their cost. Interviewers need to be compensated for the time they spend recruiting respondents, interviewing them, and, in the case of personal interviews, traveling to meet them. Interviews are thus substantially more expensive than self-administered surveys, especially if they invest in insuring high data quality by using computer-aided techniques and intensive training. Mixed-mode surveys use different methods within the same survey: telephone for some and web for others; in-person for some and telephone for others, and so on. Sometimes the same person is asked to answer different questions in different survey modes. Sometimes different people are asked to participate in different modes. Usually, the idea behind mixed mode surveys is saving on costs while maintaining appropriate coverage. They allow for data to be collected at lower costs for some variables, or some observations, using the more costly survey mode only when needed. Comparability may be an issue in mixed-mode surveys when different people answer the same question in different modes. Extensive research shows that answers to many kinds of questions compare well across survey modes but that some kinds of questions tend to produce less comparable answers.

### Sampling

Sometimes data is collected on all possible observations, attempting complete coverage. This makes sense when we are targeting limited observations (e.g., employees of a medium-sized firm) or the marginal cost of data collection is negligible (as with web scraping). Often, though, finding more observations may be costly (e.g., recruiting respondents for surveys), and collecting data on new observations may also have high costs (e.g., additional personal interviews). In such cases, it makes sense to gather information on only a subset of all potential observations. Data collection here is preceded by sampling: selecting observations for which data should be collected. The set of observations on which data is collected in the end is called the sample. The larger set of observations from which a sample is selected is called the population or universe. Samples have to represent the population. A sample is representative if the distribution of all variables in the sample is the same as, or very close to, their corresponding distribution in the population. (The distribution of variables is the frequency of their values, e.g., fraction female, percent with income within a certain range) A representative sample of products in a supermarket has the same distribution of prices, sales, and frequency of purchase as all products in the supermarket.&#x20;

as when all transactions are considered. A representative sample of workers in an economy has the same distribution of demographic characteristics, skills, wages, and so on, as all workers in the economy. Representative samples do not cover all observations in the population, but they are free from selection bias. Whether a sample is representative is impossible to tell directly. We don't know the value of all variables for all observations in the population, otherwise we would not need to collect data from a sample in the first place. There are two ways of assessing whether a sample is representative: evaluating the data collection process and, if possible, benchmarking the few variables for which we know the distribution in the population. Benchmarking looks at variables for which we know something in the population. We can benchmark our sample by comparing the distribution of those variables in the sample to those in the population. One example could be comparing the share of various industries in a sample of companies to the share of industries published by the government, based on data that includes the population of companies. If this kind of benchmarking reveals substantial differences then the sample is not representative. If it shows similar distributions then the sample is representative for the variable, or variables, used in the comparison. It may or may not be representative for other variables. A successful benchmarking is necessary but not sufficient for a sample to be representative. The other way to establish whether a sample is representative, besides benchmarking, is evaluating the sampling process. That means understanding how exactly the observations were selected, what rules were supposed to be followed, and to what extent those rules were followed. To understand what good sampling methods are, the next section introduces the concept of random sampling, argues why it leads to representative samples, and provides examples of random samples. Random Sampling Random sampling is the process that most likely leads to representative samples. With the simplest ideal random sampling, all observations in the population have the same chance of being selected into the sample. In practice that chance can vary. Which observations are selected is determined by a random rule. For the purpose of getting representative samples, selection rules are random if they are not related to the distribution of the variables in the data. Textbook examples of random rules include throwing dice or drawing balls from urns. In practice, most random samples are selected with the help of random numbers generated by computers. These numbers are parts of a sequence of numbers that is built into the computer. The sequence produces numbers without a recognizable pattern. Where the sequence starts is either specified by someone or determined by the date and time the process is launched. In a sense these numbers are not truly random as they always come up the same if started from the same point, in contrast with repeatedly throwing dice or drawing balls from urns. Nevertheless, that is not a real concern here because this selection rule is unrelated to the distribution of variables in any real-life data. Other methods of random sampling include fixed rules that are unrelated to the distribution of variables in the data. Good examples are selecting people with odd-numbered birth dates (a 50% sample), or people with birthdays on the 1 5th of every month (approx. 3% sample). Again, these rules may not be viewed as "truly random" in a stricter sense, but that's not a concern for representation as long as the rules are not related to the variables used in the analysis. In contrast, non-random sampling methods may lead to selection bias. Non-random sampling methods are related to important variables. In other words, they have a higher or lower likelihood   of selecting observations that are different in some important variables. As a result, the selected observations tend to be systematically different from the population they are drawn from. Consider two examples of non-random sampling methods. Selecting people from the first half of an alphabetic order is likely to lead to selection bias because people with different names may belong to different groups of society. Selecting the most recently established 10% of firms is surely not random for many reasons. One reason is called survivor bias: newly established firms include those that would fail within a short time after their establishment while such firms are not present among older firms. The practice questions will invite you to evaluate particular sampling methods and come up with other good and not-so-good methods. Random sampling works very well if the sample is large enough. In small samples, it is possible that by chance, we pick observations that are not representative of the population. Consider for instance whether samples represent the age distribution of the population of a city. By picking a sample of two people, the share of young and older people may very well be different from their shares in the entire population. Thus, there is a considerable chance that this sample ends up being not representative even though it's a random sample. However, in a random sample of a thousand people, the share of young and old people is likely to be very similar to their shares in the population, leading to a representative sample. The larger the sample, the larger the chance of picking a representative sample. An important, although not necessarily intuitive, fact is that it is the size of the sample that matters and not its size as a proportion of the entire population. A sample of five thousand observations may equally well represent populations of fifty thousand, ten million, or three hundred million. Quite naturally, the larger the random sample, the better But real life raises other considerations such as costs and time of data collection. How large a random sample is large enough depends on many things. We shall return to this question when we first discuss inference from samples to populations, in Chapter 5. Random sampling is the best method of producing representative samples. True, it is not bulletproof, with a tiny chance a sample may be way off. But that tiny chance is really tiny, especially for large samples. Nevertheless, the fact that it is not literally bullet-proof makes some people uncomfortable when they first encounter it. In fact, it took a lot of evidence to convince most data users of the merits of random sampling. In practice, sampling often starts with a sampling frame: the list of all observations from which the sample is to be drawn. Incomplete and biased coverage may arise at this stage: the sampling frame may not include the entire population or may include observations that are not part of the population to be represented. Ideally, data is collected from all members of a sample. Often, however, that is not possible. Surveys need respondents who are willing to participate and answer the questions. The fraction of people that were successfully contacted and who answered the questionnaire is called the response rate. A low response rate increases the chance of selection bias. That is, of course, not necessarily true: a sample with an 80% response rate may be more biased than another sample with a 40% response rate. It is good practice to report response rates with the data description and, if possible, to benchmark variables available in the population. Review Box 1.5 Basic concepts of sampling • The set of all observations relevant for the analysis is called the population. • The subset for which data is collected is called a sample. e A representative sample has very similar distributions of all variables to that of the population. 1.C3 Case Study 21 • To assess if a sample is representative we can benchmark statistics available both in the sample and the population. It is a good sign if they are similar, but it's not a guarantee. • Random sampling means selecting observations by a rule that is unrelated to any variable in the data. • Random sampling is the best way to get a representative sample. • Incomplete sampling frames and nonresponse are frequent issues; whether they affect the representative nature of the sample needs to be assessed. CASE STUDY - Comparing Online and Offline Prices: Data Collection The process of sampling The BPP online—offline prices project carried out data collection in ten countries. In each country, it selected the largest retailers that sold their products both online and offline and were in the top twenty in terms of market share in their respective countries. The set of all products in these stores is the sampling frame. Sampling of products was done in the physical stores by the data collectors. The number of products to include was kept small to ensure a smooth process (e.g., to avoid conflicts with store personnel). The sample of products selected by data collectors may not have been representative of all products in the store. For example, products in more eye-catching displays may have been more likely to be sampled. At the same time, we have no reason to suspect that the online—offline price differences of these products were different from the rest, Thus, the sample of products may very well be representative of the online—offline price differences of the entire population, even though it may not be representative of the products themselves. This case study asked how to collect data so that we could compare online and offline prices. With careful sampling, and a massive effort to ensure that the very same products are compared, the project did a good job in data collection. A potential shortcoming is the fact that the products collected may not be fully representative of the consumption basket.

CASE STUDY — Management Quality and Firm Size: Data Collection

```
	22	Origins of Data
USA and the Bureau van Dijk's Orbis/Amadeus in the European countries. The sampling frame was then adjusted by keeping only firms in selected sectors and of medium size (50 to 10000 employees). The survey took a random sample. The response rate was 54%. This is considered a relatively high rate provided that participation was voluntary and respondents received no compensation.
```

The data collectors benchmarked many variables and concluded that there were two deviations: response rate was smaller in one country than the rest, and it was usually higher in larger firms. The distribution of other variables were similar in the sampling frame and the sample. The project aimed to collect systematic data on management quality that could be compared across firms and countries. The way it was set up created a unified data collection process across countries. Systematic checks were introduced to avoid bias in collecting answers. A possible shortcoming is biased selection due to nonresponse: firms that answered the survey may not fully represent all firms in the surveyed countries.

```
		Big Data
```

Data, or less structured information that can be turned into data, became ubiquitous in the twenty-first century as websites, apps, machines, sensors, and other sources, collect and store it in increasing and unfathomable amounts. The resulting information from each source is often massive to an unprecedented scale. For example, scanned barcodes at retail stores can lead to millions of observations for a retail chain in a single day. For another example, Twitter, a social media site, generates 500 million tweets per day that can be turned into data to analyze many questions. The commonly used term for such data is Big Data. It is a fairly fluid concept, and there are several definitions around. Nevertheless, most data analysts agree that Big Data provides unique opportunities but also specific challenges that often need extra care and specific tools. A frequently used definition of Big Data is the four Vs: volume (scale of data), variety (different forms), velocity (real-time data collection), and veracity (accuracy). The fourth v is actually a question of data quality that we think is better left out from the definition. Rephrasing the first three of the Vs, Big Data refers to data that is: • massive: contains many observations and/or variables; • complex: does not fit into a single data table; • continuous: often automatically and continuously collected and stored. Big Data is massive. Sometimes that means datasets whose size is beyond the ability of typical hardware and software to store, manage, and analyze it. A simpler, and more popular version, is that Big Data is data that we cannot store on our laptop computer Big Data often has a complex structure, making it rather difficult to convert into data tables. A variety of new types of data have appeared that require special analytical tools. For example, networks have observations that are linked to each other, which may be stored in various forms. Maps are multidimensional objects with spatial relationship between observations. Text, pictures, or video content may be transformed into data, but its structure is often complex. In particular, text mining has become an important source of information both for social scientists and business data scientists.

  1.9 Big Data 23 Big Data is often automatically and continuously collected. Apps and sensors collect data as part of their routine, continuously updating and storing information. As part of the functioning of social media or the operation of machines such as airplane turbines, all the data is stored. Big Data almost always arises from existing information as opposed to being collected purposely by analysts. That existing information typically comes from administrative sources, transactions, and as other by-products of day-to-day operations. Thus Big Data may be thought of as admin data with additional characteristics. As a result, it tends to share the advantages and disadvantages of admin data. A main advantage of Big Data, shared with other kinds of admin data, is that it can be collected from existing sources, which often leads to low costs. The volume, complexity, or continuous updating of information may make Big Data collection more challenging, but there are usually appropriate methods and new technology to help there. Coverage of Big Data is often high, sometimes complete. Complete coverage is a major advantage. When coverage is incomplete, though, the left-out observations are typically different from the included ones, leading to selection bias. In other words, Big Data with incomplete coverage is rarely a random sample of the population we'd like to cover. To re-iterate a point we made earlier with respect to non-coverage: higher coverage does not necessarily mean better representation. In fact a relatively small random sample may be more representative of a population than Big Data that covers a large but selected subset of, say, 80 0/0. Thus, if its coverage is incomplete, Big Data may be susceptible to selection bias — something that data analysts need to address. Another common feature of Big Data, shared with admin data, is that it may very well miss important variables, and the ones included may not have high validity for the purpose of our analysis. That can be a major disadvantage. At the same time, because of the automatic process of information gathering, the variables tend to be measured with high reliability and in comparable ways. The specific characteristics of Big Data have additional implications for data management and analysis as well as data quality. The massive size of Big Data can offer new opportunities, but it typically requires advanced technical solutions to collect, structure, store, and work with the data. Size may have implications for what data analysis methods are best to use and how to interpret their results. We'll discuss these implications as we go along. Here we just note that sometimes when the data is big because there are many observations, all analysis can be done on a random sample of the observations. Sometimes, going from a massive number of observations (say, billions or trillions) to a large but manageable number of observations (say, a few millions) can make the analysis possible without making any difference to the results. This is not always an option, but when it is, it's one to consider. We'll see an example for such massive data when we analyze the effect of a merger between two airlines on prices in Chapter 22. The original data is all tickets issued for routes in the USA. A 10 % sample of the data is made available for public use (without personal information). Even this sample data is available in multiple data files. We'll have to select parts of the data and aggregate it to apply the methods we'll cover in the textbook. If Big Data is of a complex nature, this has consequences for the management and structuring of the data. Sometimes, with thorough re-structuring, even complex data can be transformed into a single data table that we can work with. For example, connections in a network may be represented in panel data, or features of texts may be summarized by variables, such as the frequency of specific words, that can be stored as a data table. Other times, though, complexity calls for methods that are beyond what we cover in this textbook and may also be beyond the traditional toolkit of statistical analysis.

  24 Origins of Data For example, the features of routes on maps or the sound of the human voice are less straightforward to transform into data tables. With Big Data that is continuously collected and updated, the process of data work and analysis is different from the more traditional way of doing data analysis that we'll focus on in this textbook. For example, instead of having a final data table ready for analysis, such cases require constant updating of the data and with it, updating all analysis as new data comes in. This approach implies some fundamental differences to data analysis that are beyond the scope of this textbook. In the remainder of this textbook, we will focus on the most common kind of Big Data: very large numbers of observations. In addition, we'll note some issues with data that has a massive number of variables. We'll ignore complexity and continuous data collection. And we won't discuss technical issues such as the need for additional computational power or specific data management tools to store and manipulate data with billions of observations. Our focus will be on what the massive number of observations, or sometimes variables, implies for the substantive conclusions of data analysis. A final comment: most of the traditional, "small data" issues and solutions we will discuss in this textbook will remain relevant for Big Data as well. We shall always note when that is not the case. Similarly, when relevant, we shall always discuss the additional issues Big Data may imply for the methods and tools we cover. Review Box 1.6 Big Data • Big Data is characterized by a massive number of observations or variables. • Sometimes Big Data is also complex in its structure and/or continuously updated. • Typically, Big Data is collected for purposes other than analysis. Thus it shares all the advantages and disadvantages of other kinds of admin data. • Its size, complexity, and continuous updating present additional challenges for its collection, structuring, storage, and analysis. Good Practices in Data Collection Several good practices in data collection are recognized to increase or help assess data quality. Some are general across many methods; others are specific. Carrying out one or more pilot studies before data collection is general advice. To pilot a data collection method means to try it out in microcosm before doing the whole thing. Piloting is more powerful the more features of the final data collection are included. In web scraping this may mean small-scale collection of data from all websites across all kinds of items that will be relevant. In web surveys it may include recruiting a few respondents as well as asking them to fill out the entire questionnaire. With complex data collection, piloting may come in several steps, such as identifying the sampling frame, drawing a sample, identifying observations or recruiting respondents, and collecting the data itself by scraping, interviewing. Sometimes these steps are given different names as they get to include more and more parts of the entire data collection process (pilot, pretest, field rehearsal, and so on).

```
1.10 Good Practices in Data Collection	25
```

When people are involved in data collection, it is good practice to give them precise instructions to follow. An important objective of these is to get the actual content of measured variables as close as possible to their intended contents, thus increasing their validity. These practices also help with comparability and reliability by inducing different people to measure similar things in similar ways. For example, in interview-based surveys, precise instructions usually include questions to be read out (as opposed to letting interviewers ask questions using their own words), when and exactly how to clarify things, and how to translate answers into what is to be recorded. Instructions need to be easy to follow, so a balance needs to be found in how detailed and how accessible instructions are. Another good practice is training people that participate in data collection in how to follow those instructions and how to make other kinds of decisions. Good training involves many hands-on exercises with examples that are likely to come up during data collection. Training of interviewers for complex surveys may take several days and is often very costly Nevertheless, it is important to give thorough training to people involved in the data collection in order to ensure high data quality. Less frequent but very useful practices aim at assessing data quality as part of the data collection. For example, the validity of measures in surveys may be assessed with the help of cognitive interviews. These ask respondents to explain why they answered a survey question the way they did. Another technique is asking a survey question in slightly different ways for different respondents (or the same respondents) to see if differences in wording that should not matter make a difference in the answers. A useful practice to evaluate reliability is test—retest measurement: measuring the same thing more than once within the same data collection process. For example, the price of the same product in the same store may be recorded by two people, independent of each other. Or the same question may be asked of the same respondent twice within the same questionnaire, preferably with many questions in-between. Such a test-retest measurement took place within the World Management Survey: it re-interviewed several hundred firms to assess the reliability of the management quality scores. There are good practices that help assess coverage issues, too. Whether nonresponse in a survey leads to severe biases may be assessed by giving some of the would-be respondents higher incentives to participate. If that results in a higher response rate, we may compare the distributions of variables across respondents with and without the extra incentives to see if different response rates lead to different distributions. There are many other techniques and practices that data collection may include to assess various dimensions of data quality. Making use of all is practically impossible. Nevertheless, it can be very useful to include one or two of them if data collectors are concerned with one or two issues in particular The results of these techniques can not only shed light on the extent of particular issues but they may be used to mitigate their consequences in the course of the analysis. Very often, data collection is a complex task. Teamwork here is especially useful as designing and implementing data collection may require a wide range of expertise. The more complex the process, the larger the benefits of advice and collaboration. However, even seemingly simple data collection tasks may have issues that inexperienced researchers are not aware of and can result in inferior data quality. Thus, we think it always makes sense to seek advice and, if needed, mentoring during all stages of data collection. Garbage in, garbage out: if the data we collect ends up having crucial flaws, our analysis will not be able to answer our question. It's better to minimize that possibility if we can.

26 Origins of Data Review Box 1.7 Some good practices for data collection Piloting data collection. • Assessing the validity and reliability of the most important variables by, for example, cognitive interviews or test-retest measurement - when feasible and economical. • Examining sources of imperfect coverage to assess potential selection bias. Working in teams with experts to design data collection. Ethical and Legal Issues of Data Collection Observations in data analyzed in business, economics, or policy most often concern people or firms. Collecting and using such data is subject to strong legal and ethical constraints and rules. These constraints and rules are meant to protect the subjects of data collection: the business interests of firms, the physical and mental health, safety, and integrity of people. Observing these constraints and rules is extremely important: breaching them can have severe consequences for the ongoing data collection and beyond. When the rules are not observed, firms or people may decline participation or take legal action during or after data collection. These, in turn, may affect not only the ongoing data collection but also the general attitude of potential respondents toward data collection in society. One general principle is confidentiality. In general, data users should not be able to learn sensitive information about identifiable subjects of the data collection. Sensitive information includes, but is not restricted to, information that may harm firms or individuals. When the data contains sensitive information, the principle of confidentiality implies that respondents should be impossible to identify. At a minimum that means that data needs to be de-identified: names and addresses should not be part of the dataset. But it is more than that. Some variables, or combinations of variables, may help identify firms or persons even without names and addresses. Ensuring confidentiality also means ensuring that no such combination allows respondents to be identified from observations. The collection of data on people, or, as sometimes referred to, human subjects, is subject to a large body of regulation at both international and national levels. The regulation originates from medical research, but it has been adapted for data collection for non-medical purposes, too. The most important guiding principles include respect for persons (people should be treated as autonomous human beings), their protection from harm, and their equal treatment during data collection. It is good practice to obtain informed consent from people for collecting data on them. This means not only data collected here and now but also potential linkages to other data sources. In fact, when data collection is supported by research grants from government or non-governmental foundations, these, and many more, principles are required to be observed. Another general principle is ownership of information. A lot of information and a lot of data is available on the web or offline. However, availability does not imply the right to analyze that data for any purpose. Who owns that information and what the owner permits to be done with that information is not always easy to find out. Nevertheless, we should always aim to understand the details of ownership and usage rights to make sure it is ethical and legal to collect and use the data. The rules of data collection are complex. One seemingly good way to think about these issues is to consider ourselves to be a subject of the data collection and think about what protection we would need to feel safe and be willing to participate. Another seemingly good starting point is to consider whether similar data was collected recently. But these are not enough. Practices may not be OK just

```
1.12 Main Takeaways	27
```

because we, as data analysts, would feel comfortable with them, or a recent data collection project got away with them. Instead, it is strongly advised to consult experts in the legal and ethical aspects of data collection. Review Box 1.8 Ethical and legal principles • Ethical and legal rules need to be fully observed; consulting experts is good practice before collecting data. • Important rules include ensuring confidentiality and observing ownership rights. i

```
1.12	Main Takeaways
```

Know your data: how it was born and what its major advantages and disadvantages are to answer your question. Data quality is determined by how the data was born. Data is stored in data tables, with observations in rows and variables in columns. PRACTICE QUESTIONS 1 . What are in the rows and columns of a data table? What are ID variables? 2. What are xsec, tseries, and Xt panel data? What's an observation in each? Give an example for each. 3. What's the validity and what's the reliability of a variable? Give an example of a variable with high validity and one with low validity. 4. What's selection bias? Give an example of data with selection bias and are without. 5. List two common advantages of admin data and two potential disadvantages. 6. How can we tell if a sample is representative of a population? 7. List two sampling rules that likely lead to a representative sample and two sampling rules that don't. 8. List three common features of Big Data. Why does each feature make data analysis difficult? 9. An important principle for research is maintaining confidentiality. How can we achieve that when we collect survey data? 10. You want to collect data on the learning habits of students in your data analysis class. List two survey methods that you may use and highlight their advantages and disadvantages. 1 1. You want to collect data on the friendship network of students in a class. You consider two options: (1) collect their networks of Facebook users using data there (80% of them are on Facebook), or (2) conduct an online survey where they are asked to mark their friends from a list of all students. List arguments for each option, paying attention to representation, costs, and ethical issues. 12. You consider surveying a sample of employees at a large firm. List four selection methods and assess whether each would result in a representative sample.

