# Big Data

"Data, or less structured information that can be turned into data, became ubiquitous in the twenty-first century as websites, apps, machines, sensors, and other sources, collect and store it in increasing and unfathomable amounts. The resulting information from each source is often massive to an unprecedented scale. For example, scanned barcodes at retail stores can lead to millions of observations for a retail chain in a single day. For another example, Twitter, a social media site, generates 500 million tweets per day that can be turned into data to analyze many questions. The commonly used term for such data is Big Data. It is a fairly fluid concept, and there are several definitions around. Nevertheless, most data analysts agree that Big Data provides unique opportunities but also specific challenges that often need extra care and specific tools. A frequently used definition of Big Data is the four Vs: volume (scale of data), variety (different forms), velocity (real-time data collection), and veracity (accuracy). The fourth v is actually a question of data quality that we think is better left out from the definition.&#x20;

Rephrasing the first three of the Vs, Big Data refers to data that is: **massive**: contains many observations and/or variables; **complex**: does not fit into a single data table; **continuous**: often automatically and continuously collected and stored. Big Data is massive. Sometimes that means datasets whose size is beyond the ability of typical hardware and software to store, manage, and analyze it. A simpler, and more popular version, is that Big Data is data that we cannot store on our laptop computer Big Data often has a complex structure, making it rather difficult to convert into data tables. A variety of new types of data have appeared that require special analytical tools. For example, networks have observations that are linked to each other, which may be stored in various forms. Maps are multidimensional objects with spatial relationship between observations. Text, pictures, or video content may be transformed into data, but its structure is often complex. In particular, text mining has become an important source of information both for social scientists and business data scientists.

Big Data is often automatically and continuously collected. Apps and sensors collect data as part of their routine, continuously updating and storing information. As part of the functioning of social media or the operation of machines such as airplane turbines, all the data is stored. Big Data almost always arises from existing information as opposed to being collected purposely by analysts. That existing information typically comes from administrative sources, transactions, and as other by-products of day-to-day operations. Thus Big Data may be thought of as admin data with additional characteristics. As a result, it tends to share the advantages and disadvantages of admin data. A main advantage of Big Data, shared with other kinds of admin data, is that it can be collected from existing sources, which often leads to low costs. The volume, complexity, or continuous updating of information may make Big Data collection more challenging, but there are usually appropriate methods and new technology to help there. Coverage of Big Data is often high, sometimes complete. Complete coverage is a major advantage. When coverage is incomplete, though, the left-out observations are typically different from the included ones, leading to selection bias. In other words, Big Data with incomplete coverage is rarely a random sample of the population we'd like to cover. To re-iterate a point we made earlier with respect to non-coverage: higher coverage does not necessarily mean better representation. In fact, a relatively small random sample may be more representative of a population than Big Data that covers a large but selected subset of, say, 80 0/0. Thus, if its coverage is incomplete, Big Data may be susceptible to selection bias â€” something that data analysts need to address. Another common feature of Big Data, shared with admin data, is that it may very well miss important variables, and the ones included may not have high validity for the purpose of our analysis. That can be a major disadvantage. At the same time, because of the automatic process of information gathering, the variables tend to be measured with high reliability and in comparable ways. The specific characteristics of Big Data have additional implications for data management and analysis as well as data quality. The massive size of Big Data can offer new opportunities, but it typically requires advanced technical solutions to collect, structure, store, and work with the data. Size may have implications for what data analysis methods are best to use and how to interpret their results. We'll discuss these implications as we go along. Here we just note that sometimes when the data is big because there are many observations, all analysis can be done on a random sample of the observations. Sometimes, going from a massive number of observations (say, billions or trillions) to a large but manageable number of observations (say, a few millions) can make the analysis possible without making any difference to the results. This is not always an option, but when it is, it's one to consider. We'll see an example for such massive data when we analyze the effect of a merger between two airlines on prices in Chapter 22. The original data is all tickets issued for routes in the USA. A 10 % sample of the data is made available for public use (without personal information). Even this sample data is available in multiple data files. We'll have to select parts of the data and aggregate it to apply the methods we'll cover in the textbook. If Big Data is of a complex nature, this has consequences for the management and structuring of the data. Sometimes, with thorough re-structuring, even complex data can be transformed into a single data table that we can work with. For example, connections in a network may be represented in panel data, or features of texts may be summarized by variables, such as the frequency of specific words, that can be stored as a data table. Other times, though, complexity calls for methods that are beyond what we cover in this textbook and may also be beyond the traditional toolkit of statistical analysis.

For example, the features of routes on maps or the sound of the human voice are less straightforward to transform into data tables. With Big Data that is continuously collected and updated, the process of data work and analysis is different from the more traditional way of doing data analysis that we'll focus on in this textbook. For example, instead of having a final data table ready for analysis, such cases require constant updating of the data and with it, updating all analysis as new data comes in. This approach implies some fundamental differences to data analysis that are beyond the scope of this textbook. In the remainder of this textbook, we will focus on the most common kind of Big Data: very large numbers of observations. In addition, we'll note some issues with data that has a massive number of variables. We'll ignore complexity and continuous data collection. And we won't discuss technical issues such as the need for additional computational power or specific data management tools to store and manipulate data with billions of observations. Our focus will be on what the massive number of observations, or sometimes variables, implies for the substantive conclusions of data analysis. A final comment: most of the traditional, "small data" issues and solutions we will discuss in this textbook will remain relevant for Big Data as well. We shall always note when that is not the case. Similarly, when relevant, we shall always discuss the additional issues Big Data may imply for the methods and tools we cover." (Bekes, 2021)&#x20;
