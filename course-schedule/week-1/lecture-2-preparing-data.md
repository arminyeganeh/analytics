---
description: Lecture A1.2, 1500 words, 1 hour to complete
---

# Preparing Data

&#x20;

CASE STUDY — Identifying Successful Football Managers Question and data In this case study, we are interested to identify the most successful football managers in England. We'll extend this case study to focus on the impact of replacing managers later, in Chapter 24. We combine data from two sources for this analysis, one on teams and games, and one on managers. Our focus in this chapter is how to combine the two data sources and what problems may arise while doing so. Before we start, let us introduce some concepts. We will talk about football, called soccer in the USA and a few other countries. In England and some other countries, coaches are called managers — as they take on managerial duties beyond coaching. We will focus on the English Premier League (EPL, for short) — the top football division in England. A season runs for about 9 months, from mid-August to mid-May, and the league consists of 20 teams each season. Our data cover 1 1 seasons of EPL games — 2008/2009 to 2018/2019 - and come from the publicly available Football-Data.co.uk website. At the end of every season, some teams are relegated to the second division, while some are promoted to join the EPL from the second division. In this 1 1 -season long period, some teams feature in all seasons (such as Arsenal, Chelsea, or Manchester United), while others play in the EPL just once (such as Cardiff). Each observation in the data table is a single game. Key variables are the date of the game, the name of the home team, the name of the away team, the goals scored by the home team, and the goals scored by the away team. Each team features 19 times as "home team" and 19 times as "away team." We have 380 rows for each season. With 1 1 seasons, the total number of observations is 4180. A small snippet of the data is presented in Table 2.6. Is this a tidy data table? It is; each observation is a game, and each game is a separate row in the data table. Three ID variables identify each observation: date, home team, away team. The Table 2.6 Football game results — game-level data Date Home team Away team Home goals Away goals

```
2018-08-19	Brighton	Man United	3	2
2018-08-19	Burnley	Watford	1	3
2018-08-19	Man City	Huddersfield	6	1
2018-08-20	Crystal Palace	Liverpool	0	2

2.C1 Case Study	41
Date	Home team	Away team	Home goals	Away goals
```

2018-08-25 Arsenal West Ham 3 1 2018-08-25 Bournemouth Everton 2 2 2018-08-25 Huddersfield Cardiff

Note: One observation is one game. Source: football dataset. English Premier League, all games, 1 1 seasons from 2008/9 through 2018/9. N=4180. other variables describe the result of the game. From the two scores we know who won, by what margin, how many goals they scored, and how many goals they conceded. But there is an alternative way to structure the same data table, which will serve our analysis better — in this data table, each row is a game played by a team. It includes variables from the perspective of that team: when they played, who the opponent was, and what the score was. That is also a tidy data table, albeit a different one. It has twice as many rows as the original data table: 38 weeks x 20 teams = 760 observations per season, 8360 observations in total. Table 2.7 shows a small part of this tidy data table. Each game appears twice in this data table, once for each of the playing team's perspectives. For each row here we had to introduce a new variable to denote whether the team at that game was the home team or the away team. Now we have two ID variables, one denoting the team, and one denoting the date of the game. The identity of the opponent team is a qualitative variable.

```
Opponent	Opponent Home/
Date	Team	team	Goals	goals	away	Points
```

2018-08-19 Brighton Man United 3 2 home 3 2018-08-19 Burnley Watford 1 3 home 2018-08-19 Man City Huddersfield 6 1 home 3 2018-08-19 Man United Brighton 2 3 away 2018-08-19 Watford Burnley 3 1 away 3 Man City 1 observation is one game for one team. Source: football dataset. English Premier League, all games, 11 seasons from 2008/9 through 2018/9. N=8360. Our second data table is on managers. We collected data on the managers from Wikipedia. One row is one manager—team relationship: a job spell by a manager at a team. So each manager may feature more than once in this data table if they worked for multiple teams. For each observation, we have the name of the manager, their nationality, the name of the team (club), the start time of the manager's work at the team, and the end time. Table 2.8 lists a few rows. Name Nat. Club From Until

Arsene Wenger France Arsenal 1 Oct 1996 13 May 2018 Unai Emery Spain Arsenal 23 May 2018 Present\* Ron Atkinson England Aston Villa 7 June 1991 10 Nov 1994 Brian Little England Aston Villa 25 Nov 1995 24 Feb 1998 John Gregory England Aston Villa 25 Feb 1998 24 Jan 2002 Dean Smith England Aston Villa 10 2018 Present\* Alan Pardew England Crystal Palace 2 Jan 2015 22 Dec 2016 Alan Pardew England Newcastle 9 Dec 2010 2 Jan 2015

Note: Managers in EPL. One observation is a job spell by a manager at a team. \* As of September 2019. Source: football dataset. N=395. As we can see, some managers had a long spell, others a shorter spell at teams. Some managers coached more than one team: Alan Pardew, for instance, worked for both Crystal Palace and Newcastle. We have 395 observations for 241 managers. So we have a relational dataset. It has one data table with team—game observations, and one data table with manager-team observations. The first data table contains dates for the games; the second data table contains the start dates and end dates for the time each manager worked for each team. To work with the information in the two data tables together, we need to create a workfile, which is a single data table that is at the team-game level with the additional variable of who the manager was at the time of that game. We have all the information to link managers to team-games: which manager was in charge at the time a team played a game. But creating that linkage is not straightforward. We discuss some of the problems and its solutions in the next section, and we'll return to our case study afterwards. Entity Resolution: Duplicates, Ambiguous Identification, and Non-entity Rows In many data tables that we start working with, we may observe strange things: the ID variable is not unique when it should be, we appear to have multiple observations with different ID variables for entities that should be the same, or we may have rows that are not for the kinds of entities we want. These are issues with the entities in the data table. Before doing any meaningful analysis, such issues need to be resolved, to the extent it is possible. That process is called entity resolution. One potential issue is having duplicate observations, or, simply, duplicates, in the data table. Duplicates appear when some entities that should be a single observation appear more than once in the data table. While the name suggests two appearances, it may refer to three or more appearances as well. Duplicates may be the result of human error (when data is entered by hand), or the features of the data source (e.g., data scraped from classified ads with some items posted more than once). When possible, duplicates need to be reduced to a single observation. In the simplest case, duplicates are perfect: the value of all variables is the same. In such cases we have to delete the duplicates and leave one data row for each observation only. In more difficult cases, the value of one or more variables is different across data rows that appear to correspond to the same observation. Then, we have to make a decision about whether to keep all observations or select one, and if the latter, which value to keep — or maybe create a new value, such as the average. A related, but conceptually different issue is to have ambiguous identification: the same entity having different IDs across different data tables. The task here is to make sure that each entity has the same ID across data tables. That is necessary to link them properly. Entities are frequently identified by names. Unfortunately, though, names may cause issues for two main reasons: they are not unique (more than one person may be called John Smith), and different data tables may have different versions of the same name (e.g., middle names sometimes used, sometimes not: Ronald Fisher, Ronald A. Fisher, and Sir Ronald Aylmer Fisher is the same person, a famous statistician). This task is called disambiguation: making identification of entities not ambiguous. Yet another issue is having non-entity observations: rows that do not belong to an entity we want in the data table. Examples include a summary row in a table that adds up variables across some entities. For example, a data table downloaded from the World Bank on countries often includes observations on larger regions, such as Sub-Saharan Africa, or the entire World, and they are included just like other rows in the data table together with the rows for individual countries. Before any meaningful analysis can be done, we need to erase such rows from the data table. 2.C2 Case Study

The important message is this: assigning unique IDs is important. As a ground rule, we should avoid names as ID since people or firm names are not unique (remember, even the two authors have the same first name) and can be frequently misspelled. Using numerical ID variables is good practice. Finally, note that, very often, there is no single best solution to entity resolution. We may not be 100% certain if different rows in the data table belong to the same real-life observation or not. Moreover, often it is not evident what to do when different values of a variable show up for the same real-life observation. As a result, our clean data table may end up not really 100 percent clean. But that is all right for the purpose of data analysis. Data analysts have to learn to live with such imperfections. When the magnitude of the problem is small, it is unlikely to have a substantial effect on the results of our subsequent analysis. When the magnitude of the problem is large, we may have to try different versions of solving it, and see if they have an effect on the results of our analysis. 2.C2 CASE STUDY — Identifying Successful Football Managers Entity resolution Let us look at data issues for the football manager case study and see if we need to resolve entities. Indeed, the data does have ambiguous entities that we need to deal with. For instance, consider Manchester City and Manchester United, the two major football teams from the city of Manchester. When we looked through official sites, news reports, and datasets, we could find many different versions of how teams were named, as summarized in Table 2.9. Entity resolution here is defining unique IDs and deciding which names do actually belong to the same team.

Table 2.9 Different names of football teams in Manchester, UK Team ID Unified name Original name 19 Man City Manchester City 19 Man City Man City 19 Man City Man. City 19 Man City Manchester City F.C. 20 Man United Manchester United 20 Man United Manchester United F.C. 20 Man United Manchester United Football Club 20 Man United Man United

Source: Various sources including source for the football dataset. For the manager names, we sometimes see one or more space characters in the name, and sometimes we don't see them. Another issue is whether and how accents are included in names, such as "Arséne Wenger", a former manager of Arsenal. On a very few occasions, the manager's name may be missing. One example is that there are no records for the team Reading for mid-March 2013. In our case this comes from a caretaker manager who somehow was not recorded. In such cases we can create a separate ID, with the name missing. Having made these corrections and combined the datasets, we can create unambiguous ID variables for teams in the first data table and managers and teams in the second data table. With these ID variables and the dates of the games and the date for the managers' job spells, we can create a workfile by joining the managers data table to the team—games data table of the appropriate date. This procedure is not straightforward, and it can be done in multiple ways. You are invited to check our code on the textbook website to see one way to do it. With the workfile at hand, we can describe it. The workfile has 8360 team—game observations: in each of the 1 1 seasons, 20 teams playing 38 games (1 9 opponent teams twice; 11 x 20 x 19 x 2 8360). For these 1 1 seasons, we have 124 managers in the data. From this case study, we learned that entity resolution, having the data structured right, and having proper ID variables are essential when working with relational data. Only with clearly identified teams and managers can we hope to match managers to teams and the games they play.

```
Discovering Missing Values
```

With tidy data and no issues with entities to resolve, we can turn our attention to the actual content of variables. And there may be issues there, too. A frequent and important issue with variables is missing values. Missing values mean that the value of a variable is not available for some observations. They present an important problem, for three main reasons. First, missing values are not always straightforward to identify, and they may be mistakenly interpreted as some valid value. That is not a problem when missing values are represented by a specific character in the data, such as "NA" (for "not available"), a dot , or an empty space Statistical software recognizes missing values if stored appropriately. Sometimes, however, missing values are 2.8 Discovering Missing Values recorded with number values, outside the range (e.g., 0 for no, 1 for yes, 9 for missing). Such values need to be replaced with a value that the software recognizes as missing. Identifying missing values and storing them in formats recognizable by the statistical software is always a must. It should be the first step for dealing with missing values, and it needs to be done even if it affects one single observation. The second issue with missing values is that they mean fewer observations in the data with valid information. As we shall see in subsequent chapters, the number of observations is an important determinant of how confidently we can generalize our results from the particular dataset to the situation we truly care about. When a large fraction of the observations is missing for an important variable in the analysis, we have a lot fewer observations to work with than the size of the original dataset. The magnitude of the problem matters in two ways: what fraction of the observations is affected, and how many variables are affected. The problem is small if values are missing for one or two variables and only a few percent of the observations for each variable. The problem is bigger the larger fraction is missing and/or the more variables affected. For an example with small missing rates of many variables, consider a dataset on 1000 people with 50 variables that aim to capture personality characteristics. For each of these variables, 2% of the observations have missing values — i.e., we do not have information on a given characteristic for 20 of the 1000 people. Suppose, moreover, that for each variable on personal characteristics, the occurrence of missing values is independent across variables. In this case we end up with as few as 360 people with valid values for all 50 variables, which is 36% of the original number of observations (0.98 raised to the power of 50 = 0.36). This is a large drop even though each missing rate is tiny in itself. The third issue is potential selection bias. One way to think about missing values is that they lead to a dataset used for the analysis that covers fewer observations than the entire dataset. We discussed coverage earlier in Chapter 1, Section 1.3. As always, when coverage is incomplete, an important question is whether this smaller sample represents the larger dataset. When the missing observations are very similar to the included ones, thus there is no selection bias, we say that the observations are missing at random How can we tell whether values are missing at random or there is selection bias? The two approaches that work for assessing whether a sample represents a population work here as well: benchmarking and understanding the selection process. Benchmarking means comparing the distribution of variables that are available for all observations. Think of missing values of a variable y. Then benchmarking involves comparing some statistics, such as the mean or median (see more in Chapter 3, Section 3.6) of variables x, z, each of which is thought to be related to variable y, in two groups: observations with missing y and observations with non-missing y. If these statistics are different, we know there is a problem. Understanding the selection process requires knowing how the data was born and a good understanding of the content of the variables. In some other cases, missing is really just that: no information. Then we should understand why it is so — e.g., why some respondents refused to answer the survey question, or why some companies failed to report a value. However, in some cases missing doesn't really mean missing but zero, only the value zero was not filled in for the relevant observations. For example, a variable for export revenues in company-level data may be left missing for companies that do not have such revenue. But it means zero. When missing values can be replaced with meaningful ones, we should do so. In other cases, missing values decrease the quality of data, just like incomplete coverage. Managing Missing Values Having missing values for some variables is a frequent problem. Whatever the magnitude, we need to do something about them. But what can we do about them? There are two main options. First, we can work with observations that have non-missing values for all variables used in the analysis. This is the most natural and most common approach. It is usually a reasonable choice if the fraction of missing values is not too high, there are few variables affected, and selection bias is not too severe. One version of this approach is to work with observations that make up a well-defined subsample, in which the missing data problem is a lot less severe. For example, if missing values are a lot more prevalent among small firms in administrative data on firms, we may exclude all small firms from the data. The advantage of such a choice is transparency: the results of the analysis will refer to medium and large firms. The second option is filling in some value for the missing values, such as the average value. This is called imputation. Imputation may make sense in some cases but not in others. In any case, imputation does not add information to the data. For that reason, it is usually not advised to do imputation for the most important variables in the analysis. When data analysts use many variables, imputation may make sense for some of them. There are sophisticated methods of imputation, but those are beyond the scope of this textbook. Partly for that reason, we advise against imputing missing values in general. Let us offer three practical pieces of advice regarding managing missing observations. First, when possible, focus on more fully filled variables. Sometimes the variable that best captures the information we need has many missing values, but a less perfect variable is available with few missing variables. For instance, working with data on customers, we may have a variable describing family income for each customer, but that information may be missing for most customers. Instead, we may have information on the zip code of their residence available for virtually all customers. In this case it makes sense not to use the family income variable at all, and instead use a variable on the average family income in zip code locations available from another source. Importantly, when missing values are replaced with some other value, it is good practice to create a binary variable that indicates that the original value was missing. Such a variable is called a flag Second, imputation for qualitative variables should be done differently: these variables have no average value. Instead, we should add an additional category for missing values. For instance, consider the district of a hotel within a city. We would need to add " missing" as a new district value. For ordinal variables, we may also impute the median category and add a new variable denoting that the original value was missing. Third, whatever we choose to do with missing values, we should make a conscious choice and document that choice. Some choices are more reasonable than others, depending on the situation. But all choices have consequences for the analysis. As with other steps of data cleaning, there may not be an obviously best solution to deal with missing values. Such imperfections are a fact of life for data analysts. Magnitudes matter: small issues are likely to have small effects on the results of our analysis; issues that affect many observations may have substantial effects. In the latter case, data analysts often try alternative decisions during the cleaning process and see whether and how they affect their results in the end. 2.A2 Case Study 47 Duplicates and missing values We illustrate the above data problems with the raw hotels-vienna dataset with observations on hotels in Vienna for a weekday in November 2017. Recall that we replaced hotel names with a numerical ID variable for confidentiality reasons. The way we did that ensured that each hotel name corresponds to a single number. Thus, duplicates with hotel names would show up as duplicates in the ID variable, too. And there are duplicates. There are 430 observations in the raw data, yet there are only 428 different ID values. It turns out that the reason for this difference is that there are two hotels that are featured twice in the raw data table. They are listed in Table 2.10, together with the most important variables. ID Accommodationtype Price Distance centerto Stars ratingAvg. Number ratingsof hotel 242 0.0 4 4.8 404 22050 hotel 242 0.0 4 4.8 404 22185 hotel 84 0.8 3 2.2 3 22185 hotel 84 0.8 3 2.2 3 22050

Source: hotels-vienna dataset. Vienna, November 2017 weekday. N=428.

The table shows that these duplicates are of the simple kind: all variables have the same value. To resolve the issue we need to drop one of each of the duplicates. The result is 428 hotels in the data table (this is the number of observations we described in Chapter 1, Section 1 .AI). Next let's turn to missing values. The most important variables have no missing values except for average customer rating, which is missing for 35 of the 428 observations. This is an 8% missing rate. When we dig deeper into the data, we can see that the type of accommodation is strongly

48 related to whether average customer rating is missing. In particular, 34 of the 35 observations with missing values are for apartments, guest houses, or vacation homes. Of the 264 regular hotels in the data, only 1 has a missing value for average customer rating. It's a hotel with 2.5 stars, 0.7 miles from the city center, charging 106 dollars. Later, when we analyze the data to find a good deal, we'll restrict the data to regular hotels with 3 to 4 stars; this hotel would not be in that data (see Chapter 7, Section 7.A1). The Process of Cleaning Data The data cleaning process starts with the raw data and results in clean and tidy data. It involves making sure the observations are structured in an appropriate way and variables are in appropriate formats. Data cleaning may involve many steps, and it's good practice to document every step. An important step of data cleaning is to make sure all important variables are stored in an appropriate format. Binary variables are best stored as 0 or 1 . Qualitative variables with several values may be stored as text or number It is usually good practice to store them as numbers, in which case there should be a correspondence to say what number means what: this is called value labeling . Cleaning variables may include slicing up text, extracting numerical information, or transforming text into numbers. Another important step is identifying missing values and making appropriate decisions on them. As we have described above, we have several options. We may opt to leave them as missing; then missing values need to be stored in a way that the statistical software recognizes. Data cleaning also involves making sure that values of each variable are within their admissible range. Values outside the range are best replaced as missing unless it is obvious what the value was meant to be (e.g., an obvious misplacement of digits). Sometimes we need to change the units of measurement, such as prices in another currency or replace very large numbers with measures in thousands or millions. Units of measurement should always be indicated with the variables. Sometimes it makes sense to keep both the original and the new, generated variable to be able to cross-check them later in the analysis. As part of data cleaning, variable description (also called variable labels) should be prepared showing the content of variables with all the important details. Similarly, when qualitative variables are stored as numbers, value labels that show the correspondence between values and the content of the categories need to be stored as well. In a spreadsheet, those labels should be stored in a separate sheet or document. Some software can include such labels together with the data table. In any case, it is important to be economical with one's time. Often, there are variables in the data that we think we won't use for our analysis. Most of the time it does not make sense to invest time and energy to clean such variables. It is good practice to make the tidy data file leaner, containing the variables we need, and work with that data file. As the raw data is stored anyway, we can always go back and add and clean additional variables if necessary. Data wrangling is part of a larger process. Data analysts start with structuring and cleaning the data, then turn to data description and analysis. They almost always find errors and problems in those latter stages, and they go back to structuring and cleaning. For instance, in some cases, missing values tell us that there is a problem in the data management and cleaning process, and in some cases we 2.11 Reproducible Workflow: Code and Documentation 49 can improve the process and reduce the number of missing values. Maybe we actually made some coding mistakes causing errors, such as mishandled joining of two data tables. Data analysts make many choices during the process of data wrangling. Those choices need to be the results of conscious decisions and have to be properly documented. Often, there are multiple ways to address the same issue, and there may not be a single best solution available. We have seen examples of this for handling apparent duplicates in the data with different values of the same variable, or handling missing values. When the issue in question affects few observations, that ambiguity is unlikely to be a problem. When it affects many observations, it may have effects on the results of our analysis. In such a case it is good practice to try different solutions to the problem and see whether and how that affects the results of the analysis. This process is called robustness checking. Reproducible Workflow: Write Code and Document Your Steps Data wrangling should be executed in a way that is easy to repeat and reproduce. This means documenting what we do and writing code so that steps can be repeated. Documentation is very important. It is good practice to produce a short document, called README, that describes the most important information about the data used in any analysis. This document may be helpful during the analysis to recall important features of the data. Another, often longer, document can describe all data management and cleaning steps we have done. It is also essential to enable other analysts to check your work, replicate your analysis, or build on your analysis in the future (including the future version of yourself). Such a document should allow recalling the steps as well as communicating them. It is important to cite the data source, too. Table 2.1 1 offers a checklist to guide describing datasets.

Birth of data When, how, for what purpose it was collected By whom, and how is the data available Whether it's the result of an experiment Observations Type: cross-sectional, time series, and so on What an observation is, how it is identified, number of observations Variables List of variables to be used, their type, their content, range of values Number or percentage of missing values If a generated variable, how it was created Data cleaning Steps of the process Topic Content to include

It is also useful to write code for for all data wrangling steps. Yes, writing code takes more time than making edits in a spreadsheet or clicking through commands in software. However, investing some time can be rewarding. In most cases, we have to redo data cleaning after new issues emerge or the raw data changes. There the benefits of code tend to massively outweigh its costs: 50

1. It makes it easy to modify part of the data wrangling and cleaning procedure and re-do the entire procedure from the beginning to the end.
2. An automated process can be repeated when needed, perhaps due to a slight change in the underlying raw data.
3. It makes it easy for anyone else to reproduce the procedure thus increasing the credibility of the subsequent analysis.
4. Code becomes the skeleton for documentation: it shows the steps in the procedure itself.
5. Many existing platforms, such as GitHub, assist and promote collaboration. Of course, there are trade-offs between all those benefits and the work needed to write code, especially for short tasks, small datasets, and for novice data analysts. Yes, it's OK to use a spreadsheet to make some quick changes in order to save time. However, we think that it makes sense to write code all the time, or very frequently, even if it seems too big an effort at first sight. We typically don't know in advance what data cleaning decisions we'll have to make and what their consequences will be. Thus, it is quite possible that we would have to re-do the whole process when things change, or for a robustness check, even for a straightforward-looking data cleaning process. Moreover, writing code all the time helps in mastering the coding required for data cleaning and thus helps in future projects. As we have emphasized many times, data cleaning is an iterative process. We start by cleaning and creating a tidy data version and then a workfile. Then, while describing the data and working on the analysis, it is very common to discover further issues that require going back to the data cleaning step. This is yet one more reason to work with code: repeating everything and adding new elements to data cleaning is a lot easier and a lot less time consuming if written in code.

Review Box 2.4 Data wrangling: common steps

1 . Write code — it can be repeated and improved later. 2 Understand the types of observations and what actual entities make an observation. 3 Understand the types of variables and select the ones you'll need. 4 Store data in tidy data tables. 5 Resolve entities: find and deal with duplicates, ambiguous entities, non-entity rows. 6 Get each variable in an appropriate format; give variable and value labels when necessary. 7 Make sure values are in meaningful ranges; correct non-admissible values or set them as missing. 8 Identify missing values and store them in an appropriate format; make edits if needed. 9 Make a habit of looking into the actual data tables to spot issues you didn't think of. 1 0. Have a description of variables. 1 1 . Document every step of data cleaning. Organizing Data Tables for a Project After having discussed how to detect issues with observations and variables in a data table and what to do with such issues, let's turn to how the various data tables should be organized. It is good practice to organize and store the data at three levels. These are: 2.12 Organizing Data Tables for a Project 51 Raw data tables Clean and tidy data tables Workfile(s) for analysis. Raw data, the data as it was obtained in the first place, should always be stored. Raw data may come in a single file or in multiple files. It should be stored in the original format before anything is done to it. This way we can always go back and modify steps of data cleaning if necessary. And, most often it is necessary: something may go wrong, or we may uncover new issues during the process of data cleaning or data analysis. Having the raw data is also key for replicating our analysis. That becomes especially important if similar analyses are to be done in the future. The next step is producing clean and tidy data from raw data. That's the process of data cleaning. It involves making sure each observation is in one row, each variable is in one column, and variables are ready for analysis. This is the long and tedious process of data cleaning that we have discussed in the larger part of this chapter. Often tidy data means multiple data tables. It often makes sense to create different data tables for data coming from different sources. That makes the process of creating tidy data from raw data transparent. We should always create different data tables for data with different kinds of observations. In cross-sectional data this means potentially different kinds of entities, such as individuals, families, neighborhoods, schools of children, employers of workers. In time series data this means potentially different time periods, such as daily observation of prices but weekly observations of quantities. For multi-dimensional data this may mean all of the above, plus different data tables for information at different levels of observations. In cross-sectional time series data we may have data tables with one row for each cross-sectional unit iwithout any change in time, data tables with aggregate time series with observations t (that are the same for all cross-sectional units), and some data tables with observations i, t— in the long format of course. The last of the three levels of the file structure is the workfile. This is the one file on which the analysis is to be done. The rows of workfiles are observations that form the basis of the analysis. Typically, they contain only a subset of all available variables, and, with more than one tidy data file, they may use data from all or only a subset of those files. Workfiles may or may not be in tidy data format. Let us emphasize the advantage of having tidy data before turning them into a workfile for the analysis. Tidy data files tend to be more transparent. Thus they are better suited to identifying problems in the data, addressing those problems and producing clean data, and adding or taking away observations and variables. With tidy data we can add new variables to be included in the analysis, and we can produce various kinds of workfiles for various kinds of analyses. Consider the organization of files for this textbook. For all case studies, we stored raw files as they were collected. The clean folder contains cleaned tidy data tables as well as the code that produces these clean files from the raw ones. The folder for each case study includes the code that creates the workfile and performs the analysis itself. It may also include the workfile as well. We also added an output folder for storing graphs and tables.
