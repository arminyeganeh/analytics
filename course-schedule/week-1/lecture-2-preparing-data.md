---
description: Lecture A1.2, 1500 words, 1 hour to complete
---

# Preparing Data





The table shows that these duplicates are of the simple kind: all variables have the same value. To resolve the issue we need to drop one of each of the duplicates. The result is 428 hotels in the data table (this is the number of observations we described in Chapter 1, Section 1 .AI). Next let's turn to missing values. The most important variables have no missing values except for average customer rating, which is missing for 35 of the 428 observations. This is an 8% missing rate. When we dig deeper into the data, we can see that the type of accommodation is strongly

48 related to whether average customer rating is missing. In particular, 34 of the 35 observations with missing values are for apartments, guest houses, or vacation homes. Of the 264 regular hotels in the data, only 1 has a missing value for average customer rating. It's a hotel with 2.5 stars, 0.7 miles from the city center, charging 106 dollars. Later, when we analyze the data to find a good deal, we'll restrict the data to regular hotels with 3 to 4 stars; this hotel would not be in that data (see Chapter 7, Section 7.A1). The Process of Cleaning Data The data cleaning process starts with the raw data and results in clean and tidy data. It involves making sure the observations are structured in an appropriate way and variables are in appropriate formats. Data cleaning may involve many steps, and it's good practice to document every step. An important step of data cleaning is to make sure all important variables are stored in an appropriate format. Binary variables are best stored as 0 or 1 . Qualitative variables with several values may be stored as text or number It is usually good practice to store them as numbers, in which case there should be a correspondence to say what number means what: this is called value labeling . Cleaning variables may include slicing up text, extracting numerical information, or transforming text into numbers. Another important step is identifying missing values and making appropriate decisions on them. As we have described above, we have several options. We may opt to leave them as missing; then missing values need to be stored in a way that the statistical software recognizes. Data cleaning also involves making sure that values of each variable are within their admissible range. Values outside the range are best replaced as missing unless it is obvious what the value was meant to be (e.g., an obvious misplacement of digits). Sometimes we need to change the units of measurement, such as prices in another currency or replace very large numbers with measures in thousands or millions. Units of measurement should always be indicated with the variables. Sometimes it makes sense to keep both the original and the new, generated variable to be able to cross-check them later in the analysis. As part of data cleaning, variable description (also called variable labels) should be prepared showing the content of variables with all the important details. Similarly, when qualitative variables are stored as numbers, value labels that show the correspondence between values and the content of the categories need to be stored as well. In a spreadsheet, those labels should be stored in a separate sheet or document. Some software can include such labels together with the data table. In any case, it is important to be economical with one's time. Often, there are variables in the data that we think we won't use for our analysis. Most of the time it does not make sense to invest time and energy to clean such variables. It is good practice to make the tidy data file leaner, containing the variables we need, and work with that data file. As the raw data is stored anyway, we can always go back and add and clean additional variables if necessary. Data wrangling is part of a larger process. Data analysts start with structuring and cleaning the data, then turn to data description and analysis. They almost always find errors and problems in those latter stages, and they go back to structuring and cleaning. For instance, in some cases, missing values tell us that there is a problem in the data management and cleaning process, and in some cases we 2.11 Reproducible Workflow: Code and Documentation 49 can improve the process and reduce the number of missing values. Maybe we actually made some coding mistakes causing errors, such as mishandled joining of two data tables. Data analysts make many choices during the process of data wrangling. Those choices need to be the results of conscious decisions and have to be properly documented. Often, there are multiple ways to address the same issue, and there may not be a single best solution available. We have seen examples of this for handling apparent duplicates in the data with different values of the same variable, or handling missing values. When the issue in question affects few observations, that ambiguity is unlikely to be a problem. When it affects many observations, it may have effects on the results of our analysis. In such a case it is good practice to try different solutions to the problem and see whether and how that affects the results of the analysis. This process is called robustness checking. Reproducible Workflow: Write Code and Document Your Steps Data wrangling should be executed in a way that is easy to repeat and reproduce. This means documenting what we do and writing code so that steps can be repeated. Documentation is very important. It is good practice to produce a short document, called README, that describes the most important information about the data used in any analysis. This document may be helpful during the analysis to recall important features of the data. Another, often longer, document can describe all data management and cleaning steps we have done. It is also essential to enable other analysts to check your work, replicate your analysis, or build on your analysis in the future (including the future version of yourself). Such a document should allow recalling the steps as well as communicating them. It is important to cite the data source, too. Table 2.1 1 offers a checklist to guide describing datasets.

Birth of data When, how, for what purpose it was collected By whom, and how is the data available Whether it's the result of an experiment Observations Type: cross-sectional, time series, and so on What an observation is, how it is identified, number of observations Variables List of variables to be used, their type, their content, range of values Number or percentage of missing values If a generated variable, how it was created Data cleaning Steps of the process Topic Content to include

It is also useful to write code for for all data wrangling steps. Yes, writing code takes more time than making edits in a spreadsheet or clicking through commands in software. However, investing some time can be rewarding. In most cases, we have to redo data cleaning after new issues emerge or the raw data changes. There the benefits of code tend to massively outweigh its costs: 50

1. It makes it easy to modify part of the data wrangling and cleaning procedure and re-do the entire procedure from the beginning to the end.
2. An automated process can be repeated when needed, perhaps due to a slight change in the underlying raw data.
3. It makes it easy for anyone else to reproduce the procedure thus increasing the credibility of the subsequent analysis.
4. Code becomes the skeleton for documentation: it shows the steps in the procedure itself.
5. Many existing platforms, such as GitHub, assist and promote collaboration. Of course, there are trade-offs between all those benefits and the work needed to write code, especially for short tasks, small datasets, and for novice data analysts. Yes, it's OK to use a spreadsheet to make some quick changes in order to save time. However, we think that it makes sense to write code all the time, or very frequently, even if it seems too big an effort at first sight. We typically don't know in advance what data cleaning decisions we'll have to make and what their consequences will be. Thus, it is quite possible that we would have to re-do the whole process when things change, or for a robustness check, even for a straightforward-looking data cleaning process. Moreover, writing code all the time helps in mastering the coding required for data cleaning and thus helps in future projects. As we have emphasized many times, data cleaning is an iterative process. We start by cleaning and creating a tidy data version and then a workfile. Then, while describing the data and working on the analysis, it is very common to discover further issues that require going back to the data cleaning step. This is yet one more reason to work with code: repeating everything and adding new elements to data cleaning is a lot easier and a lot less time consuming if written in code.

Review Box 2.4 Data wrangling: common steps

1 . Write code â€” it can be repeated and improved later. 2 Understand the types of observations and what actual entities make an observation. 3 Understand the types of variables and select the ones you'll need. 4 Store data in tidy data tables. 5 Resolve entities: find and deal with duplicates, ambiguous entities, non-entity rows. 6 Get each variable in an appropriate format; give variable and value labels when necessary. 7 Make sure values are in meaningful ranges; correct non-admissible values or set them as missing. 8 Identify missing values and store them in an appropriate format; make edits if needed. 9 Make a habit of looking into the actual data tables to spot issues you didn't think of. 1 0. Have a description of variables. 1 1 . Document every step of data cleaning. Organizing Data Tables for a Project After having discussed how to detect issues with observations and variables in a data table and what to do with such issues, let's turn to how the various data tables should be organized. It is good practice to organize and store the data at three levels. These are: 2.12 Organizing Data Tables for a Project 51 Raw data tables Clean and tidy data tables Workfile(s) for analysis. Raw data, the data as it was obtained in the first place, should always be stored. Raw data may come in a single file or in multiple files. It should be stored in the original format before anything is done to it. This way we can always go back and modify steps of data cleaning if necessary. And, most often it is necessary: something may go wrong, or we may uncover new issues during the process of data cleaning or data analysis. Having the raw data is also key for replicating our analysis. That becomes especially important if similar analyses are to be done in the future. The next step is producing clean and tidy data from raw data. That's the process of data cleaning. It involves making sure each observation is in one row, each variable is in one column, and variables are ready for analysis. This is the long and tedious process of data cleaning that we have discussed in the larger part of this chapter. Often tidy data means multiple data tables. It often makes sense to create different data tables for data coming from different sources. That makes the process of creating tidy data from raw data transparent. We should always create different data tables for data with different kinds of observations. In cross-sectional data this means potentially different kinds of entities, such as individuals, families, neighborhoods, schools of children, employers of workers. In time series data this means potentially different time periods, such as daily observation of prices but weekly observations of quantities. For multi-dimensional data this may mean all of the above, plus different data tables for information at different levels of observations. In cross-sectional time series data we may have data tables with one row for each cross-sectional unit iwithout any change in time, data tables with aggregate time series with observations t (that are the same for all cross-sectional units), and some data tables with observations i, tâ€” in the long format of course. The last of the three levels of the file structure is the workfile. This is the one file on which the analysis is to be done. The rows of workfiles are observations that form the basis of the analysis. Typically, they contain only a subset of all available variables, and, with more than one tidy data file, they may use data from all or only a subset of those files. Workfiles may or may not be in tidy data format. Let us emphasize the advantage of having tidy data before turning them into a workfile for the analysis. Tidy data files tend to be more transparent. Thus they are better suited to identifying problems in the data, addressing those problems and producing clean data, and adding or taking away observations and variables. With tidy data we can add new variables to be included in the analysis, and we can produce various kinds of workfiles for various kinds of analyses. Consider the organization of files for this textbook. For all case studies, we stored raw files as they were collected. The clean folder contains cleaned tidy data tables as well as the code that produces these clean files from the raw ones. The folder for each case study includes the code that creates the workfile and performs the analysis itself. It may also include the workfile as well. We also added an output folder for storing graphs and tables.
