# Reproducible Workflow

"Data wrangling should be executed in a way that is easy to repeat and reproduce. This means documenting what we do and writing code so that steps can be repeated. Documentation is very important. It is good practice to produce a short document, called README, that describes the most important information about the data used in any analysis. This document may be helpful during the analysis to recall important features of the data. Another, often longer, document can describe all data management and cleaning steps we have done. It is also essential to enable other analysts to check your work, replicate your analysis, or build on your analysis in the future. Such a document should allow recalling the steps as well as communicating them. It is important to cite the data source, too.

* **Birth of data:** When, how, for what purpose it was collected, by whom, and how is the data available, whether it's the result of an experiment;
* **Observations:** Type: cross-sectional, time series, and so on, observation definition, how it is identified, number of observations;
* **Variables:** List of variables to be used, type, content, range of values, number or percentage of missing values if a generated variable, how it was created;
* **Data cleaning:** Steps of the process;

It is also useful to write code for all data wrangling steps. Yes, writing code takes more time than making edits in a spreadsheet or clicking through commands in software. However, investing some time can be rewarding. In most cases, we have to redo data cleaning after new issues emerge or the raw data changes ... We typically don't know in advance what data cleaning decisions we'll have to make and what their consequences will be. Thus, it is quite possible that we would have to re-do the whole process when things change, or for a robustness check, even for a straightforward-looking data cleaning process. Moreover, writing code all the time helps in mastering the coding required for data cleaning and thus helps in future projects. As we have emphasized many times, data cleaning is an iterative process. We start by cleaning and creating a tidy data version and then a workfile. Then, while describing the data and working on the analysis, it is very common to discover further issues that require going back to the data cleaning step." (Bekes, 2021)
