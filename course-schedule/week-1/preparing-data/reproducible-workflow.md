# Reproducible Workflow

"Data wrangling should be executed in a way that is easy to repeat and reproduce. This means documenting what we do and writing code so that steps can be repeated. Documentation is very important. It is good practice to produce a short document, called README, that describes the most important information about the data used in any analysis. This document may be helpful during the analysis to recall important features of the data. Another, often longer, document can describe all data management and cleaning steps we have done. It is also essential to enable other analysts to check your work, replicate your analysis, or build on your analysis in the future. Such a document should allow recalling the steps as well as communicating them. It is important to cite the data source, too.

* **Birth of data:** When, how, for what purpose it was collected, by whom, and how is the data available, whether it's the result of an experiment;
* **Observations:** Type: cross-sectional, time series, and so on, observation definition, how it is identified, number of observations;
* **Variables:** List of variables to be used, type, content, range of values, number or percentage of missing values if a generated variable, how it was created;
* **Data cleaning:** Steps of the process;

It is also useful to write code for for all data wrangling steps. Yes, writing code takes more time than making edits in a spreadsheet or clicking through commands in software. However, investing some time can be rewarding. In most cases, we have to redo data cleaning after new issues emerge or the raw data changes. There the benefits of code tend to massively outweigh its costs: 50

1. It makes it easy to modify part of the data wrangling and cleaning procedure and re-do the entire procedure from the beginning to the end.
2. An automated process can be repeated when needed, perhaps due to a slight change in the underlying raw data.
3. It makes it easy for anyone else to reproduce the procedure thus increasing the credibility of the subsequent analysis.
4. Code becomes the skeleton for documentation: it shows the steps in the procedure itself.
5. Many existing platforms, such as GitHub, assist and promote collaboration. Of course, there are trade-offs between all those benefits and the work needed to write code, especially for short tasks, small datasets, and for novice data analysts. Yes, it's OK to use a spreadsheet to make some quick changes in order to save time. However, we think that it makes sense to write code all the time, or very frequently, even if it seems too big an effort at first sight. We typically don't know in advance what data cleaning decisions we'll have to make and what their consequences will be. Thus, it is quite possible that we would have to re-do the whole process when things change, or for a robustness check, even for a straightforward-looking data cleaning process. Moreover, writing code all the time helps in mastering the coding required for data cleaning and thus helps in future projects. As we have emphasized many times, data cleaning is an iterative process. We start by cleaning and creating a tidy data version and then a workfile. Then, while describing the data and working on the analysis, it is very common to discover further issues that require going back to the data cleaning step. This is yet one more reason to work with code: repeating everything and adding new elements to data cleaning is a lot easier and a lot less time consuming if written in code.

Review Box 2.4 Data wrangling: common steps

1 . Write code — it can be repeated and improved later. 2 Understand the types of observations and what actual entities make an observation. 3 Understand the types of variables and select the ones you'll need. 4 Store data in tidy data tables. 5 Resolve entities: find and deal with duplicates, ambiguous entities, non-entity rows. 6 Get each variable in an appropriate format; give variable and value labels when necessary. 7 Make sure values are in meaningful ranges; correct non-admissible values or set them as missing. 8 Identify missing values and store them in an appropriate format; make edits if needed. 9 Make a habit of looking into the actual data tables to spot issues you didn't think of. 1 0. Have a description of variables. 1 1 . Document every step of data cleaning. Organizing Data Tables for a Project After having discussed how to detect issues with observations and variables in a data table and what to do with such issues, let's turn to how the various data tables should be organized. It is good practice to organize and store the data at three levels. These are: 2.12 Organizing Data Tables for a Project 51 Raw data tables Clean and tidy data tables Workfile(s) for analysis. Raw data, the data as it was obtained in the first place, should always be stored. Raw data may come in a single file or in multiple files. It should be stored in the original format before anything is done to it. This way we can always go back and modify steps of data cleaning if necessary. And, most often it is necessary: something may go wrong, or we may uncover new issues during the process of data cleaning or data analysis. Having the raw data is also key for replicating our analysis. That becomes especially important if similar analyses are to be done in the future. The next step is producing clean and tidy data from raw data. That's the process of data cleaning. It involves making sure each observation is in one row, each variable is in one column, and variables are ready for analysis. This is the long and tedious process of data cleaning that we have discussed in the larger part of this chapter. Often tidy data means multiple data tables. It often makes sense to create different data tables for data coming from different sources. That makes the process of creating tidy data from raw data transparent. We should always create different data tables for data with different kinds of observations. In cross-sectional data this means potentially different kinds of entities, such as individuals, families, neighborhoods, schools of children, employers of workers. In time series data this means potentially different time periods, such as daily observation of prices but weekly observations of quantities. For multi-dimensional data this may mean all of the above, plus different data tables for information at different levels of observations. In cross-sectional time series data we may have data tables with one row for each cross-sectional unit iwithout any change in time, data tables with aggregate time series with observations t (that are the same for all cross-sectional units), and some data tables with observations i, t— in the long format of course. The last of the three levels of the file structure is the workfile. This is the one file on which the analysis is to be done. The rows of workfiles are observations that form the basis of the analysis. Typically, they contain only a subset of all available variables, and, with more than one tidy data file, they may use data from all or only a subset of those files. Workfiles may or may not be in tidy data format. Let us emphasize the advantage of having tidy data before turning them into a workfile for the analysis. Tidy data files tend to be more transparent. Thus they are better suited to identifying problems in the data, addressing those problems and producing clean data, and adding or taking away observations and variables. With tidy data we can add new variables to be included in the analysis, and we can produce various kinds of workfiles for various kinds of analyses. Consider the organization of files for this textbook. For all case studies, we stored raw files as they were collected. The clean folder contains cleaned tidy data tables as well as the code that produces these clean files from the raw ones. The folder for each case study includes the code that creates the workfile and performs the analysis itself. It may also include the workfile as well. We also added an output folder for storing graphs and tables."
